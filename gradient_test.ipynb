{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT裁剪\n",
    "\n",
    "采用训练数据链接：\n",
    "\n",
    "https://huggingface.co/datasets/hw2942/financial-news-sentiment\n",
    "\n",
    "0:Negative, 1:Neutral, 2:Positive\n",
    "\n",
    "huggingface模型链接：\n",
    "\n",
    "https://huggingface.co/hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0634, -0.0002,  0.0107,  ..., -0.0730, -0.0641,  0.0158],\n",
       "        [ 0.0112, -0.0260,  0.0082,  ...,  0.0998,  0.0648, -0.0175],\n",
       "        [ 0.0432,  0.0092,  0.0175,  ...,  0.0038, -0.1050, -0.0408],\n",
       "        ...,\n",
       "        [-0.0089, -0.0044,  0.0227,  ...,  0.0486,  0.0171, -0.1026],\n",
       "        [ 0.0317, -0.0212, -0.0513,  ...,  0.0162,  0.0206,  0.0090],\n",
       "        [ 0.0252,  0.0828, -0.0401,  ..., -0.0359,  0.0246,  0.0610]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
      "          4.1627e-04,  5.1642e-04],\n",
      "        [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
      "          2.5252e-05, -5.1715e-04],\n",
      "        [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
      "         -4.1708e-04, -1.0099e-03],\n",
      "        ...,\n",
      "        [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
      "          3.6865e-04, -2.5955e-04],\n",
      "        [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
      "          5.9767e-04, -3.3672e-04],\n",
      "        [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
      "          5.4058e-04,  6.3561e-04]])\n"
     ]
    }
   ],
   "source": [
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要清除梯度缓存，否则会导致以上代码运行地越多，梯度变得越来越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 确保每次反向传播前梯度为零\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 打印出对head_weight的梯度\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(heads_weight\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "loss.backward()\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
      "          4.1627e-04,  5.1642e-04],\n",
      "        [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
      "          2.5252e-05, -5.1715e-04],\n",
      "        [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
      "         -4.1708e-04, -1.0099e-03],\n",
      "        ...,\n",
      "        [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
      "          3.6865e-04, -2.5955e-04],\n",
      "        [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
      "          5.9767e-04, -3.3672e-04],\n",
      "        [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
      "          5.4058e-04,  6.3561e-04]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.2833e-05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_weight.grad[0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取 BERT 的配置参数\n",
    "hidden_size = model.config.hidden_size  # BERT 的隐藏层维度，例如 768\n",
    "num_attention_heads = model.config.num_attention_heads  # 注意力头的数量，例如 12\n",
    "head_size = hidden_size // num_attention_heads  # 每个头的维度\n",
    "\n",
    "# 选择你想要的单个头的索引\n",
    "head_index = 0  # 选择第一个注意力头（索引从0开始）\n",
    "\n",
    "# 提取出单个头的 query 权重\n",
    "# query_weight 的形状为 [hidden_size, hidden_size]，需要切片提取单个头的参数\n",
    "# 对 query_weight 的第一维度进行切片，获取相应的权重\n",
    "single_head_weight = heads_weight[:, head_index * head_size:(head_index + 1) * head_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_24584\\2471559650.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  single_head_weight.grad\n"
     ]
    }
   ],
   "source": [
    "single_head_weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 64])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_head_weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_weight.grad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask = torch.tensor(12*[12*[1]])\n",
    "head_mask[0][0]=0\n",
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = TransformerPruner(model)\n",
    "ffn_mask=torch.tensor([[1]*3072]*12)\n",
    "pruner.prune(head_mask=head_mask,ffn_mask=ffn_mask,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7346e-04, -2.1618e-04,  1.9485e-04,  ...,  2.4316e-05,\n",
      "          2.4423e-04,  3.5674e-04],\n",
      "        [ 1.3257e-03, -3.4072e-04, -9.1461e-06,  ..., -3.8282e-04,\n",
      "         -2.8592e-04, -8.9046e-04],\n",
      "        [ 1.3701e-03,  1.9329e-04, -3.8640e-04,  ..., -2.7384e-04,\n",
      "         -5.4856e-04, -1.2977e-03],\n",
      "        ...,\n",
      "        [ 3.9116e-04, -1.5989e-04, -2.9226e-04,  ..., -2.4665e-04,\n",
      "          2.7834e-04, -2.9312e-04],\n",
      "        [ 1.6195e-03,  1.8898e-04,  8.0555e-06,  ..., -5.2405e-04,\n",
      "          4.5568e-04, -5.2088e-04],\n",
      "        [-3.9798e-04, -3.7070e-05,  4.8267e-04,  ...,  6.0957e-05,\n",
      "          4.6266e-04,  4.2070e-04]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.4403e-05)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_weight.grad[0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1183, -0.0081,  0.0040,  ...,  0.0138,  0.0171, -0.0819],\n",
       "        [ 0.0080, -0.0235, -0.0055,  ...,  0.0013,  0.0419,  0.0117],\n",
       "        [ 0.0150,  0.0061,  0.0354,  ...,  0.1125, -0.0318, -0.0137],\n",
       "        ...,\n",
       "        [-0.0514,  0.0237,  0.0113,  ...,  0.0543,  0.0055,  0.0721],\n",
       "        [ 0.0170, -0.0891, -0.0563,  ..., -0.0436,  0.0368, -0.0144],\n",
       "        [ 0.0128, -0.0325, -0.0010,  ...,  0.0957, -0.0337,  0.0017]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0  # 第1层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.7181e-04,  1.6097e-04, -8.2610e-04,  ...,  3.3229e-04,\n",
      "         -4.7372e-04,  5.0043e-04],\n",
      "        [-8.2390e-05,  4.5101e-04,  4.1507e-04,  ..., -2.2643e-04,\n",
      "          2.8777e-04,  4.1227e-05],\n",
      "        [ 1.9445e-04,  1.0557e-04,  6.6730e-04,  ...,  1.2498e-04,\n",
      "         -8.1389e-04,  2.1818e-03],\n",
      "        ...,\n",
      "        [ 3.0416e-04,  5.4775e-05, -4.6295e-04,  ...,  3.0760e-04,\n",
      "         -1.2061e-04, -8.1021e-04],\n",
      "        [ 6.8528e-04,  3.2869e-04, -9.6820e-04,  ...,  1.0962e-03,\n",
      "         -7.8020e-04, -1.4448e-05],\n",
      "        [ 9.4563e-05, -8.5372e-04,  6.4145e-04,  ...,  5.2069e-04,\n",
      "         -6.2215e-05,  9.0355e-04]], device='cuda:0') torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "\n",
    "pruner = TransformerPruner(model)\n",
    "\n",
    "head_mask=torch.tensor(12*[12*[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mask[1]=torch.tensor([0]*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.7181e-04,  1.6097e-04, -8.2610e-04,  ...,  3.3229e-04,\n",
      "         -4.7372e-04,  5.0043e-04],\n",
      "        [-8.2390e-05,  4.5101e-04,  4.1507e-04,  ..., -2.2643e-04,\n",
      "          2.8777e-04,  4.1227e-05],\n",
      "        [ 1.9445e-04,  1.0557e-04,  6.6730e-04,  ...,  1.2498e-04,\n",
      "         -8.1389e-04,  2.1818e-03],\n",
      "        ...,\n",
      "        [ 3.0416e-04,  5.4775e-05, -4.6295e-04,  ...,  3.0760e-04,\n",
      "         -1.2061e-04, -8.1021e-04],\n",
      "        [ 6.8528e-04,  3.2869e-04, -9.6820e-04,  ...,  1.0962e-03,\n",
      "         -7.8020e-04, -1.4448e-05],\n",
      "        [ 9.4563e-05, -8.5372e-04,  6.4145e-04,  ...,  5.2069e-04,\n",
      "         -6.2215e-05,  9.0355e-04]], device='cuda:0') torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论：\n",
    "\n",
    "在剪过了某些头之后，其他头内部参数的梯度发生变化，所以不能只算一次头的梯度，来得到这个头是否重要。可能需要多次求关于其他的头的偏导才可以。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[2.2416e-02, 1.3964e-02, 1.0259e-02,  ..., 8.6136e-03,\n",
       "            4.5173e-03, 7.3297e-01],\n",
       "           [1.0430e-01, 4.7057e-02, 3.1528e-01,  ..., 2.3341e-04,\n",
       "            7.2508e-03, 8.3452e-03],\n",
       "           [3.3869e-02, 4.7658e-01, 8.4863e-03,  ..., 3.8979e-04,\n",
       "            4.2075e-04, 2.7585e-02],\n",
       "           ...,\n",
       "           [2.6577e-02, 1.9967e-03, 1.3971e-03,  ..., 1.9350e-02,\n",
       "            2.8812e-02, 4.2120e-02],\n",
       "           [9.3660e-02, 8.7783e-03, 3.4070e-03,  ..., 4.0842e-01,\n",
       "            3.5635e-02, 2.4212e-01],\n",
       "           [5.5091e-03, 4.0392e-06, 1.3031e-05,  ..., 2.3731e-06,\n",
       "            7.7740e-06, 9.9445e-01]],\n",
       " \n",
       "          [[7.0329e-01, 6.5748e-03, 1.4338e-02,  ..., 1.2167e-02,\n",
       "            1.2683e-02, 1.6100e-02],\n",
       "           [7.5525e-01, 1.7563e-02, 1.0136e-02,  ..., 9.4450e-03,\n",
       "            6.5129e-03, 1.6172e-02],\n",
       "           [6.4728e-01, 2.1073e-02, 1.0803e-01,  ..., 1.3424e-02,\n",
       "            9.9957e-03, 9.1611e-03],\n",
       "           ...,\n",
       "           [1.0018e-01, 6.0616e-02, 2.2776e-02,  ..., 6.6616e-02,\n",
       "            5.7477e-03, 6.2395e-03],\n",
       "           [5.5635e-02, 4.8653e-02, 2.6951e-02,  ..., 8.8789e-03,\n",
       "            2.4144e-02, 5.5895e-03],\n",
       "           [3.4319e-01, 2.7126e-02, 8.5846e-03,  ..., 9.9326e-03,\n",
       "            5.6810e-03, 2.6816e-03]],\n",
       " \n",
       "          [[6.2700e-03, 6.4416e-02, 2.8954e-02,  ..., 3.2593e-02,\n",
       "            6.0728e-02, 1.3312e-02],\n",
       "           [7.5408e-02, 2.4471e-02, 4.0006e-02,  ..., 1.1358e-02,\n",
       "            1.6803e-02, 4.0208e-02],\n",
       "           [8.3400e-03, 1.2382e-02, 3.4228e-02,  ..., 5.2159e-02,\n",
       "            3.0978e-02, 1.5319e-02],\n",
       "           ...,\n",
       "           [8.1539e-03, 2.7742e-02, 1.6517e-02,  ..., 4.5979e-02,\n",
       "            6.7070e-02, 8.0281e-02],\n",
       "           [5.8205e-02, 1.3391e-02, 2.4762e-02,  ..., 6.1892e-02,\n",
       "            4.7218e-02, 1.5239e-01],\n",
       "           [2.2702e-03, 3.5459e-02, 3.0416e-02,  ..., 3.5723e-02,\n",
       "            5.1914e-02, 7.1217e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.6312e-01, 7.2885e-04, 1.2221e-03,  ..., 1.2074e-03,\n",
       "            1.6908e-03, 1.0999e-02],\n",
       "           [9.0140e-02, 6.1300e-02, 2.5058e-02,  ..., 1.2649e-02,\n",
       "            9.2214e-03, 3.0042e-02],\n",
       "           [1.2199e-01, 4.8641e-02, 1.7830e-02,  ..., 6.5411e-02,\n",
       "            3.0485e-02, 4.9056e-02],\n",
       "           ...,\n",
       "           [1.3066e-01, 3.3334e-02, 7.7133e-02,  ..., 8.7384e-02,\n",
       "            3.4283e-02, 4.4496e-02],\n",
       "           [1.0789e-01, 1.4525e-02, 3.5075e-02,  ..., 2.1042e-02,\n",
       "            1.0329e-02, 2.9344e-02],\n",
       "           [7.9892e-01, 1.6379e-03, 2.6289e-03,  ..., 3.9227e-03,\n",
       "            1.8739e-03, 1.4852e-01]],\n",
       " \n",
       "          [[9.4099e-01, 2.3810e-03, 1.8997e-03,  ..., 2.9731e-03,\n",
       "            1.4038e-03, 5.0936e-03],\n",
       "           [1.3701e-02, 3.0335e-02, 2.8639e-01,  ..., 2.4403e-03,\n",
       "            2.1990e-03, 2.3828e-02],\n",
       "           [7.1634e-02, 2.0541e-01, 6.5671e-02,  ..., 1.5891e-02,\n",
       "            2.7714e-03, 1.5762e-02],\n",
       "           ...,\n",
       "           [2.3833e-02, 1.2438e-03, 1.8269e-02,  ..., 2.7355e-02,\n",
       "            1.0931e-01, 1.7845e-01],\n",
       "           [5.0432e-03, 8.6806e-03, 7.1158e-03,  ..., 5.4427e-01,\n",
       "            1.2017e-02, 1.6043e-01],\n",
       "           [8.8272e-01, 2.8675e-04, 5.9453e-04,  ..., 1.3623e-02,\n",
       "            2.7750e-03, 9.2067e-02]],\n",
       " \n",
       "          [[9.3131e-01, 3.0844e-03, 1.1559e-03,  ..., 2.7699e-03,\n",
       "            3.3020e-03, 3.5952e-03],\n",
       "           [5.6876e-02, 3.4772e-02, 2.1383e-01,  ..., 6.0731e-03,\n",
       "            7.2735e-03, 9.6313e-03],\n",
       "           [3.8160e-02, 3.7636e-01, 2.8223e-02,  ..., 9.0513e-03,\n",
       "            5.9038e-03, 8.5072e-03],\n",
       "           ...,\n",
       "           [3.8915e-02, 2.2808e-02, 1.2532e-02,  ..., 3.4947e-02,\n",
       "            9.7645e-02, 1.3834e-01],\n",
       "           [4.0390e-02, 2.9579e-02, 1.4175e-02,  ..., 7.5658e-02,\n",
       "            2.7254e-02, 3.1304e-01],\n",
       "           [7.0269e-01, 1.1741e-03, 1.4932e-03,  ..., 2.3990e-02,\n",
       "            5.6134e-02, 1.8552e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[7.2816e-01, 1.3650e-02, 1.0702e-02,  ..., 1.0053e-02,\n",
       "            1.1798e-02, 3.9579e-02],\n",
       "           [7.2285e-01, 2.1172e-03, 3.2784e-03,  ..., 1.7199e-05,\n",
       "            1.3815e-01, 6.3878e-04],\n",
       "           [2.5053e-02, 9.6681e-01, 1.2874e-04,  ..., 1.5998e-05,\n",
       "            8.4427e-07, 9.7951e-04],\n",
       "           ...,\n",
       "           [5.8851e-01, 4.4007e-06, 4.3932e-05,  ..., 3.4435e-03,\n",
       "            1.3528e-01, 8.5570e-03],\n",
       "           [2.0570e-02, 3.4738e-05, 1.8771e-04,  ..., 9.5908e-01,\n",
       "            5.1852e-03, 4.6652e-03],\n",
       "           [9.9114e-01, 1.0592e-06, 3.2077e-04,  ..., 8.0551e-05,\n",
       "            6.6000e-03, 9.6448e-04]],\n",
       " \n",
       "          [[1.3557e-01, 3.1728e-02, 3.4227e-02,  ..., 4.0564e-02,\n",
       "            3.8722e-02, 5.9364e-02],\n",
       "           [1.3528e-01, 3.1237e-04, 2.3754e-02,  ..., 1.1894e-02,\n",
       "            4.4217e-02, 1.2469e-02],\n",
       "           [4.9493e-02, 1.1678e-01, 1.5374e-03,  ..., 3.0215e-03,\n",
       "            2.4048e-02, 9.9318e-03],\n",
       "           ...,\n",
       "           [1.6616e-01, 1.7609e-03, 2.6896e-02,  ..., 2.5119e-04,\n",
       "            5.2533e-02, 2.5412e-02],\n",
       "           [1.0010e-01, 7.1148e-02, 4.7633e-02,  ..., 8.6306e-02,\n",
       "            1.6100e-03, 2.4636e-02],\n",
       "           [3.9074e-01, 1.0787e-02, 6.5133e-03,  ..., 1.0653e-02,\n",
       "            2.1377e-02, 2.8781e-01]],\n",
       " \n",
       "          [[8.8672e-01, 6.4850e-03, 1.0909e-03,  ..., 5.3369e-03,\n",
       "            6.5582e-03, 2.4176e-02],\n",
       "           [9.1866e-02, 1.3915e-01, 1.1627e-02,  ..., 9.1785e-04,\n",
       "            2.1709e-03, 1.3009e-02],\n",
       "           [9.9371e-02, 1.3409e-01, 6.6116e-02,  ..., 1.3196e-02,\n",
       "            3.7501e-03, 3.9427e-02],\n",
       "           ...,\n",
       "           [2.4054e-02, 1.7147e-03, 2.6183e-03,  ..., 3.8443e-02,\n",
       "            7.1283e-03, 2.6827e-02],\n",
       "           [8.5942e-02, 7.2323e-03, 1.2464e-02,  ..., 3.3287e-01,\n",
       "            7.6826e-02, 1.3509e-02],\n",
       "           [9.3046e-01, 2.6175e-03, 2.8906e-04,  ..., 4.1959e-03,\n",
       "            1.1626e-02, 1.4125e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.0974e-01, 9.2851e-04, 3.5822e-03,  ..., 4.1121e-03,\n",
       "            5.6398e-03, 1.3064e-02],\n",
       "           [2.2037e-01, 2.5925e-02, 5.1383e-02,  ..., 8.7567e-04,\n",
       "            9.4422e-03, 1.5792e-02],\n",
       "           [8.5193e-02, 2.4185e-03, 5.9210e-02,  ..., 4.2135e-03,\n",
       "            5.6318e-03, 8.3019e-03],\n",
       "           ...,\n",
       "           [6.7225e-01, 1.9524e-04, 1.2204e-03,  ..., 1.8645e-01,\n",
       "            4.4234e-02, 6.1237e-02],\n",
       "           [5.2085e-01, 2.0694e-03, 8.9590e-03,  ..., 2.6332e-02,\n",
       "            2.9027e-01, 7.0768e-02],\n",
       "           [9.9828e-01, 3.4527e-06, 1.5434e-05,  ..., 7.7404e-05,\n",
       "            2.0991e-04, 6.1868e-04]],\n",
       " \n",
       "          [[7.0900e-01, 1.5633e-02, 1.4836e-02,  ..., 8.2750e-03,\n",
       "            8.8559e-03, 3.2460e-02],\n",
       "           [2.8560e-01, 2.5408e-03, 7.4577e-02,  ..., 7.4435e-04,\n",
       "            1.0184e-02, 2.5446e-02],\n",
       "           [1.1213e-01, 1.1343e-02, 2.6117e-02,  ..., 1.2294e-03,\n",
       "            1.6363e-02, 3.6170e-02],\n",
       "           ...,\n",
       "           [2.0919e-01, 1.0554e-01, 9.0860e-03,  ..., 1.2294e-02,\n",
       "            3.5644e-03, 1.6935e-02],\n",
       "           [5.9993e-01, 5.1767e-03, 1.6453e-02,  ..., 6.4010e-03,\n",
       "            8.7742e-04, 9.6990e-03],\n",
       "           [1.9400e-01, 3.3013e-02, 2.3340e-02,  ..., 9.6197e-03,\n",
       "            1.3010e-03, 4.1543e-01]],\n",
       " \n",
       "          [[9.5140e-01, 1.1207e-03, 1.2628e-03,  ..., 3.8138e-03,\n",
       "            1.2866e-03, 2.7361e-03],\n",
       "           [2.4182e-01, 1.2505e-02, 1.0951e-02,  ..., 5.8668e-02,\n",
       "            1.0701e-02, 9.1749e-03],\n",
       "           [1.0466e-01, 2.5870e-02, 1.1181e-02,  ..., 9.4272e-03,\n",
       "            7.0671e-02, 9.6590e-03],\n",
       "           ...,\n",
       "           [1.6752e-01, 2.3032e-03, 7.3352e-02,  ..., 5.8658e-02,\n",
       "            9.0552e-02, 9.9010e-04],\n",
       "           [3.3062e-01, 1.1109e-02, 6.7489e-02,  ..., 2.8664e-02,\n",
       "            2.8117e-02, 7.4763e-03],\n",
       "           [9.6159e-01, 5.0578e-04, 1.2987e-03,  ..., 3.9691e-03,\n",
       "            1.2400e-03, 6.2711e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.2478e-01, 1.4507e-03, 2.3260e-03,  ..., 1.6881e-03,\n",
       "            7.3707e-04, 1.4328e-01],\n",
       "           [5.7893e-01, 4.9558e-03, 1.0758e-02,  ..., 3.5145e-03,\n",
       "            4.0746e-03, 2.2207e-01],\n",
       "           [8.1456e-01, 2.1932e-03, 3.8565e-03,  ..., 4.8447e-03,\n",
       "            5.5962e-02, 7.0486e-02],\n",
       "           ...,\n",
       "           [7.5208e-01, 6.8475e-04, 8.1220e-04,  ..., 1.3238e-02,\n",
       "            2.3266e-02, 1.4223e-01],\n",
       "           [1.9427e-01, 5.0675e-02, 3.2154e-03,  ..., 4.6736e-03,\n",
       "            2.5260e-02, 3.7742e-02],\n",
       "           [9.4457e-01, 2.0317e-04, 4.4138e-04,  ..., 2.4061e-04,\n",
       "            1.1945e-04, 4.8138e-02]],\n",
       " \n",
       "          [[8.9549e-01, 3.9950e-04, 1.4918e-03,  ..., 5.2024e-03,\n",
       "            1.7075e-03, 6.2730e-02],\n",
       "           [7.3160e-03, 2.3605e-04, 9.8276e-01,  ..., 6.2893e-09,\n",
       "            7.5573e-06, 1.2240e-03],\n",
       "           [8.1860e-01, 3.9611e-03, 6.6867e-03,  ..., 8.4158e-05,\n",
       "            3.2596e-05, 1.4462e-01],\n",
       "           ...,\n",
       "           [1.0169e-03, 8.8615e-09, 5.7833e-06,  ..., 6.9103e-06,\n",
       "            9.9768e-01, 4.0767e-04],\n",
       "           [8.8754e-01, 1.2208e-04, 5.9000e-07,  ..., 5.7038e-04,\n",
       "            1.6111e-02, 9.2828e-02],\n",
       "           [8.2352e-01, 4.2747e-04, 2.9490e-03,  ..., 1.7894e-02,\n",
       "            5.1778e-03, 1.0614e-01]],\n",
       " \n",
       "          [[3.6829e-01, 1.7706e-04, 2.7383e-04,  ..., 4.1556e-04,\n",
       "            1.0669e-04, 6.2659e-01],\n",
       "           [3.3875e-01, 1.0488e-02, 7.8689e-02,  ..., 3.1796e-02,\n",
       "            2.4194e-02, 4.0494e-02],\n",
       "           [4.1288e-01, 1.4729e-02, 7.4260e-02,  ..., 3.8034e-02,\n",
       "            9.4534e-03, 1.1094e-01],\n",
       "           ...,\n",
       "           [5.8677e-02, 6.5902e-03, 9.2419e-03,  ..., 1.9626e-02,\n",
       "            2.5792e-02, 5.5839e-03],\n",
       "           [3.2517e-01, 7.7345e-03, 5.2522e-03,  ..., 3.6717e-01,\n",
       "            8.9815e-03, 1.0001e-01],\n",
       "           [8.1165e-01, 7.3408e-05, 4.6537e-05,  ..., 1.7993e-04,\n",
       "            8.1271e-05, 1.8652e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.7037e-01, 3.9140e-03, 2.7094e-03,  ..., 6.2905e-03,\n",
       "            2.9570e-03, 5.0784e-02],\n",
       "           [1.8624e-01, 1.8634e-02, 2.2054e-01,  ..., 1.0628e-03,\n",
       "            3.8960e-04, 5.7079e-02],\n",
       "           [2.4957e-01, 3.8036e-03, 3.7258e-02,  ..., 1.1790e-04,\n",
       "            3.6149e-05, 1.6450e-02],\n",
       "           ...,\n",
       "           [6.2504e-01, 2.2767e-04, 1.0506e-04,  ..., 6.2742e-02,\n",
       "            9.4071e-02, 1.8706e-01],\n",
       "           [5.6919e-01, 1.7228e-04, 2.0664e-03,  ..., 6.7171e-02,\n",
       "            1.7633e-01, 1.5424e-01],\n",
       "           [9.4849e-01, 1.1726e-03, 6.1648e-04,  ..., 1.9763e-03,\n",
       "            1.3057e-03, 3.2966e-02]],\n",
       " \n",
       "          [[9.0417e-01, 3.8294e-03, 2.2898e-03,  ..., 2.6217e-03,\n",
       "            9.0208e-04, 4.0653e-02],\n",
       "           [7.5261e-01, 3.0463e-02, 2.6534e-02,  ..., 2.5064e-03,\n",
       "            7.9416e-05, 1.6081e-01],\n",
       "           [5.6158e-01, 7.2125e-02, 1.2608e-01,  ..., 2.4387e-04,\n",
       "            1.0597e-03, 2.1868e-01],\n",
       "           ...,\n",
       "           [4.7666e-02, 4.8583e-05, 5.0087e-05,  ..., 3.3440e-02,\n",
       "            1.0682e-02, 4.0737e-03],\n",
       "           [2.1826e-02, 7.0863e-05, 7.5457e-06,  ..., 6.5893e-02,\n",
       "            2.6268e-02, 4.2897e-03],\n",
       "           [9.4641e-01, 1.9690e-03, 1.1758e-03,  ..., 3.3696e-03,\n",
       "            7.6120e-04, 3.2620e-02]],\n",
       " \n",
       "          [[8.8653e-01, 2.0808e-03, 1.8460e-03,  ..., 1.5336e-03,\n",
       "            2.5603e-03, 6.1562e-02],\n",
       "           [1.1979e-01, 1.6254e-04, 8.6156e-01,  ..., 8.3971e-05,\n",
       "            4.7991e-05, 8.4759e-03],\n",
       "           [7.5882e-01, 8.3747e-03, 2.5780e-03,  ..., 1.6809e-05,\n",
       "            4.7859e-05, 1.4112e-01],\n",
       "           ...,\n",
       "           [3.2177e-01, 5.5374e-06, 3.9135e-05,  ..., 3.5860e-03,\n",
       "            6.2633e-01, 1.5567e-02],\n",
       "           [7.8427e-01, 8.1520e-04, 7.6380e-07,  ..., 9.1637e-02,\n",
       "            8.4463e-04, 1.0549e-01],\n",
       "           [2.9813e-01, 7.8431e-05, 7.1723e-05,  ..., 1.5659e-05,\n",
       "            2.7108e-04, 6.9761e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.6045e-02, 1.3336e-03, 4.9298e-04,  ..., 7.2168e-03,\n",
       "            5.6856e-03, 8.9680e-01],\n",
       "           [6.6185e-02, 1.4625e-02, 2.2200e-02,  ..., 4.5827e-02,\n",
       "            4.2006e-03, 5.8914e-01],\n",
       "           [7.8962e-02, 3.6741e-02, 1.3522e-02,  ..., 4.8699e-02,\n",
       "            5.1614e-03, 2.6829e-01],\n",
       "           ...,\n",
       "           [2.5576e-02, 3.3726e-03, 1.1351e-03,  ..., 2.0310e-01,\n",
       "            7.1372e-02, 2.9942e-01],\n",
       "           [3.4455e-03, 4.0049e-04, 9.6399e-04,  ..., 8.1491e-01,\n",
       "            8.3756e-03, 3.4646e-02],\n",
       "           [2.1122e-03, 1.2674e-04, 3.8567e-05,  ..., 1.1089e-04,\n",
       "            1.3447e-04, 9.9512e-01]],\n",
       " \n",
       "          [[4.9439e-02, 3.8315e-02, 4.6051e-02,  ..., 3.2184e-02,\n",
       "            1.9906e-02, 5.2712e-02],\n",
       "           [1.3944e-01, 1.7204e-02, 8.2679e-02,  ..., 1.1570e-02,\n",
       "            2.9856e-03, 2.9487e-01],\n",
       "           [7.9048e-02, 4.3940e-02, 8.2798e-02,  ..., 2.7945e-02,\n",
       "            4.9264e-03, 4.3858e-01],\n",
       "           ...,\n",
       "           [2.5874e-01, 5.7010e-03, 1.0480e-03,  ..., 6.6510e-03,\n",
       "            1.6266e-02, 6.2355e-01],\n",
       "           [8.8609e-02, 1.1505e-02, 1.5386e-02,  ..., 4.5653e-02,\n",
       "            1.2087e-02, 5.9940e-01],\n",
       "           [5.9461e-02, 2.0328e-03, 1.2016e-03,  ..., 4.2079e-03,\n",
       "            3.1610e-03, 8.8804e-01]],\n",
       " \n",
       "          [[3.0084e-01, 2.9860e-02, 1.7672e-02,  ..., 6.7087e-03,\n",
       "            1.6321e-02, 1.7114e-01],\n",
       "           [7.1999e-02, 3.1036e-01, 9.9031e-03,  ..., 2.7463e-04,\n",
       "            2.1235e-04, 4.9878e-02],\n",
       "           [1.4763e-01, 2.5212e-02, 1.9231e-01,  ..., 1.2495e-03,\n",
       "            1.7633e-03, 1.7767e-01],\n",
       "           ...,\n",
       "           [1.6841e-02, 1.0979e-04, 3.1584e-04,  ..., 9.4813e-01,\n",
       "            6.6153e-03, 1.2878e-02],\n",
       "           [3.0379e-01, 8.6702e-04, 7.9327e-04,  ..., 9.0094e-02,\n",
       "            3.5493e-01, 1.6698e-01],\n",
       "           [2.7262e-01, 2.8798e-02, 1.5389e-02,  ..., 2.0485e-02,\n",
       "            1.9663e-02, 1.7132e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[4.2711e-02, 6.2255e-03, 2.5148e-03,  ..., 2.3871e-03,\n",
       "            1.5004e-03, 8.8779e-01],\n",
       "           [8.2646e-02, 6.5422e-03, 4.1399e-03,  ..., 8.6417e-04,\n",
       "            1.0829e-03, 8.7764e-01],\n",
       "           [2.5722e-02, 2.9145e-02, 3.0941e-03,  ..., 2.7012e-04,\n",
       "            5.3910e-04, 9.0670e-01],\n",
       "           ...,\n",
       "           [7.5122e-02, 1.0644e-03, 7.2379e-04,  ..., 6.3021e-02,\n",
       "            5.6114e-02, 4.7968e-01],\n",
       "           [5.6022e-02, 1.0661e-03, 4.2054e-04,  ..., 2.1438e-01,\n",
       "            3.4688e-02, 2.0187e-01],\n",
       "           [1.5092e-02, 3.9633e-03, 1.9022e-03,  ..., 9.7719e-04,\n",
       "            1.1545e-03, 9.5521e-01]],\n",
       " \n",
       "          [[4.7915e-02, 2.4906e-03, 3.0566e-04,  ..., 7.1247e-04,\n",
       "            2.3037e-04, 9.3549e-01],\n",
       "           [2.4839e-01, 2.2648e-01, 2.3047e-04,  ..., 1.5770e-04,\n",
       "            1.7391e-04, 1.9195e-01],\n",
       "           [2.2342e-01, 1.3314e-03, 1.4642e-01,  ..., 1.7205e-03,\n",
       "            1.7587e-04, 6.0535e-02],\n",
       "           ...,\n",
       "           [1.3923e-01, 3.2806e-04, 6.3509e-04,  ..., 2.2784e-01,\n",
       "            5.5491e-04, 6.0813e-01],\n",
       "           [1.6664e-01, 2.4163e-04, 4.7172e-04,  ..., 8.7200e-04,\n",
       "            6.0286e-02, 7.6038e-01],\n",
       "           [2.2488e-02, 8.0171e-04, 3.6980e-04,  ..., 2.5598e-04,\n",
       "            1.3411e-04, 9.6871e-01]],\n",
       " \n",
       "          [[6.1121e-02, 6.8131e-04, 3.0026e-04,  ..., 2.0023e-03,\n",
       "            4.5872e-03, 8.9881e-01],\n",
       "           [1.1837e-01, 4.6909e-03, 9.3376e-02,  ..., 1.4075e-03,\n",
       "            4.3862e-04, 2.5704e-01],\n",
       "           [5.9150e-02, 2.7966e-02, 1.8650e-02,  ..., 6.3132e-04,\n",
       "            4.8527e-04, 3.7457e-01],\n",
       "           ...,\n",
       "           [6.6594e-02, 3.4081e-04, 8.0321e-05,  ..., 2.7545e-02,\n",
       "            6.3172e-02, 8.0943e-01],\n",
       "           [5.5282e-02, 2.0090e-04, 3.6730e-05,  ..., 4.4958e-02,\n",
       "            5.0490e-02, 7.8607e-01],\n",
       "           [2.8205e-02, 7.2176e-04, 2.9447e-04,  ..., 1.1640e-03,\n",
       "            4.2757e-03, 9.4216e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.6431e-02, 1.4440e-02, 1.8601e-02,  ..., 5.2379e-02,\n",
       "            1.4430e-02, 1.3603e-01],\n",
       "           [7.3041e-03, 1.5288e-02, 3.3699e-02,  ..., 2.6369e-02,\n",
       "            9.0521e-03, 2.3735e-01],\n",
       "           [7.0522e-03, 1.0169e-02, 4.5724e-02,  ..., 2.9057e-02,\n",
       "            9.3755e-03, 6.4600e-02],\n",
       "           ...,\n",
       "           [1.1941e-02, 1.3306e-02, 7.0191e-03,  ..., 1.7594e-01,\n",
       "            2.0827e-02, 1.6875e-01],\n",
       "           [1.7110e-02, 1.5032e-02, 1.9436e-02,  ..., 1.1647e-01,\n",
       "            2.1420e-02, 1.3328e-01],\n",
       "           [1.4197e-02, 3.0375e-04, 2.6551e-04,  ..., 7.8061e-04,\n",
       "            2.3127e-04, 9.7551e-01]],\n",
       " \n",
       "          [[1.4780e-01, 5.8808e-02, 6.4066e-02,  ..., 6.9975e-03,\n",
       "            4.1051e-03, 3.0940e-01],\n",
       "           [1.7452e-01, 1.8474e-02, 3.1033e-02,  ..., 1.1894e-03,\n",
       "            3.3451e-04, 3.2711e-01],\n",
       "           [1.6223e-01, 8.2023e-03, 5.3461e-02,  ..., 1.9855e-03,\n",
       "            5.2617e-03, 1.1551e-01],\n",
       "           ...,\n",
       "           [9.9248e-02, 2.3711e-03, 4.7699e-03,  ..., 1.9192e-03,\n",
       "            5.7298e-03, 5.7883e-01],\n",
       "           [1.3075e-01, 1.9597e-03, 1.4743e-02,  ..., 6.6367e-03,\n",
       "            1.4977e-03, 6.9193e-01],\n",
       "           [8.7724e-03, 5.6386e-04, 6.9544e-04,  ..., 2.2918e-04,\n",
       "            8.7052e-04, 9.6570e-01]],\n",
       " \n",
       "          [[5.6996e-02, 3.8791e-03, 6.1308e-03,  ..., 1.6512e-03,\n",
       "            2.4975e-03, 8.1819e-01],\n",
       "           [1.7962e-02, 2.1184e-01, 4.6953e-07,  ..., 2.3554e-08,\n",
       "            1.3807e-07, 4.0920e-02],\n",
       "           [2.6025e-03, 1.1330e-07, 1.4470e-01,  ..., 2.1263e-06,\n",
       "            1.0141e-08, 1.0968e-02],\n",
       "           ...,\n",
       "           [3.8947e-03, 5.6495e-08, 5.4498e-07,  ..., 9.5756e-01,\n",
       "            1.8192e-07, 3.8450e-02],\n",
       "           [6.1141e-03, 1.6301e-06, 9.0676e-08,  ..., 2.4680e-06,\n",
       "            1.3728e-02, 9.8008e-01],\n",
       "           [1.0093e-02, 1.6086e-04, 3.9027e-04,  ..., 2.6820e-04,\n",
       "            1.3629e-04, 9.8414e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[6.8578e-02, 2.2815e-02, 2.8549e-02,  ..., 7.4962e-02,\n",
       "            7.4559e-02, 9.6789e-02],\n",
       "           [1.2715e-01, 2.1434e-02, 3.6940e-02,  ..., 3.8820e-02,\n",
       "            6.8478e-02, 1.5180e-01],\n",
       "           [6.2168e-02, 5.3939e-02, 1.3513e-01,  ..., 1.6042e-02,\n",
       "            1.5422e-02, 7.9852e-02],\n",
       "           ...,\n",
       "           [3.1546e-02, 9.8336e-04, 6.8290e-03,  ..., 1.9243e-01,\n",
       "            8.0882e-02, 2.9368e-02],\n",
       "           [5.5623e-03, 6.1466e-05, 2.3341e-03,  ..., 1.8582e-01,\n",
       "            1.5732e-02, 1.0982e-02],\n",
       "           [1.2878e-01, 3.2636e-02, 2.0483e-02,  ..., 4.4262e-02,\n",
       "            2.5876e-02, 1.0992e-01]],\n",
       " \n",
       "          [[7.1718e-03, 2.4581e-03, 1.2993e-02,  ..., 4.4097e-03,\n",
       "            2.2495e-03, 9.0933e-01],\n",
       "           [6.6202e-02, 5.9664e-02, 2.8800e-02,  ..., 2.3076e-04,\n",
       "            4.0120e-04, 6.8734e-01],\n",
       "           [2.9364e-02, 6.9555e-01, 1.5681e-02,  ..., 6.8027e-04,\n",
       "            1.0417e-04, 1.2439e-01],\n",
       "           ...,\n",
       "           [3.5277e-02, 6.9242e-04, 3.0614e-03,  ..., 1.1795e-01,\n",
       "            5.7663e-03, 3.1775e-01],\n",
       "           [2.9624e-02, 3.2823e-04, 5.9825e-03,  ..., 6.9902e-01,\n",
       "            6.1495e-04, 4.9965e-02],\n",
       "           [3.9730e-04, 2.1151e-04, 1.6078e-03,  ..., 1.4113e-03,\n",
       "            2.7655e-04, 9.8954e-01]],\n",
       " \n",
       "          [[4.7257e-02, 4.0802e-03, 1.7861e-02,  ..., 3.5115e-02,\n",
       "            2.7794e-02, 5.0935e-01],\n",
       "           [3.5889e-02, 6.9023e-03, 8.3842e-02,  ..., 3.1279e-03,\n",
       "            1.3275e-03, 5.9506e-01],\n",
       "           [6.6066e-02, 6.9695e-03, 3.8709e-04,  ..., 1.9339e-02,\n",
       "            8.8194e-03, 5.3933e-01],\n",
       "           ...,\n",
       "           [8.1561e-02, 2.8574e-03, 5.8222e-03,  ..., 2.6586e-02,\n",
       "            6.8101e-02, 5.0522e-01],\n",
       "           [8.7600e-02, 4.3940e-03, 1.6590e-02,  ..., 1.8518e-02,\n",
       "            3.4319e-03, 5.4173e-01],\n",
       "           [5.8327e-03, 6.7496e-04, 6.9839e-04,  ..., 1.4759e-03,\n",
       "            1.0348e-03, 9.7310e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.2883e-04, 3.8834e-03, 4.8390e-03,  ..., 4.2902e-03,\n",
       "            3.5555e-03, 7.9183e-01],\n",
       "           [1.3523e-01, 4.8935e-02, 3.2003e-02,  ..., 3.1022e-06,\n",
       "            7.5354e-06, 6.5845e-01],\n",
       "           [1.2235e-02, 3.0030e-03, 3.0956e-01,  ..., 4.7504e-06,\n",
       "            3.7293e-06, 3.2174e-01],\n",
       "           ...,\n",
       "           [2.7629e-02, 1.3472e-06, 1.0433e-06,  ..., 1.3311e-01,\n",
       "            2.3259e-02, 8.0927e-01],\n",
       "           [1.0684e-02, 4.1765e-06, 2.1127e-06,  ..., 4.8450e-02,\n",
       "            2.5688e-02, 9.0970e-01],\n",
       "           [2.6368e-04, 7.0586e-03, 3.6260e-03,  ..., 1.3055e-03,\n",
       "            4.4269e-03, 9.0789e-01]],\n",
       " \n",
       "          [[2.7969e-02, 2.4358e-02, 8.2947e-02,  ..., 8.4556e-03,\n",
       "            7.2842e-03, 5.5161e-01],\n",
       "           [6.5852e-02, 1.9404e-02, 6.0215e-02,  ..., 2.2105e-03,\n",
       "            2.6280e-03, 5.7999e-01],\n",
       "           [5.2532e-02, 7.0048e-03, 1.5237e-03,  ..., 1.7294e-02,\n",
       "            3.0419e-02, 4.8680e-01],\n",
       "           ...,\n",
       "           [4.1067e-02, 1.2094e-03, 7.5053e-03,  ..., 1.7628e-02,\n",
       "            1.8872e-02, 2.2329e-01],\n",
       "           [3.4383e-02, 6.4856e-04, 7.5840e-03,  ..., 2.4579e-02,\n",
       "            2.8186e-03, 2.4004e-01],\n",
       "           [1.1940e-02, 6.8780e-03, 6.7209e-03,  ..., 7.0512e-03,\n",
       "            9.6335e-03, 8.0872e-01]],\n",
       " \n",
       "          [[9.9409e-03, 8.0271e-03, 1.6485e-01,  ..., 6.0749e-03,\n",
       "            7.9928e-04, 5.3550e-02],\n",
       "           [1.0287e-02, 1.9096e-02, 1.7680e-01,  ..., 4.1328e-03,\n",
       "            3.3933e-03, 4.2723e-01],\n",
       "           [2.8792e-02, 1.3606e-02, 2.7618e-01,  ..., 4.9126e-03,\n",
       "            1.8909e-03, 7.5416e-02],\n",
       "           ...,\n",
       "           [6.9507e-02, 4.7721e-04, 1.3549e-03,  ..., 2.2622e-01,\n",
       "            5.2780e-02, 3.0250e-01],\n",
       "           [2.7483e-02, 5.9297e-05, 1.1182e-03,  ..., 1.0095e-01,\n",
       "            3.3687e-02, 5.9217e-01],\n",
       "           [1.4984e-04, 3.6979e-05, 3.3668e-04,  ..., 7.4776e-05,\n",
       "            4.6267e-05, 9.9812e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.9365e-03, 9.8894e-02, 4.2220e-02,  ..., 2.8483e-03,\n",
       "            4.7024e-03, 5.4533e-01],\n",
       "           [7.1722e-03, 1.7398e-02, 5.8795e-02,  ..., 3.7283e-03,\n",
       "            2.4995e-03, 6.1223e-01],\n",
       "           [1.5973e-02, 4.0780e-03, 1.3811e-02,  ..., 6.4863e-03,\n",
       "            5.2529e-03, 8.0464e-01],\n",
       "           ...,\n",
       "           [1.8565e-02, 1.2655e-03, 8.6393e-04,  ..., 1.2349e-01,\n",
       "            7.3436e-02, 5.9617e-01],\n",
       "           [8.7766e-03, 5.3479e-04, 1.0052e-03,  ..., 3.7000e-01,\n",
       "            2.3407e-01, 2.3049e-01],\n",
       "           [2.3356e-03, 3.4345e-03, 4.1097e-03,  ..., 3.2567e-03,\n",
       "            3.2092e-03, 9.3310e-01]],\n",
       " \n",
       "          [[8.5229e-03, 1.5486e-02, 9.5131e-03,  ..., 2.7544e-03,\n",
       "            4.6494e-03, 8.7412e-01],\n",
       "           [3.1833e-02, 1.1530e-02, 5.1785e-01,  ..., 6.0369e-06,\n",
       "            1.3369e-06, 3.9376e-01],\n",
       "           [1.2044e-02, 9.8264e-03, 1.1453e-02,  ..., 6.7044e-05,\n",
       "            7.4795e-06, 7.3748e-01],\n",
       "           ...,\n",
       "           [3.5996e-02, 1.8530e-06, 7.8606e-06,  ..., 1.6970e-02,\n",
       "            1.3249e-01, 8.0074e-01],\n",
       "           [5.6704e-03, 4.7716e-05, 2.7288e-06,  ..., 9.0365e-02,\n",
       "            1.9653e-02, 8.7187e-01],\n",
       "           [3.2777e-03, 4.7986e-03, 7.9479e-03,  ..., 3.4338e-03,\n",
       "            7.7477e-03, 9.1498e-01]],\n",
       " \n",
       "          [[7.2738e-03, 1.5056e-03, 1.7149e-03,  ..., 4.2404e-03,\n",
       "            9.3835e-03, 9.1005e-01],\n",
       "           [2.3066e-02, 1.3128e-02, 5.8754e-03,  ..., 4.1405e-06,\n",
       "            4.0105e-05, 8.7689e-01],\n",
       "           [2.4808e-03, 3.2998e-01, 6.5060e-04,  ..., 3.1140e-06,\n",
       "            3.7706e-06, 1.4091e-02],\n",
       "           ...,\n",
       "           [1.6487e-03, 4.6063e-06, 1.8288e-04,  ..., 1.1009e-03,\n",
       "            1.1175e-02, 9.2034e-01],\n",
       "           [6.8517e-03, 6.3614e-07, 1.0680e-06,  ..., 9.4467e-01,\n",
       "            3.2685e-03, 4.2705e-02],\n",
       "           [5.6776e-03, 1.2401e-03, 6.3766e-03,  ..., 3.0247e-03,\n",
       "            5.6267e-03, 8.9821e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.7578e-01, 9.7044e-02, 4.9237e-02,  ..., 6.2935e-04,\n",
       "            7.3673e-04, 4.7246e-01],\n",
       "           [2.2380e-02, 3.0376e-03, 2.3757e-02,  ..., 6.0609e-04,\n",
       "            1.4709e-03, 3.3190e-01],\n",
       "           [8.4453e-02, 5.1861e-03, 4.6346e-02,  ..., 5.2616e-04,\n",
       "            4.3086e-04, 2.3648e-01],\n",
       "           ...,\n",
       "           [2.5432e-01, 2.4334e-04, 2.5895e-04,  ..., 2.5346e-02,\n",
       "            1.3847e-02, 6.7682e-01],\n",
       "           [3.7125e-01, 3.8830e-04, 2.7847e-04,  ..., 4.1233e-02,\n",
       "            6.3867e-03, 5.5178e-01],\n",
       "           [9.4529e-03, 1.3447e-03, 9.9934e-04,  ..., 6.0439e-03,\n",
       "            3.9799e-03, 9.1155e-01]],\n",
       " \n",
       "          [[4.2968e-03, 8.1184e-03, 7.6619e-03,  ..., 9.8971e-03,\n",
       "            7.3692e-03, 9.0580e-01],\n",
       "           [7.0499e-02, 9.5988e-03, 6.9380e-03,  ..., 5.7510e-05,\n",
       "            5.1424e-05, 9.0299e-01],\n",
       "           [2.0109e-02, 4.9435e-03, 3.9224e-03,  ..., 1.0594e-05,\n",
       "            2.7878e-05, 9.5993e-01],\n",
       "           ...,\n",
       "           [1.2813e-03, 3.8495e-06, 1.4847e-05,  ..., 8.9938e-03,\n",
       "            1.3939e-02, 7.3820e-02],\n",
       "           [2.2767e-04, 2.3286e-06, 6.1982e-06,  ..., 2.4015e-03,\n",
       "            6.7763e-03, 5.6691e-02],\n",
       "           [7.2918e-04, 5.9745e-04, 4.3586e-04,  ..., 2.0597e-03,\n",
       "            2.2539e-03, 9.8031e-01]],\n",
       " \n",
       "          [[7.2160e-03, 4.4324e-03, 4.0979e-03,  ..., 5.5485e-04,\n",
       "            1.4703e-03, 9.7648e-01],\n",
       "           [1.4981e-02, 6.5409e-03, 1.0686e-01,  ..., 2.1270e-07,\n",
       "            4.1410e-06, 8.6613e-01],\n",
       "           [2.1077e-02, 2.1900e-02, 9.8063e-03,  ..., 3.1934e-06,\n",
       "            3.9812e-06, 9.1893e-01],\n",
       "           ...,\n",
       "           [3.2987e-03, 7.3617e-08, 3.1454e-06,  ..., 6.8130e-03,\n",
       "            1.1877e-01, 8.6291e-01],\n",
       "           [5.6736e-03, 5.7365e-06, 2.1237e-06,  ..., 1.4953e-01,\n",
       "            2.4005e-02, 8.1428e-01],\n",
       "           [1.6922e-03, 2.9005e-03, 1.0398e-02,  ..., 3.8586e-03,\n",
       "            3.3544e-03, 9.3626e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.5169e-03, 3.6095e-01, 8.2031e-02,  ..., 4.9126e-04,\n",
       "            4.9274e-04, 5.1505e-01],\n",
       "           [2.7748e-02, 1.3687e-02, 4.2913e-03,  ..., 3.1129e-05,\n",
       "            6.8670e-06, 8.1682e-01],\n",
       "           [2.3181e-02, 7.6849e-03, 1.7165e-02,  ..., 8.4057e-06,\n",
       "            4.4363e-05, 8.9809e-01],\n",
       "           ...,\n",
       "           [5.0444e-02, 1.1477e-04, 4.5298e-06,  ..., 1.2567e-01,\n",
       "            2.5711e-02, 7.4509e-01],\n",
       "           [1.3725e-01, 1.9302e-05, 2.0697e-05,  ..., 4.6699e-02,\n",
       "            9.8737e-02, 6.1107e-01],\n",
       "           [1.5403e-03, 1.7762e-02, 1.5188e-02,  ..., 2.1686e-03,\n",
       "            2.4125e-03, 8.7636e-01]],\n",
       " \n",
       "          [[1.1689e-02, 1.2485e-03, 2.2715e-03,  ..., 2.5999e-03,\n",
       "            2.5745e-03, 8.0853e-01],\n",
       "           [2.2874e-03, 4.7476e-03, 7.2436e-03,  ..., 2.6607e-03,\n",
       "            2.7754e-03, 8.8630e-01],\n",
       "           [8.0190e-03, 6.2020e-03, 2.4251e-02,  ..., 2.9059e-03,\n",
       "            2.7975e-03, 8.5757e-01],\n",
       "           ...,\n",
       "           [1.0880e-02, 2.5726e-05, 2.6273e-05,  ..., 1.5107e-02,\n",
       "            4.1970e-03, 9.6215e-01],\n",
       "           [1.5277e-02, 1.2286e-05, 6.8707e-06,  ..., 3.0494e-02,\n",
       "            7.4804e-03, 9.4101e-01],\n",
       "           [9.2522e-04, 4.2751e-04, 4.5966e-04,  ..., 1.2479e-03,\n",
       "            9.9332e-04, 9.6375e-01]],\n",
       " \n",
       "          [[2.0279e-03, 1.0024e-03, 2.0649e-03,  ..., 4.7436e-03,\n",
       "            1.0126e-04, 9.6675e-01],\n",
       "           [1.1921e-02, 2.5333e-02, 1.8424e-02,  ..., 7.8791e-07,\n",
       "            3.7795e-05, 9.2515e-01],\n",
       "           [2.4648e-02, 3.1267e-02, 2.8757e-03,  ..., 4.0164e-06,\n",
       "            2.7218e-06, 9.3695e-01],\n",
       "           ...,\n",
       "           [5.2737e-04, 4.2177e-08, 3.2125e-07,  ..., 1.1527e-02,\n",
       "            3.4654e-02, 9.3771e-01],\n",
       "           [2.7579e-02, 6.8544e-06, 3.8409e-06,  ..., 2.4632e-01,\n",
       "            1.6768e-02, 7.0469e-01],\n",
       "           [4.1464e-04, 6.7895e-04, 1.1512e-02,  ..., 8.1340e-03,\n",
       "            1.3694e-03, 8.6124e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.0122e-03, 8.1217e-05, 5.0957e-05,  ..., 2.2191e-04,\n",
       "            5.2255e-04, 9.9516e-01],\n",
       "           [7.0067e-03, 1.9190e-03, 1.5365e-02,  ..., 4.4901e-06,\n",
       "            6.8170e-05, 9.7318e-01],\n",
       "           [6.5300e-03, 2.3009e-02, 7.7281e-03,  ..., 1.5058e-05,\n",
       "            1.5962e-05, 9.6098e-01],\n",
       "           ...,\n",
       "           [4.5669e-04, 1.5011e-06, 2.6012e-06,  ..., 2.3849e-03,\n",
       "            1.9838e-02, 9.7326e-01],\n",
       "           [2.4194e-04, 1.7642e-05, 1.1439e-05,  ..., 1.4198e-01,\n",
       "            8.7377e-02, 7.2666e-01],\n",
       "           [1.0804e-03, 2.0153e-04, 2.4749e-03,  ..., 2.5922e-04,\n",
       "            5.8005e-04, 9.7801e-01]],\n",
       " \n",
       "          [[6.3371e-04, 2.8160e-02, 3.6221e-03,  ..., 6.1304e-03,\n",
       "            2.1923e-03, 9.0681e-01],\n",
       "           [1.6585e-01, 1.0440e-01, 1.5400e-01,  ..., 2.3078e-05,\n",
       "            4.0073e-05, 4.8489e-01],\n",
       "           [8.3757e-04, 9.5808e-01, 6.7348e-03,  ..., 7.7427e-06,\n",
       "            4.0716e-07, 1.8378e-02],\n",
       "           ...,\n",
       "           [7.6256e-05, 4.0677e-06, 6.0732e-05,  ..., 8.1286e-02,\n",
       "            7.3774e-02, 3.0226e-01],\n",
       "           [1.0083e-05, 1.4714e-06, 9.8067e-06,  ..., 8.3002e-01,\n",
       "            3.0780e-02, 5.7953e-02],\n",
       "           [1.2399e-03, 4.3175e-03, 4.7767e-03,  ..., 1.1792e-02,\n",
       "            3.7550e-03, 8.8919e-01]],\n",
       " \n",
       "          [[1.1702e-03, 4.7366e-04, 5.2530e-04,  ..., 1.3114e-03,\n",
       "            6.4275e-04, 9.8880e-01],\n",
       "           [1.3918e-01, 4.1660e-02, 1.4090e-01,  ..., 6.6408e-03,\n",
       "            3.1774e-03, 4.7787e-01],\n",
       "           [1.0248e-01, 4.4966e-02, 1.3099e-01,  ..., 3.8558e-03,\n",
       "            1.4843e-03, 5.6241e-01],\n",
       "           ...,\n",
       "           [1.8096e-02, 1.4594e-03, 2.9183e-03,  ..., 3.4805e-02,\n",
       "            1.6515e-02, 3.4881e-01],\n",
       "           [1.1890e-02, 1.1089e-03, 2.2046e-03,  ..., 1.4137e-02,\n",
       "            8.3942e-03, 2.8805e-01],\n",
       "           [9.8424e-04, 7.0407e-04, 1.4834e-03,  ..., 4.7718e-03,\n",
       "            3.9526e-03, 9.6310e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1668e-03, 6.4590e-04, 2.4191e-04,  ..., 7.5883e-03,\n",
       "            6.1978e-03, 9.5891e-01],\n",
       "           [7.2093e-02, 2.7738e-02, 5.8976e-02,  ..., 7.5965e-04,\n",
       "            2.9269e-04, 8.1721e-01],\n",
       "           [3.3243e-02, 2.4131e-02, 1.5460e-02,  ..., 2.1617e-04,\n",
       "            8.7164e-05, 9.1204e-01],\n",
       "           ...,\n",
       "           [8.5394e-03, 1.2815e-02, 1.4184e-02,  ..., 3.1503e-02,\n",
       "            1.2764e-02, 2.4016e-01],\n",
       "           [9.0190e-03, 1.4075e-02, 4.6217e-03,  ..., 4.8961e-02,\n",
       "            2.3418e-02, 2.8216e-01],\n",
       "           [8.8942e-03, 1.3697e-03, 1.3129e-03,  ..., 2.6768e-03,\n",
       "            1.7950e-03, 9.5261e-01]],\n",
       " \n",
       "          [[1.6104e-03, 1.1293e-04, 1.6402e-03,  ..., 2.6244e-05,\n",
       "            6.4345e-05, 9.9523e-01],\n",
       "           [7.1103e-03, 1.1692e-03, 1.4794e-01,  ..., 1.0342e-05,\n",
       "            5.3917e-05, 8.2989e-01],\n",
       "           [1.5513e-03, 4.2035e-04, 2.4090e-02,  ..., 2.9111e-05,\n",
       "            1.1852e-04, 9.4524e-01],\n",
       "           ...,\n",
       "           [2.1236e-03, 4.1676e-06, 7.1024e-06,  ..., 1.7491e-03,\n",
       "            8.5092e-03, 9.8235e-01],\n",
       "           [5.9407e-04, 3.8500e-06, 8.7957e-06,  ..., 2.8891e-03,\n",
       "            7.0167e-03, 9.8608e-01],\n",
       "           [8.5438e-04, 6.5916e-03, 1.4898e-02,  ..., 1.0780e-03,\n",
       "            2.2753e-04, 9.4006e-01]],\n",
       " \n",
       "          [[6.9520e-04, 3.6061e-03, 4.6173e-03,  ..., 7.0554e-03,\n",
       "            2.2225e-03, 9.0945e-01],\n",
       "           [3.1013e-04, 7.5734e-03, 5.5856e-02,  ..., 2.4182e-02,\n",
       "            5.2118e-03, 1.7178e-01],\n",
       "           [3.1133e-04, 3.7869e-03, 1.5175e-02,  ..., 5.9743e-02,\n",
       "            1.0551e-02, 2.1956e-01],\n",
       "           ...,\n",
       "           [7.4293e-03, 3.2650e-04, 1.4661e-03,  ..., 1.8747e-01,\n",
       "            8.8276e-02, 4.6323e-01],\n",
       "           [1.6307e-02, 1.2144e-04, 6.1221e-04,  ..., 1.6445e-01,\n",
       "            6.1625e-02, 5.5042e-01],\n",
       "           [2.0373e-03, 8.2956e-04, 1.6045e-03,  ..., 2.7834e-03,\n",
       "            1.1994e-03, 9.3994e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.5659e-04, 7.8333e-04, 3.1615e-04,  ..., 4.4584e-01,\n",
       "            2.3662e-01, 4.4526e-03],\n",
       "           [2.4346e-02, 2.0393e-02, 2.1937e-02,  ..., 6.4059e-03,\n",
       "            4.6557e-03, 6.1514e-01],\n",
       "           [1.6852e-02, 1.9657e-02, 1.5207e-02,  ..., 2.5472e-03,\n",
       "            1.3066e-03, 7.3624e-01],\n",
       "           ...,\n",
       "           [1.8200e-02, 5.6336e-03, 8.2051e-03,  ..., 7.2592e-03,\n",
       "            6.3282e-03, 7.9598e-01],\n",
       "           [2.1508e-02, 5.7250e-03, 5.4228e-03,  ..., 1.2911e-02,\n",
       "            1.0087e-02, 7.5128e-01],\n",
       "           [1.7564e-03, 4.0523e-04, 6.3966e-04,  ..., 1.6430e-04,\n",
       "            1.6421e-04, 9.8416e-01]],\n",
       " \n",
       "          [[1.2434e-02, 6.0254e-04, 1.3559e-03,  ..., 1.5077e-01,\n",
       "            3.8504e-02, 3.3200e-01],\n",
       "           [2.2153e-02, 1.2704e-01, 8.3512e-02,  ..., 5.5960e-03,\n",
       "            1.4208e-03, 6.3944e-01],\n",
       "           [8.5475e-03, 3.7622e-02, 9.6831e-02,  ..., 1.1646e-03,\n",
       "            5.0538e-04, 7.8154e-01],\n",
       "           ...,\n",
       "           [1.8464e-02, 1.8369e-04, 5.9610e-04,  ..., 7.1807e-02,\n",
       "            3.0585e-02, 7.2514e-01],\n",
       "           [7.6600e-03, 9.1763e-05, 2.9970e-04,  ..., 7.1068e-02,\n",
       "            3.9404e-02, 7.6036e-01],\n",
       "           [7.2842e-03, 5.2107e-03, 9.0081e-03,  ..., 2.8191e-03,\n",
       "            3.9847e-03, 8.9477e-01]],\n",
       " \n",
       "          [[2.5208e-03, 9.5986e-03, 9.4096e-03,  ..., 6.4491e-02,\n",
       "            2.7853e-02, 7.3211e-01],\n",
       "           [1.8273e-03, 2.2477e-02, 2.5288e-02,  ..., 7.2128e-04,\n",
       "            3.6244e-04, 9.1783e-01],\n",
       "           [5.9960e-04, 4.5289e-03, 7.7237e-02,  ..., 2.5335e-05,\n",
       "            3.3016e-05, 9.0199e-01],\n",
       "           ...,\n",
       "           [3.7279e-03, 6.5433e-04, 2.3308e-04,  ..., 5.1526e-02,\n",
       "            1.6566e-02, 7.1085e-01],\n",
       "           [2.9203e-03, 2.3288e-04, 3.2883e-04,  ..., 5.7923e-02,\n",
       "            2.0827e-02, 7.2036e-01],\n",
       "           [2.0965e-03, 3.4218e-03, 1.6534e-03,  ..., 7.4052e-03,\n",
       "            2.4409e-03, 9.2637e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[6.6481e-03, 4.8925e-04, 2.8525e-04,  ..., 1.2761e-01,\n",
       "            7.2575e-02, 1.4008e-01],\n",
       "           [4.9031e-03, 2.9192e-02, 1.3028e-01,  ..., 7.5849e-02,\n",
       "            5.8606e-02, 3.2497e-01],\n",
       "           [5.7787e-03, 2.3349e-02, 1.0362e-01,  ..., 4.5762e-02,\n",
       "            5.8662e-02, 3.8028e-01],\n",
       "           ...,\n",
       "           [2.1116e-01, 1.2772e-03, 1.5123e-04,  ..., 4.4302e-02,\n",
       "            3.0170e-02, 4.0992e-01],\n",
       "           [1.3419e-01, 1.4546e-03, 1.1685e-04,  ..., 1.2033e-02,\n",
       "            7.4406e-03, 5.0846e-01],\n",
       "           [3.9661e-03, 1.3367e-03, 1.5257e-03,  ..., 2.1473e-03,\n",
       "            2.5826e-03, 8.9817e-01]],\n",
       " \n",
       "          [[1.0249e-02, 1.8725e-03, 4.0173e-04,  ..., 9.4965e-03,\n",
       "            8.2891e-03, 9.2074e-01],\n",
       "           [6.6900e-03, 9.0205e-03, 2.0711e-02,  ..., 3.6937e-04,\n",
       "            7.0473e-04, 8.8533e-01],\n",
       "           [1.4096e-03, 2.4468e-02, 8.3849e-04,  ..., 2.0470e-04,\n",
       "            3.1704e-05, 9.6526e-01],\n",
       "           ...,\n",
       "           [3.1950e-02, 1.0305e-05, 5.4268e-05,  ..., 6.0563e-02,\n",
       "            1.4645e-01, 6.4575e-01],\n",
       "           [9.5706e-03, 1.6446e-05, 2.5435e-05,  ..., 5.5903e-01,\n",
       "            1.3599e-01, 1.8285e-01],\n",
       "           [5.5967e-03, 1.4333e-03, 1.9182e-03,  ..., 8.4372e-03,\n",
       "            1.2390e-02, 9.2535e-01]],\n",
       " \n",
       "          [[4.2587e-03, 2.2915e-04, 1.0899e-04,  ..., 3.7615e-02,\n",
       "            2.4815e-02, 7.4103e-01],\n",
       "           [8.0752e-04, 6.2066e-02, 5.5302e-03,  ..., 2.1400e-05,\n",
       "            7.5534e-06, 9.2127e-01],\n",
       "           [2.4621e-04, 7.8406e-03, 7.7940e-02,  ..., 1.5740e-05,\n",
       "            8.3937e-06, 9.0425e-01],\n",
       "           ...,\n",
       "           [6.0832e-03, 6.5257e-06, 1.0905e-05,  ..., 1.9510e-02,\n",
       "            3.3721e-03, 9.5235e-01],\n",
       "           [3.6846e-03, 4.5381e-06, 9.4386e-06,  ..., 2.7846e-02,\n",
       "            2.8353e-02, 9.1320e-01],\n",
       "           [1.7063e-03, 4.4011e-03, 4.1830e-03,  ..., 2.3315e-03,\n",
       "            1.3795e-03, 9.5932e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.5124e-01, 6.8557e-02, 1.7308e-02,  ..., 2.8859e-02,\n",
       "            4.3337e-02, 3.7393e-02],\n",
       "           [2.8804e-02, 1.0600e-01, 7.5556e-02,  ..., 3.1348e-02,\n",
       "            2.1798e-02, 3.4884e-02],\n",
       "           [1.5750e-02, 1.0409e-01, 1.9696e-01,  ..., 2.1912e-02,\n",
       "            8.6950e-03, 2.4140e-02],\n",
       "           ...,\n",
       "           [4.3741e-01, 8.1590e-02, 3.6318e-03,  ..., 1.0869e-02,\n",
       "            8.8429e-03, 2.0535e-01],\n",
       "           [6.7267e-01, 2.0907e-02, 1.0304e-03,  ..., 7.3641e-03,\n",
       "            1.0026e-02, 1.1111e-01],\n",
       "           [1.1677e-04, 1.0731e-03, 9.9839e-04,  ..., 4.3175e-04,\n",
       "            6.2343e-04, 9.8304e-01]],\n",
       " \n",
       "          [[6.7182e-04, 1.5907e-04, 2.1340e-04,  ..., 1.1753e-01,\n",
       "            2.6769e-02, 7.1004e-01],\n",
       "           [7.5964e-05, 6.6475e-03, 6.3830e-03,  ..., 3.2203e-03,\n",
       "            1.7087e-03, 8.7845e-01],\n",
       "           [6.9665e-05, 2.4815e-02, 2.5304e-02,  ..., 3.9901e-03,\n",
       "            2.0299e-03, 8.4806e-01],\n",
       "           ...,\n",
       "           [2.6690e-04, 7.2946e-04, 4.8607e-04,  ..., 9.6049e-03,\n",
       "            5.5081e-03, 9.2703e-01],\n",
       "           [5.9121e-04, 6.6764e-04, 4.4365e-04,  ..., 1.4098e-02,\n",
       "            6.5122e-03, 8.7984e-01],\n",
       "           [1.5426e-04, 5.5864e-04, 5.0795e-04,  ..., 3.6108e-04,\n",
       "            3.4871e-04, 9.9033e-01]],\n",
       " \n",
       "          [[5.1135e-04, 1.3627e-04, 5.5535e-06,  ..., 9.9005e-03,\n",
       "            5.3391e-03, 9.6168e-01],\n",
       "           [3.3816e-04, 1.9364e-02, 1.2347e-02,  ..., 1.5603e-02,\n",
       "            4.7661e-03, 7.1242e-01],\n",
       "           [3.4460e-04, 4.1490e-02, 1.7264e-02,  ..., 3.4824e-02,\n",
       "            9.0354e-03, 4.4117e-01],\n",
       "           ...,\n",
       "           [3.8447e-04, 1.4265e-04, 2.5892e-05,  ..., 1.2239e-02,\n",
       "            5.4744e-03, 9.4262e-01],\n",
       "           [4.5274e-04, 1.0205e-04, 2.7461e-05,  ..., 1.9110e-02,\n",
       "            7.0593e-03, 9.3837e-01],\n",
       "           [3.9942e-04, 1.2109e-03, 3.2203e-04,  ..., 1.2271e-03,\n",
       "            1.0931e-03, 9.4656e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.7103e-01, 2.6643e-03, 8.6575e-04,  ..., 9.0173e-02,\n",
       "            2.3419e-02, 2.9212e-01],\n",
       "           [8.7811e-04, 2.8554e-02, 1.6362e-02,  ..., 1.2216e-04,\n",
       "            4.9147e-05, 2.2247e-01],\n",
       "           [9.2012e-04, 4.0649e-02, 6.7857e-02,  ..., 1.3539e-04,\n",
       "            1.0768e-04, 4.0485e-01],\n",
       "           ...,\n",
       "           [1.3583e-01, 5.6028e-04, 8.4258e-04,  ..., 1.5481e-01,\n",
       "            4.2001e-02, 4.6187e-01],\n",
       "           [7.7338e-02, 3.0492e-04, 6.7147e-04,  ..., 1.3007e-01,\n",
       "            6.7485e-02, 5.5671e-01],\n",
       "           [2.4750e-03, 3.9265e-03, 3.0766e-03,  ..., 3.4822e-03,\n",
       "            2.6531e-03, 9.4164e-01]],\n",
       " \n",
       "          [[5.4573e-03, 3.9463e-04, 1.0292e-04,  ..., 1.4908e-02,\n",
       "            3.6275e-02, 9.0185e-01],\n",
       "           [3.9175e-05, 1.8241e-03, 8.7265e-04,  ..., 2.0401e-04,\n",
       "            7.3166e-04, 9.7411e-01],\n",
       "           [9.1668e-06, 3.6572e-03, 2.6439e-03,  ..., 3.4606e-05,\n",
       "            1.1399e-04, 9.7435e-01],\n",
       "           ...,\n",
       "           [4.4900e-04, 1.1441e-04, 3.0207e-05,  ..., 3.6636e-04,\n",
       "            3.9763e-04, 9.9557e-01],\n",
       "           [1.1715e-03, 2.2217e-04, 4.4564e-05,  ..., 5.8196e-04,\n",
       "            3.8933e-04, 9.9142e-01],\n",
       "           [6.6971e-05, 3.0379e-04, 3.1920e-04,  ..., 3.7382e-04,\n",
       "            6.1748e-04, 9.9622e-01]],\n",
       " \n",
       "          [[7.1230e-02, 1.1983e-03, 1.2793e-03,  ..., 2.8022e-01,\n",
       "            7.7003e-02, 1.8186e-01],\n",
       "           [6.3345e-03, 1.1598e-01, 3.3872e-02,  ..., 3.6836e-03,\n",
       "            9.2800e-04, 7.3195e-01],\n",
       "           [2.3925e-03, 3.0393e-01, 3.0378e-02,  ..., 1.1420e-03,\n",
       "            1.1052e-04, 5.6367e-01],\n",
       "           ...,\n",
       "           [1.4207e-02, 1.1643e-03, 1.1430e-03,  ..., 9.3305e-02,\n",
       "            1.5334e-02, 5.5486e-01],\n",
       "           [6.4952e-03, 3.7935e-04, 3.9337e-04,  ..., 1.2854e-01,\n",
       "            1.2598e-02, 6.1200e-01],\n",
       "           [9.0941e-03, 3.3981e-03, 2.0359e-03,  ..., 1.2399e-02,\n",
       "            3.4546e-03, 9.0140e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.8371e-03, 8.0661e-04, 1.5662e-03,  ..., 1.6597e-02,\n",
       "            2.1452e-02, 6.7379e-01],\n",
       "           [1.6360e-03, 8.5889e-03, 5.4090e-03,  ..., 1.5771e-02,\n",
       "            1.0668e-02, 6.9330e-01],\n",
       "           [1.2419e-03, 5.3394e-04, 2.2378e-02,  ..., 9.0201e-03,\n",
       "            1.1857e-02, 4.5202e-01],\n",
       "           ...,\n",
       "           [2.2558e-03, 3.1882e-04, 5.5131e-04,  ..., 5.2813e-02,\n",
       "            5.0622e-02, 5.7800e-01],\n",
       "           [2.0117e-03, 6.8552e-05, 4.9613e-04,  ..., 1.7785e-02,\n",
       "            9.4152e-02, 6.0706e-01],\n",
       "           [4.2240e-04, 1.7418e-04, 3.8448e-04,  ..., 1.6623e-03,\n",
       "            1.1124e-03, 9.6094e-01]],\n",
       " \n",
       "          [[7.6365e-04, 3.8008e-04, 1.8772e-03,  ..., 6.0840e-03,\n",
       "            4.2287e-03, 9.1939e-01],\n",
       "           [1.0260e-04, 4.3437e-04, 3.0239e-04,  ..., 2.4788e-04,\n",
       "            2.2229e-04, 9.4663e-01],\n",
       "           [5.6369e-04, 2.0207e-03, 5.8390e-04,  ..., 1.0789e-03,\n",
       "            4.1483e-04, 9.6408e-01],\n",
       "           ...,\n",
       "           [5.8657e-04, 8.1568e-04, 8.5789e-04,  ..., 8.6282e-04,\n",
       "            5.4103e-04, 9.6283e-01],\n",
       "           [7.0578e-04, 9.0094e-04, 6.5641e-04,  ..., 1.7997e-03,\n",
       "            6.9896e-04, 9.6212e-01],\n",
       "           [7.1930e-04, 1.4124e-03, 8.9800e-04,  ..., 5.4633e-03,\n",
       "            4.1345e-03, 9.3779e-01]],\n",
       " \n",
       "          [[4.4036e-02, 1.5084e-03, 1.0376e-03,  ..., 6.5554e-02,\n",
       "            2.1091e-02, 6.9183e-01],\n",
       "           [1.2046e-02, 3.0155e-02, 1.0157e-03,  ..., 5.3122e-03,\n",
       "            2.6601e-03, 9.0885e-01],\n",
       "           [1.6357e-02, 9.8135e-03, 8.3045e-03,  ..., 8.6200e-03,\n",
       "            2.5429e-03, 7.9284e-01],\n",
       "           ...,\n",
       "           [8.2889e-02, 1.3315e-03, 5.7812e-04,  ..., 9.8949e-02,\n",
       "            7.4000e-03, 6.4114e-01],\n",
       "           [2.6958e-02, 5.8806e-04, 2.9405e-04,  ..., 1.5700e-02,\n",
       "            9.9280e-02, 8.0662e-01],\n",
       "           [1.1510e-03, 6.6514e-04, 1.2996e-03,  ..., 1.8515e-03,\n",
       "            3.4068e-03, 9.4447e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.9995e-07, 1.2707e-04, 1.7569e-05,  ..., 2.6600e-04,\n",
       "            2.6281e-04, 9.9700e-01],\n",
       "           [1.3233e-05, 1.0178e-03, 7.6721e-04,  ..., 5.8160e-04,\n",
       "            5.1735e-04, 9.7304e-01],\n",
       "           [7.0322e-06, 1.0150e-03, 3.1717e-04,  ..., 2.8681e-04,\n",
       "            1.9177e-04, 9.8691e-01],\n",
       "           ...,\n",
       "           [1.7486e-05, 3.2155e-04, 1.2453e-04,  ..., 1.5354e-03,\n",
       "            1.2794e-03, 9.7686e-01],\n",
       "           [3.4712e-06, 4.7608e-05, 1.8114e-05,  ..., 7.0749e-04,\n",
       "            4.8588e-04, 9.9515e-01],\n",
       "           [3.2747e-05, 8.8279e-04, 8.2320e-04,  ..., 7.9091e-04,\n",
       "            4.6870e-04, 9.7406e-01]],\n",
       " \n",
       "          [[1.4391e-03, 2.6180e-04, 9.9782e-05,  ..., 2.5721e-02,\n",
       "            3.8233e-03, 8.9300e-01],\n",
       "           [3.7446e-04, 7.1415e-03, 6.5744e-03,  ..., 5.5789e-03,\n",
       "            7.1981e-04, 7.7818e-01],\n",
       "           [3.0362e-04, 1.5262e-02, 2.7030e-02,  ..., 7.7084e-03,\n",
       "            7.3484e-04, 6.8909e-01],\n",
       "           ...,\n",
       "           [4.9793e-03, 6.6292e-04, 1.5392e-04,  ..., 1.0081e-02,\n",
       "            2.3777e-03, 9.2655e-01],\n",
       "           [8.4972e-03, 4.8642e-04, 8.6891e-05,  ..., 2.5193e-02,\n",
       "            4.8390e-03, 8.9422e-01],\n",
       "           [1.1041e-03, 3.6399e-04, 5.4201e-04,  ..., 2.4447e-03,\n",
       "            1.1990e-03, 9.3632e-01]],\n",
       " \n",
       "          [[1.0535e-01, 7.5360e-03, 2.1320e-03,  ..., 4.1416e-02,\n",
       "            2.8452e-02, 1.6187e-01],\n",
       "           [3.2263e-03, 5.7920e-02, 7.7178e-02,  ..., 7.3217e-03,\n",
       "            2.9230e-03, 6.2565e-01],\n",
       "           [7.4952e-03, 1.1547e-01, 7.9540e-03,  ..., 9.5632e-03,\n",
       "            1.0299e-03, 6.2769e-01],\n",
       "           ...,\n",
       "           [1.9476e-01, 1.5286e-03, 1.0873e-03,  ..., 4.4255e-02,\n",
       "            5.3483e-02, 3.9032e-01],\n",
       "           [1.6016e-01, 1.6682e-03, 5.9422e-04,  ..., 3.9878e-02,\n",
       "            2.8192e-02, 5.5796e-01],\n",
       "           [2.9228e-03, 1.2329e-03, 1.6661e-03,  ..., 4.5304e-03,\n",
       "            5.3198e-03, 9.1804e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.3050e-03, 1.3513e-02, 2.1180e-02,  ..., 1.5519e-02,\n",
       "            1.3999e-02, 1.2807e-01],\n",
       "           [9.3478e-03, 2.1665e-02, 8.0429e-03,  ..., 7.6117e-03,\n",
       "            6.5755e-03, 7.1428e-01],\n",
       "           [9.2046e-03, 2.4774e-02, 1.3733e-02,  ..., 5.8350e-03,\n",
       "            4.4133e-03, 7.2904e-01],\n",
       "           ...,\n",
       "           [8.6909e-03, 7.6655e-03, 6.7335e-03,  ..., 2.9682e-02,\n",
       "            1.8820e-02, 4.8424e-01],\n",
       "           [5.1553e-03, 2.9038e-03, 4.4751e-03,  ..., 2.1351e-02,\n",
       "            1.0002e-02, 6.0163e-01],\n",
       "           [2.2458e-02, 2.8026e-02, 3.7934e-02,  ..., 2.7287e-02,\n",
       "            2.4711e-02, 2.8062e-02]],\n",
       " \n",
       "          [[6.5180e-02, 1.2888e-02, 5.0180e-03,  ..., 1.9656e-01,\n",
       "            9.0688e-02, 1.6349e-01],\n",
       "           [1.9807e-02, 4.3263e-01, 8.1560e-03,  ..., 4.0185e-02,\n",
       "            1.1908e-02, 4.3652e-02],\n",
       "           [7.3666e-03, 2.0090e-02, 1.6133e-01,  ..., 2.2619e-02,\n",
       "            2.3239e-02, 2.0288e-01],\n",
       "           ...,\n",
       "           [1.7112e-02, 2.5164e-03, 1.1381e-03,  ..., 6.5668e-01,\n",
       "            3.4053e-02, 1.0092e-02],\n",
       "           [1.2621e-02, 1.6216e-03, 5.7980e-04,  ..., 5.7460e-02,\n",
       "            7.4973e-01, 1.5447e-02],\n",
       "           [5.0394e-03, 2.1985e-02, 3.1021e-02,  ..., 3.2019e-02,\n",
       "            5.1941e-02, 4.4826e-01]],\n",
       " \n",
       "          [[1.3812e-02, 9.6786e-03, 2.2632e-03,  ..., 2.5735e-01,\n",
       "            1.4970e-01, 8.9610e-02],\n",
       "           [7.9751e-03, 3.2584e-02, 1.1160e-02,  ..., 9.3945e-02,\n",
       "            2.5556e-02, 4.9731e-01],\n",
       "           [8.7017e-03, 4.5657e-02, 2.5272e-02,  ..., 8.7809e-02,\n",
       "            2.5506e-02, 3.9842e-01],\n",
       "           ...,\n",
       "           [1.3014e-02, 5.0950e-03, 1.3071e-03,  ..., 2.0744e-01,\n",
       "            9.1953e-02, 1.6277e-01],\n",
       "           [1.7510e-02, 4.0066e-03, 1.3319e-03,  ..., 1.9782e-01,\n",
       "            8.3696e-02, 2.1140e-01],\n",
       "           [3.9549e-02, 2.9692e-02, 3.1879e-02,  ..., 7.1909e-02,\n",
       "            7.7184e-02, 4.4272e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4747e-02, 1.8492e-02, 7.1752e-03,  ..., 1.1666e-01,\n",
       "            3.0228e-01, 5.3406e-02],\n",
       "           [2.0862e-02, 9.1611e-02, 8.3930e-03,  ..., 5.6959e-03,\n",
       "            1.3550e-02, 3.9833e-01],\n",
       "           [1.1331e-02, 5.4621e-02, 2.8136e-02,  ..., 5.7595e-03,\n",
       "            1.6341e-02, 3.9328e-01],\n",
       "           ...,\n",
       "           [3.2237e-02, 1.3221e-02, 6.2950e-03,  ..., 2.0845e-02,\n",
       "            3.5529e-02, 2.9023e-01],\n",
       "           [2.3100e-02, 1.4860e-02, 1.0213e-02,  ..., 8.9557e-03,\n",
       "            5.6248e-02, 4.9171e-01],\n",
       "           [4.4016e-02, 4.2001e-02, 3.7644e-02,  ..., 5.3717e-02,\n",
       "            4.2077e-02, 2.9990e-02]],\n",
       " \n",
       "          [[4.2617e-02, 1.8609e-02, 1.1766e-02,  ..., 1.4463e-01,\n",
       "            1.2864e-01, 3.5197e-02],\n",
       "           [1.2399e-02, 2.2582e-01, 6.5018e-03,  ..., 7.3202e-02,\n",
       "            3.8986e-02, 1.7769e-01],\n",
       "           [1.1467e-02, 8.0202e-03, 3.3406e-02,  ..., 2.8858e-02,\n",
       "            4.4759e-02, 2.8732e-01],\n",
       "           ...,\n",
       "           [2.6662e-02, 5.8471e-03, 4.2525e-03,  ..., 1.3444e-01,\n",
       "            1.3707e-01, 1.1871e-01],\n",
       "           [2.1947e-02, 4.9051e-03, 7.4144e-03,  ..., 6.1757e-02,\n",
       "            2.6468e-01, 1.7436e-01],\n",
       "           [1.9877e-02, 4.0563e-02, 5.1519e-02,  ..., 4.2729e-02,\n",
       "            3.9038e-02, 2.4179e-02]],\n",
       " \n",
       "          [[1.2119e-01, 2.5681e-02, 4.2216e-03,  ..., 1.8364e-01,\n",
       "            1.2238e-01, 5.9852e-02],\n",
       "           [1.4877e-02, 5.2085e-01, 1.1521e-02,  ..., 2.0659e-02,\n",
       "            1.7390e-02, 2.2555e-01],\n",
       "           [7.8549e-03, 1.0154e-02, 2.2279e-01,  ..., 1.5005e-02,\n",
       "            1.9516e-02, 4.6180e-01],\n",
       "           ...,\n",
       "           [3.7435e-02, 4.3291e-02, 3.1762e-03,  ..., 3.7097e-01,\n",
       "            5.5829e-02, 1.3035e-01],\n",
       "           [2.7637e-02, 1.6105e-02, 5.2634e-03,  ..., 5.0370e-02,\n",
       "            4.2084e-01, 2.1095e-01],\n",
       "           [1.3137e-02, 3.9587e-02, 1.0582e-02,  ..., 4.2015e-02,\n",
       "            6.4967e-02, 2.5247e-01]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**input_ids).attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101., 3189., 1039.,  677., 1453., 1112., 4958., 1928., 1928., 2189.,\n",
       "         1158., 1724.,  702., 3299., 3173.,  856., 5401., 1039.,  120., 3189.,\n",
       "         1039.,  856.,  855., 4669., 3146.,  102.]]), 'token_type_ids': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]), 'attention_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m input_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m input_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mretain_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 计算第1个头相对于输入的梯度 (一阶导数)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m head_1_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(head_1\u001b[38;5;241m.\u001b[39msum(), outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使得输入可用于梯度计算\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt') \n",
    "outputs = model(**input_ids)\n",
    "attention_outputs = model(**input_ids).attentions  # 提取注意力输出\n",
    "\n",
    "# 假设我们选取第12层的第1头和第2头\n",
    "head_1 = attention_outputs[1][0, 0]  # 第2层，第1个注意力头\n",
    "head_2 = attention_outputs[1][0, 1]  # 第2层，第2个注意力头\n",
    "head_1.requires_grad_(True)\n",
    "head_2.requires_grad_(True)\n",
    "input_ids['input_ids']=input_ids['input_ids'].float()\n",
    "input_ids['token_type_ids']=input_ids['token_type_ids'].float()\n",
    "input_ids['attention_mask']=input_ids['attention_mask'].float()\n",
    "input_ids['input_ids'].requires_grad_(True)\n",
    "input_ids['token_type_ids'].requires_grad_(True)\n",
    "input_ids['attention_mask'].requires_grad_(True)\n",
    "outputs.last_hidden_state.retain_grad()\n",
    "# 计算第1个头相对于输入的梯度 (一阶导数)\n",
    "head_1_grad = torch.autograd.grad(head_1.sum(), outputs.last_hidden_state, create_graph=True,allow_unused=True)[0]\n",
    "\n",
    "# 计算第2个头的混合导数，即对第1个头的梯度再对第2个头进行求导\n",
    "# 这是混合二阶偏导数\n",
    "mixed_second_order_derivative = torch.autograd.grad(head_2.sum(), outputs.last_hidden_state, grad_outputs=head_1_grad, create_graph=True,allow_unused=True)[0]\n",
    "\n",
    "print(\"混合二阶导数: \", mixed_second_order_derivative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求偏导不现实，因为要剪k个头就要求k阶混合偏导，这个144中挑k个的组合数太大，更不用说代码运行时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 现在需要验证一个结论，如果被剪的头被剪后，被剪的头梯度变了，那么再剪别的头时它的梯度就不会再变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0634, -0.0002,  0.0107,  ..., -0.0730, -0.0641,  0.0158],\n",
       "        [ 0.0112, -0.0260,  0.0082,  ...,  0.0998,  0.0648, -0.0175],\n",
       "        [ 0.0432,  0.0092,  0.0175,  ...,  0.0038, -0.1050, -0.0408],\n",
       "        ...,\n",
       "        [-0.0089, -0.0044,  0.0227,  ...,  0.0486,  0.0171, -0.1026],\n",
       "        [ 0.0317, -0.0212, -0.0513,  ...,  0.0162,  0.0206,  0.0090],\n",
       "        [ 0.0252,  0.0828, -0.0401,  ..., -0.0359,  0.0246,  0.0610]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
      "          4.1627e-04,  5.1642e-04],\n",
      "        [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
      "          2.5252e-05, -5.1715e-04],\n",
      "        [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
      "         -4.1708e-04, -1.0099e-03],\n",
      "        ...,\n",
      "        [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
      "          3.6865e-04, -2.5955e-04],\n",
      "        [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
      "          5.9767e-04, -3.3672e-04],\n",
      "        [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
      "          5.4058e-04,  6.3561e-04]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=heads_weight.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.3833e-04, -5.8542e-04,  2.3521e-03,  ...,  5.5316e-05,\n",
      "         -1.2867e-03, -1.0289e-03],\n",
      "        [ 3.7727e-04,  3.2622e-04, -1.1731e-03,  ...,  1.4615e-04,\n",
      "          8.9240e-04, -6.0019e-04],\n",
      "        [ 9.2544e-04,  9.2564e-04, -1.2427e-03,  ..., -2.4506e-04,\n",
      "         -1.1435e-04,  2.4197e-04],\n",
      "        ...,\n",
      "        [ 2.7608e-04,  5.0513e-04,  2.4779e-04,  ...,  8.2564e-05,\n",
      "         -3.2886e-04, -5.9152e-05],\n",
      "        [-7.0283e-04, -4.2315e-04, -4.5128e-04,  ...,  8.2481e-05,\n",
      "          2.4653e-04,  3.4056e-04],\n",
      "        [ 3.3185e-05, -3.8162e-04, -1.0771e-04,  ..., -1.1146e-04,\n",
      "          4.0910e-05,  4.1269e-05]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "n = 3  # 第4层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "\n",
    "pruner = TransformerPruner(model)\n",
    "\n",
    "head_mask=torch.tensor(12*[12*[1]])\n",
    "head_mask[1]=torch.tensor([0]*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 768)) torch.Size([0, 768])\n"
     ]
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.value.weight.grad.size()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.9040e-03, -1.1444e-02,  1.1102e-02,  ...,  1.7492e-03,\n",
      "         -7.2757e-03,  4.0425e-03],\n",
      "        [-2.1859e-03,  1.3451e-03, -8.8417e-03,  ..., -6.1145e-03,\n",
      "          7.6276e-03, -8.8556e-03],\n",
      "        [ 7.9869e-03,  9.9755e-03, -5.9779e-03,  ...,  2.8143e-03,\n",
      "         -2.1314e-03,  2.3945e-03],\n",
      "        ...,\n",
      "        [ 2.5783e-04, -4.8985e-04,  4.0316e-03,  ..., -1.0271e-03,\n",
      "          1.5244e-03, -2.5975e-03],\n",
      "        [-5.4784e-03, -3.5003e-05, -5.2764e-03,  ...,  5.6498e-03,\n",
      "         -4.9502e-03,  2.5676e-04],\n",
      "        [-7.4197e-04, -1.8828e-03,  1.8733e-03,  ..., -8.3730e-04,\n",
      "          2.1019e-03, -4.0491e-03]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "n = 3  # 第4层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论：梯度直接消失没有了，这里需要验证只剪一个头，该层参数的梯度如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0634, -0.0002,  0.0107,  ..., -0.0730, -0.0641,  0.0158],\n",
       "        [ 0.0112, -0.0260,  0.0082,  ...,  0.0998,  0.0648, -0.0175],\n",
       "        [ 0.0432,  0.0092,  0.0175,  ...,  0.0038, -0.1050, -0.0408],\n",
       "        ...,\n",
       "        [-0.0089, -0.0044,  0.0227,  ...,  0.0486,  0.0171, -0.1026],\n",
       "        [ 0.0317, -0.0212, -0.0513,  ...,  0.0162,  0.0206,  0.0090],\n",
       "        [ 0.0252,  0.0828, -0.0401,  ..., -0.0359,  0.0246,  0.0610]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
      "          4.1627e-04,  5.1642e-04],\n",
      "        [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
      "          2.5252e-05, -5.1715e-04],\n",
      "        [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
      "         -4.1708e-04, -1.0099e-03],\n",
      "        ...,\n",
      "        [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
      "          3.6865e-04, -2.5955e-04],\n",
      "        [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
      "          5.9767e-04, -3.3672e-04],\n",
      "        [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
      "          5.4058e-04,  6.3561e-04]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "\n",
    "pruner = TransformerPruner(model)\n",
    "\n",
    "head_mask=torch.tensor(12*[12*[1]])\n",
    "head_mask[1][0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
      "          4.1627e-04,  5.1642e-04],\n",
      "        [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
      "          2.5252e-05, -5.1715e-04],\n",
      "        [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
      "         -4.1708e-04, -1.0099e-03],\n",
      "        ...,\n",
      "        [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
      "          3.6865e-04, -2.5955e-04],\n",
      "        [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
      "          5.9767e-04, -3.3672e-04],\n",
      "        [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
      "          5.4058e-04,  6.3561e-04]]) torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# 打印出对head_weight的梯度\n",
    "print(heads_weight.grad,heads_weight.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.query.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 $$\\frac{\\frac{\\partial F}{\\partial K_j}}{\\frac{\\partial u_{ij}}{\\partial K_j}} = \\frac{\\frac{\\partial F}{\\partial Q_i}}{\\frac{\\partial u_{ij}}{\\partial Q_i}}=\\frac{\\partial F}{\\partial u_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_Q_2thlayer = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_Q_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_weight_K_2thlayer = attention_layer.key.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_K_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4871e-04, -4.7463e-04,  4.9912e-04,  ...,  1.0182e-04,\n",
       "           4.1627e-04,  5.1642e-04],\n",
       "         [ 1.5761e-04, -6.2480e-04, -3.0400e-04,  ..., -3.0718e-04,\n",
       "           2.5252e-05, -5.1715e-04],\n",
       "         [ 4.3950e-04,  3.3384e-05, -5.8299e-04,  ..., -1.7127e-04,\n",
       "          -4.1708e-04, -1.0099e-03],\n",
       "         ...,\n",
       "         [-8.8954e-06, -3.4051e-04, -1.5886e-04,  ..., -2.4521e-04,\n",
       "           3.6865e-04, -2.5955e-04],\n",
       "         [ 1.4707e-03, -6.8299e-05, -9.0577e-05,  ..., -5.2714e-04,\n",
       "           5.9767e-04, -3.3672e-04],\n",
       "         [-1.4086e-04, -1.1484e-04,  5.5247e-04,  ...,  9.8638e-05,\n",
       "           5.4058e-04,  6.3561e-04]]),\n",
       " tensor([[-1.4614e-03,  6.4599e-04,  1.0378e-03,  ...,  1.6211e-04,\n",
       "           1.5565e-04,  1.2345e-04],\n",
       "         [-4.6430e-04,  4.0170e-04,  3.7591e-04,  ..., -1.3523e-05,\n",
       "           3.1846e-04,  5.1352e-05],\n",
       "         [ 1.4723e-03,  5.6938e-04, -5.6556e-04,  ..., -3.3249e-05,\n",
       "          -5.4656e-04, -1.8784e-04],\n",
       "         ...,\n",
       "         [ 6.2536e-04, -6.7116e-04, -8.8650e-04,  ..., -4.7569e-04,\n",
       "          -4.7657e-04, -4.2106e-04],\n",
       "         [-5.0697e-04,  3.8135e-04,  4.2305e-04,  ...,  4.6965e-04,\n",
       "           4.7410e-04,  2.8931e-04],\n",
       "         [-2.7056e-04,  7.5816e-04,  2.6060e-04,  ...,  3.2782e-04,\n",
       "          -5.6475e-04, -2.0562e-04]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_weight_Q_2thlayer.grad,heads_weight_K_2thlayer.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_weight_Q_2thlayer_grad=heads_weight_Q_2thlayer.grad\n",
    "heads_weight_K_2thlayer_grad=heads_weight_K_2thlayer.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 64, 768]), torch.Size([12, 64, 768]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads_weight_Q_2thlayer_grad=heads_weight_Q_2thlayer_grad.view(12,64,768)\n",
    "heads_weight_K_2thlayer_grad=heads_weight_K_2thlayer_grad.view(12,64,768)\n",
    "heads_weight_Q_2thlayer_grad.size(),heads_weight_K_2thlayer_grad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1thhead2thlayer_grad = heads_weight_Q_2thlayer_grad[0].T\n",
    "K1thhead2thlayer_grad = heads_weight_K_2thlayer_grad[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_weight_Q_2thlayer_weight = heads_weight_Q_2thlayer.view(12,64,768)\n",
    "heads_weight_K_2thlayer_weight = heads_weight_K_2thlayer.view(12,64,768)\n",
    "Q1thhead2thlayer_weight = heads_weight_Q_2thlayer_weight[0].T\n",
    "K1thhead2thlayer_weight = heads_weight_K_2thlayer_weight[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sum(K1thhead2thlayer_grad[0]*Q1thhead2thlayer_weight[0])/sum(Q1thhead2thlayer_weight[0]**2)\n",
    "b=sum(Q1thhead2thlayer_grad[0]*K1thhead2thlayer_weight[0])/sum(K1thhead2thlayer_weight[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0033, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0025, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((Q1thhead2thlayer_grad[0]+K1thhead2thlayer_grad[0])*(K1thhead2thlayer_weight[0]+Q1thhead2thlayer_weight[0]))/sum((K1thhead2thlayer_weight[0]+Q1thhead2thlayer_weight[0])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度修剪前后比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_Q_2thlayer = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_Q_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "grad = heads_weight_Q_2thlayer.grad\n",
    "\n",
    "downsampled_data = torch.nn.functional.avg_pool2d(grad.unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n",
    "\n",
    "# 将下采样后的张量转换为 NumPy 数组\n",
    "data = downsampled_data.detach().numpy()\n",
    "\n",
    "# 使用 Seaborn 的 heatmap 函数来绘制下采样后的热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data, annot=False, fmt=\".2f\", cmap='viridis', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "-->\n",
    "\n",
    "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "\n",
    "pruner = TransformerPruner(model)\n",
    "\n",
    "head_mask=torch.tensor(12*[12*[1]])\n",
    "head_mask[0][0]=torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner.prune(head_mask=head_mask, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pruned_grad = heads_weight_Q_2thlayer.grad\n",
    "\n",
    "pruned_downsampled_data = torch.nn.functional.avg_pool2d(grad.unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n",
    "\n",
    "# 将下采样后的张量转换为 NumPy 数组\n",
    "data = downsampled_data.detach().numpy()\n",
    "\n",
    "# 使用 Seaborn 的 heatmap 函数来绘制下采样后的热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data, annot=False, fmt=\".2f\", cmap='viridis', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pruned_grad = heads_weight_Q_2thlayer.grad\n",
    "\n",
    "__data = torch.nn.functional.avg_pool2d(((pruned_grad-grad)).unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n",
    "\n",
    "# 将下采样后的张量转换为 NumPy 数组\n",
    "data = __data.detach().numpy()\n",
    "\n",
    "# 使用 Seaborn 的 heatmap 函数来绘制下采样后的热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data, annot=False, fmt=\".2f\", cmap='viridis', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "\n",
    "pruner = TransformerPruner(model)\n",
    "\n",
    "head_mask=torch.tensor(12*[12*[1]])\n",
    "head_mask[1][1]=torch.tensor(0)\n",
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner.prune(head_mask=head_mask, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_Q_2thlayer = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_Q_2thlayer.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "-->\n",
    "\n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "# 创建输入数据\n",
    "input_ids = tokenizer(ds['train'][0]['Title'],return_tensors='pt')  # 示例token ID\n",
    "outputs = model(**input_ids)\n",
    "\n",
    "# 获取模型的输出\n",
    "sequence_output = outputs.logits  # 分类模型的logits输出\n",
    "\n",
    "# 选择一个标量输出，例如logits的L2范数\n",
    "loss = torch.norm(sequence_output)\n",
    "\n",
    "# 反向传播前清除梯度\n",
    "model.zero_grad()  # 确保每次反向传播前梯度为零\n",
    "\n",
    "# 保持计算图以便后续可以再次使用\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_grad = heads_weight_Q_2thlayer.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_grad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_weights_to_768x768(tensor, pruned_heads):\n",
    "    # 初始化列表，存储拼接的张量\n",
    "    tensors_to_concat = []\n",
    "    start = 0\n",
    "    # 按照 pruned_heads 进行插入\n",
    "    for head_idx, keep in enumerate(pruned_heads):\n",
    "        if keep == 1:\n",
    "            # 如果该头保留，保留原来的 64x768\n",
    "            tensors_to_concat.append(tensor[start:start+64, :])\n",
    "            start += 64  # 每次移动 64 行\n",
    "        else:\n",
    "            # 如果该头被剪掉，插入 64x768 的零矩阵\n",
    "            tensors_to_concat.append(torch.zeros(64, tensor.size(1)).to(tensor.device)) \n",
    "    # 将所有部分拼接起来，得到最终的 768x768 张量\n",
    "    expanded_tensor = torch.cat(tensors_to_concat, dim=0)\n",
    "    return expanded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded=expand_weights_to_768x768(pruned_grad, [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0895e-04, -4.4732e-04,  5.0412e-04,  ...,  9.6234e-05,\n",
       "          3.9502e-04,  5.4519e-04],\n",
       "        [ 4.7686e-04, -5.1584e-04, -2.2737e-04,  ..., -3.6277e-04,\n",
       "         -4.3752e-06, -5.6137e-04],\n",
       "        [ 8.2640e-04,  2.4393e-04, -6.6715e-04,  ..., -2.8535e-04,\n",
       "         -4.9796e-04, -1.1981e-03],\n",
       "        ...,\n",
       "        [ 3.2160e-05, -3.5559e-04, -2.3101e-04,  ..., -2.9017e-04,\n",
       "          4.1452e-04, -2.5832e-04],\n",
       "        [ 1.4643e-03, -7.0267e-05, -1.3691e-04,  ..., -5.6653e-04,\n",
       "          6.4252e-04, -4.0457e-04],\n",
       "        [-1.2116e-04, -5.0179e-06,  5.6153e-04,  ...,  1.4062e-04,\n",
       "          4.3794e-04,  5.9372e-04]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_downsampled_data==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pruned_downsampled_data = torch.nn.functional.avg_pool2d(expanded.unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n",
    "\n",
    "# 将下采样后的张量转换为 NumPy 数组\n",
    "data = pruned_downsampled_data.detach().numpy()\n",
    "\n",
    "# 使用 Seaborn 的 heatmap 函数来绘制下采样后的热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data, annot=False, fmt=\".2f\", cmap='viridis', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.0234e-05,  2.7303e-05,  5.0017e-06,  ..., -5.5830e-06,\n",
       "         -2.1251e-05,  2.8767e-05],\n",
       "        [ 3.1925e-04,  1.0895e-04,  7.6633e-05,  ..., -5.5594e-05,\n",
       "         -2.9627e-05, -4.4221e-05],\n",
       "        [ 3.8690e-04,  2.1055e-04, -8.4163e-05,  ..., -1.1408e-04,\n",
       "         -8.0883e-05, -1.8829e-04],\n",
       "        ...,\n",
       "        [ 4.1055e-05, -1.5080e-05, -7.2158e-05,  ..., -4.4961e-05,\n",
       "          4.5867e-05,  1.2338e-06],\n",
       "        [-6.4055e-06, -1.9678e-06, -4.6333e-05,  ..., -3.9385e-05,\n",
       "          4.4856e-05, -6.7847e-05],\n",
       "        [ 1.9701e-05,  1.0982e-04,  9.0587e-06,  ...,  4.1983e-05,\n",
       "         -1.0264e-04, -4.1888e-05]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.to('cpu')-grad.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4219e-05, -2.5768e-05,  8.2757e-06,  3.3295e-05,  3.2264e-06,\n",
       "          3.1769e-05,  1.5409e-05, -3.7171e-05, -4.8013e-06,  1.5994e-05,\n",
       "          1.1378e-05, -4.6372e-05],\n",
       "        [-1.7727e-06, -1.9918e-05,  6.2925e-06,  2.6289e-06, -4.4849e-06,\n",
       "          7.0756e-06,  3.1787e-06, -2.4790e-06, -3.2432e-06,  1.0016e-05,\n",
       "          2.9412e-06,  5.0679e-06],\n",
       "        [ 3.3330e-06,  9.1206e-06,  3.3667e-06, -1.8496e-06,  4.0062e-06,\n",
       "         -4.5837e-06, -5.0212e-06,  2.5179e-06,  2.9977e-06, -1.5282e-05,\n",
       "         -1.6864e-06,  1.0525e-06],\n",
       "        [-1.9569e-06, -2.7728e-06, -4.9660e-06, -6.3421e-07,  5.3859e-06,\n",
       "         -3.2925e-06,  4.3605e-07,  5.1308e-06,  4.5271e-06,  1.2605e-06,\n",
       "          2.0085e-08, -2.6638e-06],\n",
       "        [ 1.1733e-05, -1.7280e-05,  5.6780e-06, -5.5208e-07, -2.6383e-06,\n",
       "         -8.4926e-06, -1.1593e-06,  7.9904e-07,  4.0593e-06, -5.8550e-06,\n",
       "          9.4146e-06,  1.6090e-06],\n",
       "        [-1.0435e-05,  7.5152e-08,  5.6060e-06, -1.7710e-05,  3.8629e-07,\n",
       "         -3.6277e-06, -6.5420e-06,  5.9133e-06,  7.4340e-06, -4.7379e-06,\n",
       "          2.0219e-06,  2.4246e-05],\n",
       "        [ 2.6446e-05,  4.6812e-05, -4.3289e-05,  6.1646e-05, -2.9049e-05,\n",
       "         -3.9422e-06,  2.9545e-07, -9.3915e-06,  5.9050e-06,  1.4733e-05,\n",
       "          1.5081e-05, -7.8498e-05],\n",
       "        [-2.9564e-06, -4.0696e-06,  3.6598e-06,  4.9182e-07, -3.7764e-06,\n",
       "          5.1168e-06, -3.2707e-06, -4.5107e-07, -2.6049e-06,  9.5512e-06,\n",
       "         -2.0470e-07, -9.8433e-07],\n",
       "        [ 2.9509e-06,  6.2954e-06, -6.7474e-06,  5.3588e-06, -2.1238e-06,\n",
       "         -1.7495e-07,  4.2053e-06, -3.4736e-06, -2.2988e-06,  1.6893e-06,\n",
       "         -8.0986e-07, -8.9724e-06],\n",
       "        [-3.7734e-06,  5.8401e-07, -3.5214e-07,  3.9971e-06, -4.4630e-06,\n",
       "          3.5611e-06,  2.2792e-06, -4.7434e-06, -2.5047e-06,  5.0198e-07,\n",
       "          3.5033e-06,  1.8049e-06],\n",
       "        [ 2.7661e-06, -5.3974e-06, -4.0475e-06,  4.8813e-06, -5.0366e-07,\n",
       "         -1.7769e-06,  1.2718e-06, -3.5134e-06,  3.0584e-06,  5.2429e-06,\n",
       "         -1.4238e-06,  3.0601e-06],\n",
       "        [ 1.2960e-05, -1.6609e-05,  5.3697e-06, -2.4349e-06,  4.7311e-06,\n",
       "         -1.0611e-06, -2.4708e-06, -9.8494e-06,  5.7952e-06,  2.5284e-06,\n",
       "          2.9118e-06, -4.9504e-06]], device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.avg_pool2d(grad.unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "__data = torch.nn.functional.avg_pool2d((expanded.to('cpu')-grad.to('cpu')).unsqueeze(0).unsqueeze(0), kernel_size=64).squeeze(0).squeeze(0)\n",
    "\n",
    "# 将下采样后的张量转换为 NumPy 数组\n",
    "data = __data.detach().numpy()\n",
    "\n",
    "# 使用 Seaborn 的 heatmap 函数来绘制下采样后的热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data, annot=False, fmt=\".2f\", cmap='viridis', linewidths=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试梯度矩阵形状变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_Q_2thlayer = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_Q_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner=TransformerPruner(model)\n",
    "head_mask=torch.tensor(12*[[1]*12])\n",
    "head_mask[1][0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner.prune(save_model=False,head_mask=head_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_Q_2thlayer = attention_layer.query.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_Q_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_K_2thlayer = attention_layer.key.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_K_2thlayer.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([704, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1  # 第2层（索引从0开始）\n",
    "attention_layer = model.bert.encoder.layer[n].attention.self\n",
    "# 多头中的所有权重会被打包在一起，m 是你感兴趣的头的索引\n",
    "heads_weight_V_2thlayer = attention_layer.value.weight  # 例如这里访问 query 的权重\n",
    "heads_weight_V_2thlayer.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以后没准可以用这个试试可不可以剪，至少可以用它内置的方法得到头部掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]],\n",
       "\n",
       "          [[1.]]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_head_mask(head_mask,num_hidden_layers=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
