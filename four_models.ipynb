{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT裁剪\n",
    "\n",
    "采用训练数据链接：\n",
    "\n",
    "https://huggingface.co/datasets/hw2942/financial-news-sentiment\n",
    "\n",
    "0:Negative, 1:Neutral, 2:Positive\n",
    "\n",
    "huggingface模型链接：\n",
    "\n",
    "https://huggingface.co/hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n",
    "\n",
    "def get_acc(test_dataset,model,tokenizer,device='cuda'):\n",
    "    total = 0\n",
    "    right = 0\n",
    "    model.to(device)\n",
    "    for data in test_dataset:\n",
    "        inputs = tokenizer(data['Title'],return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs)\n",
    "        total  += 1\n",
    "        if torch.max(outputs[0][0].softmax(0),dim=0).indices==data['labels']:\n",
    "            right += 1\n",
    "    return right/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ds['train']\n",
    "get_acc(test_dataset,model,tokenizer,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = TransformerPruner(model)\n",
    "head_mask = torch.tensor(12*[[0]*12])\n",
    "ffn_mask=torch.tensor([[1]*3072]*12)\n",
    "pruner.prune(head_mask=head_mask,ffn_mask=ffn_mask,save_model=False)\n",
    "get_acc(test_dataset,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_mask=torch.tensor([[0]*3072]*12)\n",
    "pruner.prune(head_mask=head_mask,ffn_mask=ffn_mask,save_model=False)\n",
    "get_acc(test_dataset,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT裁剪\n",
    "\n",
    "数据链接：\n",
    "https://huggingface.co/datasets/nyu-mll/glue\n",
    "\n",
    "模型链接：\n",
    "https://huggingface.co/Alireza1044/albert-base-v2-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alireza1044/albert-base-v2-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Alireza1044/albert-base-v2-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model,size):\n",
    "    count = 0\n",
    "\n",
    "    cursor = 0\n",
    "\n",
    "    for data in tqdm(ds['train']):\n",
    "        cursor += 1\n",
    "        inputs = tokenizer(data['premise'],data['hypothesis'],return_tensors='pt') # 这里会自动在两个输入的语句中插入分隔符\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        if predicted_class_id == data['label']:\n",
    "            count += 1\n",
    "        if cursor > size:\n",
    "            break\n",
    "\n",
    "    return count / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textpruner\n",
    "from textpruner import TransformerPruner\n",
    "pruner = TransformerPruner(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mask = torch.tensor(1*[[0]*12])\n",
    "# ffn_mask = torch.tensor(6*[[0]*3072])\n",
    "pruner.prune(head_mask=head_mask,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBERTA裁剪\n",
    "\n",
    "数据链接：\n",
    "\n",
    "https://huggingface.co/siebert/sentiment-roberta-large-english\n",
    "\n",
    "模型链接：\n",
    "\n",
    "https://huggingface.co/datasets/rahmaabusalma/tweets_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"rahmaabusalma/tweets_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_acc(model,size,ds):\n",
    "    count = 0\n",
    "\n",
    "    cursor = 0\n",
    "\n",
    "    for data in tqdm(ds['train']):\n",
    "        if data['label']==1:\n",
    "            continue\n",
    "        cursor += 1\n",
    "        inputs = tokenizer(data['text'],return_tensors='pt') # 这里会自动在两个输入的语句中插入分隔符\n",
    "        with torch.no_grad():\n",
    "            predicted_class_id = model(**inputs)\n",
    "        if torch.max(predicted_class_id[0],dim=1).indices[0] == (1 if data['label']==2 else 0):\n",
    "            count += 1\n",
    "        if cursor > size:\n",
    "            break\n",
    "\n",
    "    return count / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc(model,1000,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import TransformerPruner\n",
    "import torch\n",
    "pruner = TransformerPruner(model)\n",
    "head_mask = torch.tensor(24*[12*[0]])\n",
    "pruner.prune(head_mask=head_mask,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_mask = torch.tensor(24*[4096*[0]])\n",
    "model.to('cuda')\n",
    "pruner.prune(ffn_mask=ffn_mask,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000,ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-ROBERTA裁剪\n",
    "\n",
    "数据链接：\n",
    "\n",
    "https://huggingface.co/datasets/papluca/language-identification\n",
    "\n",
    "模型链接：\n",
    "\n",
    "https://huggingface.co/papluca/xlm-roberta-base-language-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"papluca/language-identification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner=TransformerPruner(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_acc(model,size,ds):\n",
    "    count = 0\n",
    "    id2lang = model.config.id2label\n",
    "    cursor = 0\n",
    "\n",
    "    for data in tqdm(ds['train']):\n",
    "        cursor += 1\n",
    "        inputs = tokenizer(data['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        preds = torch.softmax(logits, dim=-1)\n",
    "        vals, idx = torch.max(preds, dim=1)\n",
    "        if id2lang[idx.item()]==data['labels']:\n",
    "            count += 1\n",
    "        if cursor > size:\n",
    "            break\n",
    "\n",
    "    return count / size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_mask = torch.tensor(12*[12*[0]])\n",
    "pruner.prune(head_mask=head_mask,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "get_acc(model,1000,ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力熵\n",
    "\n",
    "根据注意力矩阵，计算注意力熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.39:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            \n",
    "            data = list(data.values())\n",
    "            \n",
    "            input_text = data[0]\n",
    "            \n",
    "            input_ids = tokenizer.encode(input_text,return_tensors='pt').to('cuda')\n",
    "            \n",
    "            output = model(input_ids)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            data = list(data.values())\n",
    "            input_text = data[0]\n",
    "            input_ids = tokenizer.encode(input_text,return_tensors='pt').to('cpu')\n",
    "            output = model(input_ids)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)\n",
    "\n",
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.35:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            input_text = data['Title']\n",
    "            \n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cuda')\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            input_text = data['Title']\n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cpu')\n",
    "            output = model(**inputs)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from textpruner import TransformerPruner\n",
    "\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\",output_attentions=True)\n",
    "\n",
    "# load the dataset \n",
    "ds = load_dataset(\"hw2942/financial-news-sentiment\")\n",
    "\n",
    "def get_acc(test_dataset,model,tokenizer,device='cuda'):\n",
    "    total = 0\n",
    "    right = 0\n",
    "    model.to(device)\n",
    "    for data in test_dataset:\n",
    "        inputs = tokenizer(data['Title'],return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs)\n",
    "        total  += 1\n",
    "        if torch.max(outputs[0][0].softmax(0),dim=0).indices==data['labels']:\n",
    "            right += 1\n",
    "    return right/total\n",
    "\n",
    "train_dataset = ds['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'labels'],\n",
       "    num_rows: 2329\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[6.2043e-04, 2.6172e-03, 8.9522e-04,  ..., 1.2980e-03,\n",
       "            1.7472e-03, 9.6261e-01],\n",
       "           [3.0339e-02, 7.0712e-02, 2.1275e-02,  ..., 2.1203e-02,\n",
       "            4.5651e-02, 6.6954e-03],\n",
       "           [3.1019e-02, 2.7490e-02, 9.1975e-03,  ..., 4.5575e-02,\n",
       "            1.7051e-02, 6.8314e-03],\n",
       "           ...,\n",
       "           [4.5614e-02, 3.8275e-02, 3.3295e-02,  ..., 5.8891e-02,\n",
       "            2.1311e-02, 1.8636e-02],\n",
       "           [3.5039e-02, 1.5726e-02, 4.6151e-02,  ..., 6.7893e-02,\n",
       "            6.2155e-03, 6.7138e-03],\n",
       "           [9.1244e-01, 5.0548e-03, 1.4844e-03,  ..., 5.4512e-03,\n",
       "            4.8061e-03, 2.7211e-03]],\n",
       " \n",
       "          [[2.2416e-02, 1.3964e-02, 1.0259e-02,  ..., 8.6136e-03,\n",
       "            4.5173e-03, 7.3297e-01],\n",
       "           [1.0430e-01, 4.7057e-02, 3.1528e-01,  ..., 2.3341e-04,\n",
       "            7.2508e-03, 8.3452e-03],\n",
       "           [3.3869e-02, 4.7658e-01, 8.4863e-03,  ..., 3.8979e-04,\n",
       "            4.2075e-04, 2.7585e-02],\n",
       "           ...,\n",
       "           [2.6577e-02, 1.9967e-03, 1.3971e-03,  ..., 1.9350e-02,\n",
       "            2.8812e-02, 4.2120e-02],\n",
       "           [9.3660e-02, 8.7783e-03, 3.4070e-03,  ..., 4.0842e-01,\n",
       "            3.5635e-02, 2.4212e-01],\n",
       "           [5.5091e-03, 4.0392e-06, 1.3031e-05,  ..., 2.3731e-06,\n",
       "            7.7740e-06, 9.9445e-01]],\n",
       " \n",
       "          [[7.0329e-01, 6.5748e-03, 1.4338e-02,  ..., 1.2167e-02,\n",
       "            1.2683e-02, 1.6100e-02],\n",
       "           [7.5525e-01, 1.7563e-02, 1.0136e-02,  ..., 9.4450e-03,\n",
       "            6.5129e-03, 1.6172e-02],\n",
       "           [6.4728e-01, 2.1073e-02, 1.0803e-01,  ..., 1.3424e-02,\n",
       "            9.9957e-03, 9.1611e-03],\n",
       "           ...,\n",
       "           [1.0018e-01, 6.0616e-02, 2.2776e-02,  ..., 6.6616e-02,\n",
       "            5.7477e-03, 6.2395e-03],\n",
       "           [5.5635e-02, 4.8653e-02, 2.6951e-02,  ..., 8.8789e-03,\n",
       "            2.4144e-02, 5.5895e-03],\n",
       "           [3.4319e-01, 2.7126e-02, 8.5846e-03,  ..., 9.9326e-03,\n",
       "            5.6810e-03, 2.6816e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.6312e-01, 7.2885e-04, 1.2221e-03,  ..., 1.2074e-03,\n",
       "            1.6908e-03, 1.0999e-02],\n",
       "           [9.0140e-02, 6.1300e-02, 2.5058e-02,  ..., 1.2649e-02,\n",
       "            9.2214e-03, 3.0042e-02],\n",
       "           [1.2199e-01, 4.8641e-02, 1.7830e-02,  ..., 6.5411e-02,\n",
       "            3.0485e-02, 4.9056e-02],\n",
       "           ...,\n",
       "           [1.3066e-01, 3.3334e-02, 7.7133e-02,  ..., 8.7384e-02,\n",
       "            3.4283e-02, 4.4496e-02],\n",
       "           [1.0789e-01, 1.4525e-02, 3.5075e-02,  ..., 2.1042e-02,\n",
       "            1.0329e-02, 2.9344e-02],\n",
       "           [7.9892e-01, 1.6379e-03, 2.6289e-03,  ..., 3.9227e-03,\n",
       "            1.8739e-03, 1.4852e-01]],\n",
       " \n",
       "          [[9.4099e-01, 2.3810e-03, 1.8997e-03,  ..., 2.9731e-03,\n",
       "            1.4038e-03, 5.0936e-03],\n",
       "           [1.3701e-02, 3.0335e-02, 2.8639e-01,  ..., 2.4403e-03,\n",
       "            2.1990e-03, 2.3828e-02],\n",
       "           [7.1634e-02, 2.0541e-01, 6.5671e-02,  ..., 1.5891e-02,\n",
       "            2.7714e-03, 1.5762e-02],\n",
       "           ...,\n",
       "           [2.3833e-02, 1.2438e-03, 1.8269e-02,  ..., 2.7355e-02,\n",
       "            1.0931e-01, 1.7845e-01],\n",
       "           [5.0432e-03, 8.6806e-03, 7.1158e-03,  ..., 5.4427e-01,\n",
       "            1.2017e-02, 1.6043e-01],\n",
       "           [8.8272e-01, 2.8675e-04, 5.9453e-04,  ..., 1.3623e-02,\n",
       "            2.7750e-03, 9.2067e-02]],\n",
       " \n",
       "          [[9.3131e-01, 3.0844e-03, 1.1559e-03,  ..., 2.7699e-03,\n",
       "            3.3020e-03, 3.5952e-03],\n",
       "           [5.6876e-02, 3.4772e-02, 2.1383e-01,  ..., 6.0731e-03,\n",
       "            7.2735e-03, 9.6313e-03],\n",
       "           [3.8160e-02, 3.7636e-01, 2.8223e-02,  ..., 9.0513e-03,\n",
       "            5.9038e-03, 8.5072e-03],\n",
       "           ...,\n",
       "           [3.8915e-02, 2.2808e-02, 1.2532e-02,  ..., 3.4947e-02,\n",
       "            9.7645e-02, 1.3834e-01],\n",
       "           [4.0390e-02, 2.9579e-02, 1.4175e-02,  ..., 7.5658e-02,\n",
       "            2.7254e-02, 3.1304e-01],\n",
       "           [7.0269e-01, 1.1741e-03, 1.4932e-03,  ..., 2.3990e-02,\n",
       "            5.6134e-02, 1.8552e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.6329e-01, 1.7640e-02, 1.2812e-02,  ..., 1.1879e-02,\n",
       "            1.1829e-02, 5.5437e-02],\n",
       "           [6.7246e-01, 2.4664e-03, 3.2126e-03,  ..., 2.1479e-05,\n",
       "            1.6853e-01, 6.0165e-04],\n",
       "           [2.2273e-02, 9.6985e-01, 1.5423e-04,  ..., 1.7053e-05,\n",
       "            1.0052e-06, 1.0169e-03],\n",
       "           ...,\n",
       "           [5.3531e-01, 6.0274e-06, 5.8463e-05,  ..., 4.0152e-03,\n",
       "            1.7713e-01, 7.9418e-03],\n",
       "           [2.5557e-02, 4.1419e-05, 2.6535e-04,  ..., 9.4917e-01,\n",
       "            6.7660e-03, 5.3351e-03],\n",
       "           [9.8904e-01, 1.4075e-06, 3.6009e-04,  ..., 8.3621e-05,\n",
       "            7.9781e-03, 1.3527e-03]],\n",
       " \n",
       "          [[9.3014e-02, 3.1621e-02, 3.7763e-02,  ..., 3.8536e-02,\n",
       "            4.0394e-02, 4.4288e-02],\n",
       "           [1.2937e-01, 3.6423e-04, 2.4618e-02,  ..., 9.9059e-03,\n",
       "            4.0750e-02, 1.1152e-02],\n",
       "           [6.2737e-02, 9.2560e-02, 1.9188e-03,  ..., 2.7533e-03,\n",
       "            2.7068e-02, 1.0499e-02],\n",
       "           ...,\n",
       "           [1.4025e-01, 1.5132e-03, 2.7283e-02,  ..., 2.8575e-04,\n",
       "            5.1286e-02, 1.7805e-02],\n",
       "           [1.1686e-01, 5.5110e-02, 5.7476e-02,  ..., 8.3319e-02,\n",
       "            2.0624e-03, 2.4074e-02],\n",
       "           [3.9093e-01, 8.8152e-03, 7.0994e-03,  ..., 1.0200e-02,\n",
       "            1.7788e-02, 3.1392e-01]],\n",
       " \n",
       "          [[8.9092e-01, 5.4951e-03, 1.1209e-03,  ..., 4.9597e-03,\n",
       "            5.6596e-03, 2.2012e-02],\n",
       "           [1.1515e-01, 1.2898e-01, 1.1938e-02,  ..., 9.9467e-04,\n",
       "            1.9644e-03, 1.4470e-02],\n",
       "           [1.3460e-01, 1.2368e-01, 6.4026e-02,  ..., 1.1537e-02,\n",
       "            4.1494e-03, 5.8104e-02],\n",
       "           ...,\n",
       "           [2.7736e-02, 1.9261e-03, 2.5284e-03,  ..., 3.6065e-02,\n",
       "            7.7169e-03, 3.6336e-02],\n",
       "           [1.2819e-01, 7.4466e-03, 1.2018e-02,  ..., 2.9157e-01,\n",
       "            7.2868e-02, 1.9031e-02],\n",
       "           [9.4987e-01, 1.7236e-03, 1.9885e-04,  ..., 3.4296e-03,\n",
       "            7.9175e-03, 1.1335e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.9649e-01, 1.1936e-03, 4.3939e-03,  ..., 4.7052e-03,\n",
       "            6.6763e-03, 1.2835e-02],\n",
       "           [2.5393e-01, 2.4125e-02, 4.8540e-02,  ..., 9.9256e-04,\n",
       "            9.7305e-03, 1.3744e-02],\n",
       "           [1.0491e-01, 2.7079e-03, 5.5384e-02,  ..., 4.4226e-03,\n",
       "            6.2976e-03, 8.3545e-03],\n",
       "           ...,\n",
       "           [7.4676e-01, 1.6776e-04, 9.9512e-04,  ..., 1.4243e-01,\n",
       "            3.7613e-02, 4.4936e-02],\n",
       "           [6.0649e-01, 1.6570e-03, 7.2138e-03,  ..., 1.9620e-02,\n",
       "            2.4488e-01, 5.7122e-02],\n",
       "           [9.9876e-01, 2.4878e-06, 1.3381e-05,  ..., 5.6412e-05,\n",
       "            1.6168e-04, 4.0134e-04]],\n",
       " \n",
       "          [[7.1510e-01, 1.5662e-02, 1.4217e-02,  ..., 9.5790e-03,\n",
       "            1.0153e-02, 1.7487e-02],\n",
       "           [2.9815e-01, 3.2052e-03, 7.2472e-02,  ..., 6.9685e-04,\n",
       "            1.0322e-02, 2.2555e-02],\n",
       "           [1.2102e-01, 1.4133e-02, 2.8134e-02,  ..., 1.3583e-03,\n",
       "            1.6044e-02, 3.3545e-02],\n",
       "           ...,\n",
       "           [2.4329e-01, 1.0938e-01, 7.9434e-03,  ..., 1.0165e-02,\n",
       "            3.1731e-03, 1.1666e-02],\n",
       "           [6.7250e-01, 4.6665e-03, 1.2339e-02,  ..., 4.9308e-03,\n",
       "            6.4865e-04, 6.2777e-03],\n",
       "           [1.5014e-01, 3.4178e-02, 2.7593e-02,  ..., 1.0042e-02,\n",
       "            1.4683e-03, 4.3514e-01]],\n",
       " \n",
       "          [[9.3780e-01, 1.2723e-03, 1.5351e-03,  ..., 4.3230e-03,\n",
       "            1.8163e-03, 4.0603e-03],\n",
       "           [2.5917e-01, 9.4075e-03, 1.0681e-02,  ..., 5.7923e-02,\n",
       "            1.1908e-02, 1.0166e-02],\n",
       "           [1.0720e-01, 1.8852e-02, 1.0820e-02,  ..., 8.8494e-03,\n",
       "            8.7582e-02, 9.1686e-03],\n",
       "           ...,\n",
       "           [1.7637e-01, 1.9694e-03, 7.0992e-02,  ..., 6.5048e-02,\n",
       "            1.0321e-01, 1.0604e-03],\n",
       "           [4.0605e-01, 7.9048e-03, 6.1698e-02,  ..., 2.7248e-02,\n",
       "            2.8466e-02, 6.9575e-03],\n",
       "           [9.6428e-01, 4.6604e-04, 1.1487e-03,  ..., 3.9847e-03,\n",
       "            1.3112e-03, 5.5353e-03]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.0831e-01, 1.8235e-03, 2.7445e-03,  ..., 1.6598e-03,\n",
       "            8.7992e-04, 1.5462e-01],\n",
       "           [5.6487e-01, 4.9350e-03, 1.0090e-02,  ..., 3.4771e-03,\n",
       "            4.0974e-03, 2.4754e-01],\n",
       "           [7.9231e-01, 2.0584e-03, 3.8638e-03,  ..., 4.5109e-03,\n",
       "            6.1884e-02, 8.4380e-02],\n",
       "           ...,\n",
       "           [7.4473e-01, 7.5445e-04, 1.0168e-03,  ..., 1.1114e-02,\n",
       "            2.3914e-02, 1.4635e-01],\n",
       "           [2.1072e-01, 5.9785e-02, 4.0983e-03,  ..., 4.3151e-03,\n",
       "            2.1776e-02, 4.5887e-02],\n",
       "           [9.4511e-01, 2.1644e-04, 4.9935e-04,  ..., 2.2283e-04,\n",
       "            1.2893e-04, 4.7220e-02]],\n",
       " \n",
       "          [[9.0213e-01, 3.8508e-04, 1.3124e-03,  ..., 4.7000e-03,\n",
       "            1.7957e-03, 5.7440e-02],\n",
       "           [9.3268e-03, 2.4445e-04, 9.8201e-01,  ..., 7.8083e-09,\n",
       "            8.5057e-06, 1.4193e-03],\n",
       "           [8.3223e-01, 2.6613e-03, 5.3720e-03,  ..., 5.9653e-05,\n",
       "            3.0533e-05, 1.3603e-01],\n",
       "           ...,\n",
       "           [8.4790e-04, 6.6418e-09, 3.1311e-06,  ..., 6.2174e-06,\n",
       "            9.9809e-01, 3.0497e-04],\n",
       "           [8.8543e-01, 1.1890e-04, 5.5810e-07,  ..., 5.0336e-04,\n",
       "            1.8787e-02, 9.2663e-02],\n",
       "           [8.3908e-01, 3.6154e-04, 2.6428e-03,  ..., 1.5511e-02,\n",
       "            4.8238e-03, 9.7614e-02]],\n",
       " \n",
       "          [[3.5117e-01, 1.7762e-04, 3.0045e-04,  ..., 4.2031e-04,\n",
       "            1.0959e-04, 6.4349e-01],\n",
       "           [3.6422e-01, 8.2706e-03, 6.3380e-02,  ..., 3.3880e-02,\n",
       "            2.6516e-02, 4.0640e-02],\n",
       "           [4.2521e-01, 1.2574e-02, 6.1863e-02,  ..., 3.9583e-02,\n",
       "            9.5018e-03, 1.1662e-01],\n",
       "           ...,\n",
       "           [8.9189e-02, 7.6786e-03, 9.5673e-03,  ..., 1.8992e-02,\n",
       "            2.7243e-02, 8.9961e-03],\n",
       "           [4.0045e-01, 6.9825e-03, 5.1901e-03,  ..., 2.9682e-01,\n",
       "            7.6082e-03, 1.1309e-01],\n",
       "           [8.1407e-01, 8.5547e-05, 5.5127e-05,  ..., 1.8120e-04,\n",
       "            9.1654e-05, 1.8384e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.6728e-01, 3.6012e-03, 2.7419e-03,  ..., 6.2824e-03,\n",
       "            3.3047e-03, 5.1070e-02],\n",
       "           [2.1592e-01, 1.6838e-02, 2.0944e-01,  ..., 1.0679e-03,\n",
       "            4.4516e-04, 6.3860e-02],\n",
       "           [2.6566e-01, 3.4785e-03, 3.8946e-02,  ..., 1.6317e-04,\n",
       "            5.2134e-05, 1.9443e-02],\n",
       "           ...,\n",
       "           [6.2889e-01, 2.1145e-04, 9.5697e-05,  ..., 6.1054e-02,\n",
       "            9.0956e-02, 1.8910e-01],\n",
       "           [5.7484e-01, 1.4489e-04, 2.2115e-03,  ..., 6.1297e-02,\n",
       "            1.8168e-01, 1.5210e-01],\n",
       "           [9.5077e-01, 1.0231e-03, 5.8659e-04,  ..., 1.8688e-03,\n",
       "            1.4224e-03, 3.1118e-02]],\n",
       " \n",
       "          [[9.0290e-01, 3.4559e-03, 2.3501e-03,  ..., 2.6846e-03,\n",
       "            1.0271e-03, 3.9019e-02],\n",
       "           [7.6786e-01, 2.6809e-02, 2.2436e-02,  ..., 2.0738e-03,\n",
       "            7.3650e-05, 1.5512e-01],\n",
       "           [5.6677e-01, 5.8843e-02, 1.2494e-01,  ..., 2.1111e-04,\n",
       "            1.3643e-03, 2.2971e-01],\n",
       "           ...,\n",
       "           [4.3958e-02, 4.3619e-05, 5.9364e-05,  ..., 2.9694e-02,\n",
       "            9.7340e-03, 4.1084e-03],\n",
       "           [2.0713e-02, 6.5693e-05, 9.1432e-06,  ..., 5.8464e-02,\n",
       "            2.6354e-02, 4.6226e-03],\n",
       "           [9.4525e-01, 1.8064e-03, 1.1954e-03,  ..., 3.3248e-03,\n",
       "            7.9168e-04, 3.3982e-02]],\n",
       " \n",
       "          [[8.9412e-01, 2.0430e-03, 1.6421e-03,  ..., 1.5256e-03,\n",
       "            2.6374e-03, 5.4082e-02],\n",
       "           [1.3520e-01, 1.0320e-04, 8.4826e-01,  ..., 5.8167e-05,\n",
       "            4.5769e-05, 7.6724e-03],\n",
       "           [7.6729e-01, 6.5362e-03, 2.0645e-03,  ..., 1.4726e-05,\n",
       "            5.4184e-05, 1.4391e-01],\n",
       "           ...,\n",
       "           [3.1136e-01, 3.5509e-06, 3.3739e-05,  ..., 2.9110e-03,\n",
       "            6.4564e-01, 1.2541e-02],\n",
       "           [7.9280e-01, 7.2669e-04, 6.9169e-07,  ..., 9.0299e-02,\n",
       "            8.8044e-04, 1.0067e-01],\n",
       "           [3.0698e-01, 6.5609e-05, 6.6571e-05,  ..., 1.5759e-05,\n",
       "            2.4776e-04, 6.8908e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.3658e-02, 1.1895e-03, 5.1595e-04,  ..., 7.0314e-03,\n",
       "            5.9926e-03, 8.9961e-01],\n",
       "           [7.6009e-02, 1.4463e-02, 2.0403e-02,  ..., 3.5591e-02,\n",
       "            4.3540e-03, 6.0552e-01],\n",
       "           [8.7942e-02, 3.3397e-02, 1.3531e-02,  ..., 5.1168e-02,\n",
       "            5.8428e-03, 2.7818e-01],\n",
       "           ...,\n",
       "           [2.6985e-02, 2.8487e-03, 1.3331e-03,  ..., 2.0673e-01,\n",
       "            7.6888e-02, 2.8025e-01],\n",
       "           [3.8230e-03, 3.3976e-04, 1.0208e-03,  ..., 8.2117e-01,\n",
       "            8.7119e-03, 3.2510e-02],\n",
       "           [2.0065e-03, 1.2065e-04, 4.1007e-05,  ..., 9.7033e-05,\n",
       "            1.6073e-04, 9.9522e-01]],\n",
       " \n",
       "          [[4.3666e-02, 4.0174e-02, 4.4595e-02,  ..., 3.2392e-02,\n",
       "            1.8135e-02, 5.1256e-02],\n",
       "           [1.4449e-01, 1.6694e-02, 8.4846e-02,  ..., 1.0140e-02,\n",
       "            2.4841e-03, 3.0704e-01],\n",
       "           [8.6134e-02, 4.2086e-02, 8.2595e-02,  ..., 2.5394e-02,\n",
       "            4.4377e-03, 4.6236e-01],\n",
       "           ...,\n",
       "           [2.5759e-01, 5.1838e-03, 1.1481e-03,  ..., 5.4970e-03,\n",
       "            1.7202e-02, 6.2935e-01],\n",
       "           [8.5317e-02, 9.9431e-03, 1.4367e-02,  ..., 3.9523e-02,\n",
       "            1.0119e-02, 6.2677e-01],\n",
       "           [5.8604e-02, 2.1287e-03, 1.3905e-03,  ..., 4.4761e-03,\n",
       "            3.6683e-03, 8.8541e-01]],\n",
       " \n",
       "          [[3.0860e-01, 2.8595e-02, 1.8422e-02,  ..., 6.2770e-03,\n",
       "            1.6254e-02, 1.7554e-01],\n",
       "           [9.0643e-02, 2.9619e-01, 1.1429e-02,  ..., 2.9578e-04,\n",
       "            2.5673e-04, 6.1081e-02],\n",
       "           [1.6602e-01, 2.0927e-02, 1.8796e-01,  ..., 1.1771e-03,\n",
       "            1.9108e-03, 1.9091e-01],\n",
       "           ...,\n",
       "           [1.7292e-02, 1.1694e-04, 3.6556e-04,  ..., 9.4534e-01,\n",
       "            8.7738e-03, 1.2380e-02],\n",
       "           [3.2099e-01, 6.5404e-04, 7.5248e-04,  ..., 9.1977e-02,\n",
       "            3.4515e-01, 1.6928e-01],\n",
       "           [2.7773e-01, 2.8617e-02, 1.6107e-02,  ..., 1.9310e-02,\n",
       "            1.8959e-02, 1.7396e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[4.1719e-02, 6.4526e-03, 3.1056e-03,  ..., 2.3292e-03,\n",
       "            1.6298e-03, 8.8241e-01],\n",
       "           [7.4004e-02, 5.0343e-03, 3.7614e-03,  ..., 5.7447e-04,\n",
       "            8.7548e-04, 8.9127e-01],\n",
       "           [2.3519e-02, 2.3507e-02, 3.1751e-03,  ..., 2.2751e-04,\n",
       "            5.1726e-04, 9.1603e-01],\n",
       "           ...,\n",
       "           [7.4807e-02, 1.0809e-03, 8.8235e-04,  ..., 5.4432e-02,\n",
       "            5.2578e-02, 4.9772e-01],\n",
       "           [5.8372e-02, 1.2918e-03, 6.0847e-04,  ..., 2.0520e-01,\n",
       "            3.3914e-02, 2.0314e-01],\n",
       "           [1.5084e-02, 4.2142e-03, 2.1604e-03,  ..., 9.7067e-04,\n",
       "            1.2400e-03, 9.5341e-01]],\n",
       " \n",
       "          [[5.1748e-02, 2.7503e-03, 3.8341e-04,  ..., 6.7087e-04,\n",
       "            2.4163e-04, 9.3118e-01],\n",
       "           [2.5524e-01, 2.3063e-01, 2.6796e-04,  ..., 1.2667e-04,\n",
       "            1.6614e-04, 1.7472e-01],\n",
       "           [2.3388e-01, 1.5523e-03, 1.4761e-01,  ..., 1.3172e-03,\n",
       "            1.4395e-04, 5.8371e-02],\n",
       "           ...,\n",
       "           [1.3738e-01, 3.0981e-04, 5.7181e-04,  ..., 2.2188e-01,\n",
       "            6.7440e-04, 6.1720e-01],\n",
       "           [1.7024e-01, 2.5209e-04, 4.2492e-04,  ..., 1.0604e-03,\n",
       "            5.6240e-02, 7.5999e-01],\n",
       "           [2.3265e-02, 7.4737e-04, 3.8097e-04,  ..., 2.5787e-04,\n",
       "            1.3704e-04, 9.6804e-01]],\n",
       " \n",
       "          [[6.0364e-02, 6.3705e-04, 3.5644e-04,  ..., 2.0235e-03,\n",
       "            5.0233e-03, 8.9706e-01],\n",
       "           [1.2420e-01, 4.7718e-03, 9.5098e-02,  ..., 1.3356e-03,\n",
       "            4.5575e-04, 2.8307e-01],\n",
       "           [5.7984e-02, 2.6141e-02, 1.9218e-02,  ..., 6.6951e-04,\n",
       "            5.8840e-04, 4.0189e-01],\n",
       "           ...,\n",
       "           [6.2324e-02, 3.6032e-04, 1.1160e-04,  ..., 2.7151e-02,\n",
       "            7.3735e-02, 7.9369e-01],\n",
       "           [5.5475e-02, 2.0504e-04, 4.6138e-05,  ..., 4.3210e-02,\n",
       "            4.8360e-02, 7.7662e-01],\n",
       "           [2.7740e-02, 6.8115e-04, 3.3643e-04,  ..., 1.0921e-03,\n",
       "            4.5631e-03, 9.4205e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.4631e-02, 1.7243e-02, 1.9244e-02,  ..., 5.1861e-02,\n",
       "            1.4936e-02, 1.3237e-01],\n",
       "           [6.6042e-03, 1.6682e-02, 3.9733e-02,  ..., 2.5088e-02,\n",
       "            9.1897e-03, 2.1376e-01],\n",
       "           [6.6183e-03, 1.1713e-02, 5.9939e-02,  ..., 2.1545e-02,\n",
       "            9.0719e-03, 5.7113e-02],\n",
       "           ...,\n",
       "           [1.2027e-02, 1.3257e-02, 5.8894e-03,  ..., 1.9081e-01,\n",
       "            2.2267e-02, 2.0514e-01],\n",
       "           [1.6456e-02, 1.7477e-02, 1.7056e-02,  ..., 1.2066e-01,\n",
       "            2.3520e-02, 1.4963e-01],\n",
       "           [1.3503e-02, 3.4820e-04, 2.7581e-04,  ..., 7.9407e-04,\n",
       "            2.2699e-04, 9.7616e-01]],\n",
       " \n",
       "          [[1.4941e-01, 5.8787e-02, 6.6495e-02,  ..., 6.4897e-03,\n",
       "            3.7020e-03, 3.1043e-01],\n",
       "           [1.7319e-01, 2.2059e-02, 3.5831e-02,  ..., 9.7068e-04,\n",
       "            2.7191e-04, 3.2602e-01],\n",
       "           [1.5712e-01, 8.9094e-03, 7.1288e-02,  ..., 1.6078e-03,\n",
       "            4.4145e-03, 1.1291e-01],\n",
       "           ...,\n",
       "           [8.6086e-02, 2.0848e-03, 4.5829e-03,  ..., 2.0507e-03,\n",
       "            6.1298e-03, 6.1917e-01],\n",
       "           [1.1531e-01, 1.6568e-03, 1.3921e-02,  ..., 6.5531e-03,\n",
       "            1.5362e-03, 7.1353e-01],\n",
       "           [8.2575e-03, 5.9298e-04, 7.4363e-04,  ..., 2.1062e-04,\n",
       "            8.6198e-04, 9.6536e-01]],\n",
       " \n",
       "          [[5.7086e-02, 4.2041e-03, 6.8961e-03,  ..., 1.8197e-03,\n",
       "            2.7930e-03, 8.1644e-01],\n",
       "           [1.5210e-02, 2.2192e-01, 3.7289e-07,  ..., 1.5049e-08,\n",
       "            1.0400e-07, 4.0554e-02],\n",
       "           [2.1651e-03, 8.9954e-08, 1.0540e-01,  ..., 1.3573e-06,\n",
       "            9.2674e-09, 9.9292e-03],\n",
       "           ...,\n",
       "           [5.2480e-03, 5.8179e-08, 7.6552e-07,  ..., 9.3226e-01,\n",
       "            3.3234e-07, 6.2372e-02],\n",
       "           [5.1285e-03, 1.4696e-06, 1.1614e-07,  ..., 2.4165e-06,\n",
       "            1.1399e-02, 9.8340e-01],\n",
       "           [9.6878e-03, 1.6556e-04, 4.2364e-04,  ..., 2.5440e-04,\n",
       "            1.4874e-04, 9.8447e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[6.9146e-02, 2.3288e-02, 3.4089e-02,  ..., 6.9838e-02,\n",
       "            6.8676e-02, 9.6205e-02],\n",
       "           [1.3298e-01, 2.4965e-02, 4.2307e-02,  ..., 3.9390e-02,\n",
       "            6.8578e-02, 1.4883e-01],\n",
       "           [5.9645e-02, 5.1899e-02, 1.4550e-01,  ..., 1.5796e-02,\n",
       "            1.5774e-02, 7.0845e-02],\n",
       "           ...,\n",
       "           [2.7684e-02, 9.3272e-04, 5.7410e-03,  ..., 1.8726e-01,\n",
       "            9.4526e-02, 2.6427e-02],\n",
       "           [4.5425e-03, 5.5359e-05, 2.2066e-03,  ..., 1.3068e-01,\n",
       "            1.7890e-02, 9.0412e-03],\n",
       "           [1.2656e-01, 3.2105e-02, 2.1369e-02,  ..., 4.2057e-02,\n",
       "            2.6446e-02, 1.1006e-01]],\n",
       " \n",
       "          [[6.7743e-03, 2.8035e-03, 1.6104e-02,  ..., 4.0411e-03,\n",
       "            2.2914e-03, 9.0156e-01],\n",
       "           [5.8599e-02, 5.7523e-02, 2.7653e-02,  ..., 1.8124e-04,\n",
       "            3.9811e-04, 7.0371e-01],\n",
       "           [3.0381e-02, 6.7800e-01, 1.4334e-02,  ..., 6.6568e-04,\n",
       "            9.8762e-05, 1.3672e-01],\n",
       "           ...,\n",
       "           [2.9473e-02, 6.9419e-04, 4.0090e-03,  ..., 1.0780e-01,\n",
       "            5.2319e-03, 3.3535e-01],\n",
       "           [2.6800e-02, 3.7377e-04, 8.2620e-03,  ..., 6.9927e-01,\n",
       "            6.1356e-04, 5.2542e-02],\n",
       "           [3.8032e-04, 2.3016e-04, 2.0021e-03,  ..., 1.4384e-03,\n",
       "            2.8665e-04, 9.8868e-01]],\n",
       " \n",
       "          [[4.5100e-02, 4.3134e-03, 2.1123e-02,  ..., 3.3023e-02,\n",
       "            2.9910e-02, 4.7824e-01],\n",
       "           [3.9468e-02, 6.4091e-03, 7.4783e-02,  ..., 3.4016e-03,\n",
       "            1.6568e-03, 6.0009e-01],\n",
       "           [6.9019e-02, 6.6289e-03, 4.0443e-04,  ..., 1.9323e-02,\n",
       "            1.1302e-02, 5.3101e-01],\n",
       "           ...,\n",
       "           [8.0087e-02, 2.8279e-03, 6.1872e-03,  ..., 2.4567e-02,\n",
       "            6.0752e-02, 5.0781e-01],\n",
       "           [9.2470e-02, 4.8492e-03, 2.1305e-02,  ..., 1.5849e-02,\n",
       "            2.8556e-03, 5.1287e-01],\n",
       "           [5.8188e-03, 7.2224e-04, 8.7313e-04,  ..., 1.5217e-03,\n",
       "            1.0980e-03, 9.7044e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[5.9840e-04, 3.5502e-03, 4.2247e-03,  ..., 3.3452e-03,\n",
       "            2.8558e-03, 8.1640e-01],\n",
       "           [1.2674e-01, 3.8730e-02, 2.0781e-02,  ..., 2.2430e-06,\n",
       "            6.2520e-06, 7.2488e-01],\n",
       "           [1.3198e-02, 3.1386e-03, 2.4652e-01,  ..., 3.5819e-06,\n",
       "            2.7215e-06, 4.1193e-01],\n",
       "           ...,\n",
       "           [2.2590e-02, 9.0798e-07, 6.1761e-07,  ..., 9.2709e-02,\n",
       "            1.6943e-02, 8.6220e-01],\n",
       "           [7.7577e-03, 3.0116e-06, 1.4370e-06,  ..., 3.9342e-02,\n",
       "            1.8436e-02, 9.2992e-01],\n",
       "           [2.6119e-04, 7.5227e-03, 4.2259e-03,  ..., 1.1540e-03,\n",
       "            4.0594e-03, 9.0621e-01]],\n",
       " \n",
       "          [[2.7470e-02, 2.4587e-02, 8.8286e-02,  ..., 7.3745e-03,\n",
       "            7.4536e-03, 5.5476e-01],\n",
       "           [6.6129e-02, 1.7789e-02, 5.4621e-02,  ..., 2.1748e-03,\n",
       "            2.9996e-03, 5.9548e-01],\n",
       "           [5.2841e-02, 6.4949e-03, 1.9282e-03,  ..., 1.4337e-02,\n",
       "            3.2880e-02, 4.6686e-01],\n",
       "           ...,\n",
       "           [3.6972e-02, 1.0846e-03, 5.7824e-03,  ..., 1.6402e-02,\n",
       "            1.6305e-02, 2.1221e-01],\n",
       "           [3.1829e-02, 6.3175e-04, 7.2537e-03,  ..., 1.7615e-02,\n",
       "            1.7892e-03, 2.2537e-01],\n",
       "           [1.1638e-02, 6.7524e-03, 7.2768e-03,  ..., 7.1174e-03,\n",
       "            1.0184e-02, 8.0182e-01]],\n",
       " \n",
       "          [[1.0208e-02, 7.5338e-03, 1.7744e-01,  ..., 5.3005e-03,\n",
       "            7.6135e-04, 6.0231e-02],\n",
       "           [1.1207e-02, 1.6643e-02, 1.4885e-01,  ..., 3.5948e-03,\n",
       "            3.0560e-03, 5.0240e-01],\n",
       "           [3.7982e-02, 1.6627e-02, 2.5628e-01,  ..., 4.3024e-03,\n",
       "            2.1069e-03, 1.0442e-01],\n",
       "           ...,\n",
       "           [6.5100e-02, 3.5933e-04, 8.9296e-04,  ..., 2.1306e-01,\n",
       "            5.1197e-02, 3.6047e-01],\n",
       "           [2.3430e-02, 4.2829e-05, 6.9621e-04,  ..., 9.3204e-02,\n",
       "            3.5562e-02, 6.2800e-01],\n",
       "           [1.3809e-04, 3.5410e-05, 2.9335e-04,  ..., 6.3936e-05,\n",
       "            4.0745e-05, 9.9825e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.4845e-03, 9.7111e-02, 4.5880e-02,  ..., 2.6095e-03,\n",
       "            4.7653e-03, 5.4422e-01],\n",
       "           [7.3126e-03, 1.6048e-02, 5.3502e-02,  ..., 3.4744e-03,\n",
       "            2.8495e-03, 6.2102e-01],\n",
       "           [1.6231e-02, 3.7238e-03, 1.2326e-02,  ..., 5.9328e-03,\n",
       "            6.0624e-03, 8.0901e-01],\n",
       "           ...,\n",
       "           [1.8626e-02, 9.0248e-04, 8.4568e-04,  ..., 1.2929e-01,\n",
       "            7.5244e-02, 5.8028e-01],\n",
       "           [9.8710e-03, 4.8922e-04, 1.1991e-03,  ..., 3.5279e-01,\n",
       "            2.2640e-01, 2.4789e-01],\n",
       "           [2.2394e-03, 3.7561e-03, 4.8514e-03,  ..., 3.0439e-03,\n",
       "            3.1816e-03, 9.2997e-01]],\n",
       " \n",
       "          [[8.1805e-03, 1.5032e-02, 9.7996e-03,  ..., 2.8127e-03,\n",
       "            4.7042e-03, 8.7039e-01],\n",
       "           [2.7240e-02, 8.4523e-03, 5.6884e-01,  ..., 4.3703e-06,\n",
       "            1.0207e-06, 3.4801e-01],\n",
       "           [9.9825e-03, 8.1005e-03, 1.0225e-02,  ..., 6.3525e-05,\n",
       "            7.3927e-06, 7.6862e-01],\n",
       "           ...,\n",
       "           [3.5732e-02, 1.3537e-06, 6.9209e-06,  ..., 1.7562e-02,\n",
       "            1.1470e-01, 8.1798e-01],\n",
       "           [5.6754e-03, 4.9283e-05, 2.6605e-06,  ..., 9.3322e-02,\n",
       "            1.5093e-02, 8.7524e-01],\n",
       "           [3.0710e-03, 4.9851e-03, 8.5110e-03,  ..., 3.2954e-03,\n",
       "            7.0733e-03, 9.1506e-01]],\n",
       " \n",
       "          [[6.6906e-03, 1.5690e-03, 1.8862e-03,  ..., 5.0830e-03,\n",
       "            1.0620e-02, 8.9985e-01],\n",
       "           [2.1224e-02, 1.3664e-02, 5.1196e-03,  ..., 2.4808e-06,\n",
       "            3.6923e-05, 8.8052e-01],\n",
       "           [2.7134e-03, 3.3071e-01, 7.1013e-04,  ..., 2.8353e-06,\n",
       "            3.6637e-06, 1.7916e-02],\n",
       "           ...,\n",
       "           [1.3853e-03, 4.2380e-06, 1.3384e-04,  ..., 1.0067e-03,\n",
       "            1.0354e-02, 9.2606e-01],\n",
       "           [7.5433e-03, 7.9824e-07, 1.5144e-06,  ..., 9.2706e-01,\n",
       "            3.9722e-03, 5.8290e-02],\n",
       "           [5.3829e-03, 1.2703e-03, 6.5499e-03,  ..., 3.1825e-03,\n",
       "            6.3052e-03, 8.9372e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.5899e-01, 1.1176e-01, 5.2103e-02,  ..., 6.2255e-04,\n",
       "            1.0456e-03, 4.6157e-01],\n",
       "           [2.4216e-02, 2.6766e-03, 2.0704e-02,  ..., 6.6445e-04,\n",
       "            2.2440e-03, 3.3055e-01],\n",
       "           [8.8109e-02, 5.3567e-03, 4.4959e-02,  ..., 5.8913e-04,\n",
       "            6.3464e-04, 2.6325e-01],\n",
       "           ...,\n",
       "           [2.6862e-01, 2.2396e-04, 2.5838e-04,  ..., 2.4875e-02,\n",
       "            1.4415e-02, 6.5411e-01],\n",
       "           [3.3405e-01, 3.7108e-04, 2.9438e-04,  ..., 3.6482e-02,\n",
       "            5.8368e-03, 5.8319e-01],\n",
       "           [9.1375e-03, 1.3766e-03, 1.0656e-03,  ..., 5.6617e-03,\n",
       "            3.8905e-03, 9.0916e-01]],\n",
       " \n",
       "          [[3.9425e-03, 7.7628e-03, 6.9310e-03,  ..., 9.8601e-03,\n",
       "            8.1358e-03, 9.0716e-01],\n",
       "           [7.4877e-02, 8.7189e-03, 6.1056e-03,  ..., 5.1097e-05,\n",
       "            6.3694e-05, 9.0027e-01],\n",
       "           [2.0288e-02, 4.2643e-03, 3.0804e-03,  ..., 9.4519e-06,\n",
       "            3.1535e-05, 9.6095e-01],\n",
       "           ...,\n",
       "           [1.4198e-03, 4.3954e-06, 1.6681e-05,  ..., 9.5038e-03,\n",
       "            1.3029e-02, 7.6421e-02],\n",
       "           [4.0231e-04, 4.1516e-06, 1.1579e-05,  ..., 2.8060e-03,\n",
       "            6.5707e-03, 7.6627e-02],\n",
       "           [7.0443e-04, 6.1630e-04, 4.5210e-04,  ..., 2.1236e-03,\n",
       "            2.3955e-03, 9.7954e-01]],\n",
       " \n",
       "          [[7.0658e-03, 4.8714e-03, 4.4122e-03,  ..., 5.2254e-04,\n",
       "            1.6960e-03, 9.7537e-01],\n",
       "           [1.4528e-02, 6.5990e-03, 9.1362e-02,  ..., 1.7633e-07,\n",
       "            4.7345e-06, 8.8249e-01],\n",
       "           [2.1075e-02, 2.2024e-02, 9.6179e-03,  ..., 3.0351e-06,\n",
       "            4.3412e-06, 9.2226e-01],\n",
       "           ...,\n",
       "           [3.5994e-03, 5.7992e-08, 2.5817e-06,  ..., 6.0168e-03,\n",
       "            1.1545e-01, 8.6530e-01],\n",
       "           [5.6577e-03, 6.8831e-06, 2.6207e-06,  ..., 1.2404e-01,\n",
       "            2.6590e-02, 8.3672e-01],\n",
       "           [1.5991e-03, 2.8521e-03, 1.0589e-02,  ..., 3.7004e-03,\n",
       "            3.4699e-03, 9.3522e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4078e-03, 3.6420e-01, 7.6618e-02,  ..., 4.8312e-04,\n",
       "            6.1684e-04, 5.1793e-01],\n",
       "           [2.4886e-02, 1.4034e-02, 3.8108e-03,  ..., 3.3634e-05,\n",
       "            9.2560e-06, 8.1037e-01],\n",
       "           [2.2522e-02, 6.9298e-03, 1.4975e-02,  ..., 9.2043e-06,\n",
       "            6.1818e-05, 9.0715e-01],\n",
       "           ...,\n",
       "           [5.6105e-02, 1.2059e-04, 4.9904e-06,  ..., 1.4295e-01,\n",
       "            2.6727e-02, 7.0901e-01],\n",
       "           [1.2817e-01, 2.3999e-05, 2.3031e-05,  ..., 4.7400e-02,\n",
       "            8.2839e-02, 6.1527e-01],\n",
       "           [1.4112e-03, 1.7794e-02, 1.4820e-02,  ..., 2.1429e-03,\n",
       "            2.6056e-03, 8.7460e-01]],\n",
       " \n",
       "          [[1.1809e-02, 1.2224e-03, 2.1562e-03,  ..., 2.4017e-03,\n",
       "            2.6730e-03, 8.0637e-01],\n",
       "           [2.3761e-03, 3.9715e-03, 6.2083e-03,  ..., 3.1416e-03,\n",
       "            3.4881e-03, 8.7894e-01],\n",
       "           [7.7046e-03, 5.9349e-03, 2.1583e-02,  ..., 3.1388e-03,\n",
       "            3.2058e-03, 8.6713e-01],\n",
       "           ...,\n",
       "           [1.1513e-02, 2.1439e-05, 2.2266e-05,  ..., 1.4475e-02,\n",
       "            4.2478e-03, 9.6182e-01],\n",
       "           [1.3861e-02, 1.0617e-05, 5.0800e-06,  ..., 2.5723e-02,\n",
       "            6.3499e-03, 9.4860e-01],\n",
       "           [8.9509e-04, 4.2519e-04, 4.5650e-04,  ..., 1.1906e-03,\n",
       "            1.0199e-03, 9.6270e-01]],\n",
       " \n",
       "          [[1.9052e-03, 1.1329e-03, 2.2338e-03,  ..., 5.4349e-03,\n",
       "            1.1775e-04, 9.6430e-01],\n",
       "           [1.1870e-02, 2.5205e-02, 1.9801e-02,  ..., 7.9706e-07,\n",
       "            4.9309e-05, 9.2177e-01],\n",
       "           [2.7364e-02, 3.6217e-02, 3.6359e-03,  ..., 4.5937e-06,\n",
       "            3.6345e-06, 9.2699e-01],\n",
       "           ...,\n",
       "           [5.7510e-04, 4.5298e-08, 3.2007e-07,  ..., 1.1611e-02,\n",
       "            3.1428e-02, 9.4261e-01],\n",
       "           [3.3751e-02, 1.0389e-05, 4.8903e-06,  ..., 2.0561e-01,\n",
       "            1.7151e-02, 7.3956e-01],\n",
       "           [3.8981e-04, 6.3165e-04, 1.1920e-02,  ..., 8.5089e-03,\n",
       "            1.2735e-03, 8.5771e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.0766e-03, 9.3989e-05, 5.4355e-05,  ..., 2.4907e-04,\n",
       "            6.4929e-04, 9.9470e-01],\n",
       "           [7.7219e-03, 2.2197e-03, 1.8270e-02,  ..., 4.8656e-06,\n",
       "            8.2705e-05, 9.6881e-01],\n",
       "           [7.7063e-03, 2.6048e-02, 7.9336e-03,  ..., 1.4642e-05,\n",
       "            1.8187e-05, 9.5647e-01],\n",
       "           ...,\n",
       "           [5.4882e-04, 1.8487e-06, 2.8402e-06,  ..., 2.9403e-03,\n",
       "            2.3707e-02, 9.6775e-01],\n",
       "           [2.9705e-04, 2.0189e-05, 1.3335e-05,  ..., 1.6736e-01,\n",
       "            9.3802e-02, 6.8736e-01],\n",
       "           [1.0723e-03, 2.0526e-04, 2.6804e-03,  ..., 2.7773e-04,\n",
       "            7.1350e-04, 9.7760e-01]],\n",
       " \n",
       "          [[5.8584e-04, 2.6297e-02, 3.7900e-03,  ..., 5.8434e-03,\n",
       "            2.2667e-03, 9.0712e-01],\n",
       "           [1.4743e-01, 8.9592e-02, 1.5053e-01,  ..., 1.8022e-05,\n",
       "            3.4503e-05, 5.3312e-01],\n",
       "           [8.1163e-04, 9.5304e-01, 7.2626e-03,  ..., 7.0045e-06,\n",
       "            4.1936e-07, 2.1500e-02],\n",
       "           ...,\n",
       "           [7.9470e-05, 3.5345e-06, 6.1580e-05,  ..., 8.7093e-02,\n",
       "            8.0259e-02, 3.1320e-01],\n",
       "           [1.5793e-05, 1.5109e-06, 9.3624e-06,  ..., 8.3841e-01,\n",
       "            3.2727e-02, 5.1431e-02],\n",
       "           [1.1765e-03, 4.6162e-03, 5.5039e-03,  ..., 1.1433e-02,\n",
       "            3.5582e-03, 8.8809e-01]],\n",
       " \n",
       "          [[1.2673e-03, 5.7234e-04, 6.3746e-04,  ..., 1.5389e-03,\n",
       "            8.2774e-04, 9.8738e-01],\n",
       "           [1.2833e-01, 4.2485e-02, 1.4106e-01,  ..., 5.9906e-03,\n",
       "            2.9740e-03, 4.9907e-01],\n",
       "           [9.7670e-02, 4.8760e-02, 1.3228e-01,  ..., 4.0646e-03,\n",
       "            1.4933e-03, 5.7161e-01],\n",
       "           ...,\n",
       "           [1.8906e-02, 1.5107e-03, 3.0095e-03,  ..., 3.5069e-02,\n",
       "            1.8813e-02, 3.5309e-01],\n",
       "           [1.2778e-02, 1.2553e-03, 2.4268e-03,  ..., 1.4489e-02,\n",
       "            9.8767e-03, 2.5759e-01],\n",
       "           [9.6786e-04, 7.9969e-04, 1.6639e-03,  ..., 4.7687e-03,\n",
       "            4.1148e-03, 9.6182e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.2315e-03, 7.7186e-04, 3.1389e-04,  ..., 8.5750e-03,\n",
       "            7.3956e-03, 9.5531e-01],\n",
       "           [7.2081e-02, 2.7611e-02, 5.7215e-02,  ..., 4.3854e-04,\n",
       "            1.8974e-04, 8.2299e-01],\n",
       "           [3.6170e-02, 2.4147e-02, 1.4701e-02,  ..., 1.8675e-04,\n",
       "            8.3456e-05, 9.1001e-01],\n",
       "           ...,\n",
       "           [8.5368e-03, 1.4292e-02, 1.2503e-02,  ..., 3.0242e-02,\n",
       "            1.2477e-02, 2.1551e-01],\n",
       "           [7.9328e-03, 1.7160e-02, 4.6085e-03,  ..., 4.5183e-02,\n",
       "            2.0323e-02, 2.5536e-01],\n",
       "           [8.9079e-03, 1.4794e-03, 1.4243e-03,  ..., 2.4264e-03,\n",
       "            1.6552e-03, 9.5103e-01]],\n",
       " \n",
       "          [[1.7030e-03, 1.1542e-04, 1.7026e-03,  ..., 2.7792e-05,\n",
       "            8.4227e-05, 9.9502e-01],\n",
       "           [6.7852e-03, 1.2320e-03, 1.4895e-01,  ..., 9.5130e-06,\n",
       "            5.4843e-05, 8.2818e-01],\n",
       "           [1.5136e-03, 4.4047e-04, 2.2367e-02,  ..., 3.0819e-05,\n",
       "            1.2693e-04, 9.4293e-01],\n",
       "           ...,\n",
       "           [2.6954e-03, 4.0937e-06, 7.4716e-06,  ..., 2.1151e-03,\n",
       "            1.2021e-02, 9.7665e-01],\n",
       "           [8.0627e-04, 4.3967e-06, 1.0210e-05,  ..., 3.1369e-03,\n",
       "            1.0043e-02, 9.8202e-01],\n",
       "           [8.4388e-04, 6.5384e-03, 1.4662e-02,  ..., 1.1175e-03,\n",
       "            2.5644e-04, 9.3878e-01]],\n",
       " \n",
       "          [[6.9746e-04, 3.6721e-03, 5.0912e-03,  ..., 6.8030e-03,\n",
       "            2.5854e-03, 9.0721e-01],\n",
       "           [3.0691e-04, 7.1666e-03, 5.0112e-02,  ..., 2.2080e-02,\n",
       "            5.5616e-03, 1.5588e-01],\n",
       "           [3.5546e-04, 3.5964e-03, 1.4824e-02,  ..., 5.6868e-02,\n",
       "            1.1469e-02, 2.1889e-01],\n",
       "           ...,\n",
       "           [8.6547e-03, 2.6231e-04, 1.3916e-03,  ..., 1.8143e-01,\n",
       "            1.0891e-01, 4.2713e-01],\n",
       "           [1.7469e-02, 9.1580e-05, 5.4657e-04,  ..., 1.5912e-01,\n",
       "            7.2962e-02, 5.2148e-01],\n",
       "           [2.0324e-03, 9.0607e-04, 1.6326e-03,  ..., 2.6123e-03,\n",
       "            1.2450e-03, 9.3900e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.7035e-04, 6.6273e-04, 3.1548e-04,  ..., 4.6880e-01,\n",
       "            2.4298e-01, 4.0880e-03],\n",
       "           [2.5124e-02, 2.2906e-02, 2.4144e-02,  ..., 6.3782e-03,\n",
       "            4.3502e-03, 5.8847e-01],\n",
       "           [1.8390e-02, 2.2016e-02, 1.5878e-02,  ..., 2.5119e-03,\n",
       "            1.2242e-03, 7.1027e-01],\n",
       "           ...,\n",
       "           [2.0598e-02, 4.8754e-03, 7.9941e-03,  ..., 7.5951e-03,\n",
       "            6.9068e-03, 7.7588e-01],\n",
       "           [2.4006e-02, 5.2057e-03, 5.5154e-03,  ..., 1.2439e-02,\n",
       "            9.7868e-03, 7.3806e-01],\n",
       "           [1.8664e-03, 4.5269e-04, 6.8492e-04,  ..., 1.7699e-04,\n",
       "            1.9027e-04, 9.8310e-01]],\n",
       " \n",
       "          [[1.4092e-02, 5.3038e-04, 1.1395e-03,  ..., 1.5791e-01,\n",
       "            4.3015e-02, 3.5375e-01],\n",
       "           [2.0374e-02, 1.3499e-01, 8.6120e-02,  ..., 4.3970e-03,\n",
       "            1.2781e-03, 6.4809e-01],\n",
       "           [8.6208e-03, 4.0958e-02, 1.1124e-01,  ..., 8.9019e-04,\n",
       "            4.2454e-04, 7.7233e-01],\n",
       "           ...,\n",
       "           [1.8425e-02, 1.5793e-04, 4.5628e-04,  ..., 6.8182e-02,\n",
       "            2.8804e-02, 7.4002e-01],\n",
       "           [8.0549e-03, 8.2522e-05, 2.6153e-04,  ..., 8.1099e-02,\n",
       "            4.3911e-02, 7.4114e-01],\n",
       "           [7.0421e-03, 5.6429e-03, 9.7099e-03,  ..., 2.8549e-03,\n",
       "            3.9207e-03, 8.9401e-01]],\n",
       " \n",
       "          [[2.6524e-03, 8.9729e-03, 1.0740e-02,  ..., 6.5617e-02,\n",
       "            3.0440e-02, 7.5051e-01],\n",
       "           [1.7489e-03, 2.1253e-02, 2.4756e-02,  ..., 6.9484e-04,\n",
       "            3.3506e-04, 9.2144e-01],\n",
       "           [5.4658e-04, 4.4043e-03, 8.6245e-02,  ..., 2.4847e-05,\n",
       "            2.9130e-05, 8.9597e-01],\n",
       "           ...,\n",
       "           [4.3482e-03, 6.6646e-04, 2.2082e-04,  ..., 6.2332e-02,\n",
       "            1.8985e-02, 6.7506e-01],\n",
       "           [3.5593e-03, 2.5087e-04, 3.8919e-04,  ..., 7.6221e-02,\n",
       "            2.7108e-02, 6.4265e-01],\n",
       "           [2.1206e-03, 3.4233e-03, 1.7516e-03,  ..., 7.7520e-03,\n",
       "            2.6020e-03, 9.2584e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[6.5851e-03, 4.7983e-04, 2.4464e-04,  ..., 1.4406e-01,\n",
       "            8.9366e-02, 1.3366e-01],\n",
       "           [4.2356e-03, 3.5285e-02, 1.4380e-01,  ..., 6.1142e-02,\n",
       "            4.6399e-02, 3.7137e-01],\n",
       "           [5.2088e-03, 2.6477e-02, 1.2102e-01,  ..., 3.9831e-02,\n",
       "            4.3164e-02, 4.0470e-01],\n",
       "           ...,\n",
       "           [2.2568e-01, 1.4019e-03, 1.4419e-04,  ..., 5.2635e-02,\n",
       "            4.0421e-02, 4.0424e-01],\n",
       "           [1.4945e-01, 1.6494e-03, 1.1615e-04,  ..., 1.6082e-02,\n",
       "            1.0805e-02, 4.7293e-01],\n",
       "           [3.9387e-03, 1.4207e-03, 1.6236e-03,  ..., 2.1423e-03,\n",
       "            2.7449e-03, 8.9299e-01]],\n",
       " \n",
       "          [[1.0249e-02, 1.6112e-03, 3.6532e-04,  ..., 1.0044e-02,\n",
       "            8.7733e-03, 9.2081e-01],\n",
       "           [5.5742e-03, 9.4551e-03, 2.4414e-02,  ..., 3.0583e-04,\n",
       "            6.7185e-04, 8.8026e-01],\n",
       "           [1.1500e-03, 2.4210e-02, 9.7911e-04,  ..., 1.6233e-04,\n",
       "            3.2017e-05, 9.6540e-01],\n",
       "           ...,\n",
       "           [3.3294e-02, 9.8463e-06, 4.8273e-05,  ..., 5.8140e-02,\n",
       "            1.5310e-01, 6.5007e-01],\n",
       "           [1.0561e-02, 1.6665e-05, 2.5446e-05,  ..., 5.4991e-01,\n",
       "            1.4988e-01, 1.8256e-01],\n",
       "           [5.4669e-03, 1.3732e-03, 1.9164e-03,  ..., 8.3701e-03,\n",
       "            1.1559e-02, 9.2694e-01]],\n",
       " \n",
       "          [[4.2574e-03, 1.6708e-04, 8.8940e-05,  ..., 3.5752e-02,\n",
       "            2.6924e-02, 7.4861e-01],\n",
       "           [6.9672e-04, 6.3704e-02, 5.2452e-03,  ..., 1.2793e-05,\n",
       "            5.4797e-06, 9.2007e-01],\n",
       "           [2.3746e-04, 7.6609e-03, 7.7575e-02,  ..., 1.1274e-05,\n",
       "            6.6618e-06, 9.0474e-01],\n",
       "           ...,\n",
       "           [5.8084e-03, 4.4355e-06, 7.8248e-06,  ..., 2.2688e-02,\n",
       "            4.4504e-03, 9.4662e-01],\n",
       "           [3.5850e-03, 3.2957e-06, 6.3866e-06,  ..., 3.4684e-02,\n",
       "            3.8233e-02, 8.9597e-01],\n",
       "           [1.6714e-03, 4.1191e-03, 4.0474e-03,  ..., 2.2188e-03,\n",
       "            1.3894e-03, 9.5914e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.4828e-01, 7.6153e-02, 1.4744e-02,  ..., 3.4117e-02,\n",
       "            3.9385e-02, 3.6441e-02],\n",
       "           [2.6078e-02, 1.0824e-01, 7.4377e-02,  ..., 2.6190e-02,\n",
       "            1.7059e-02, 4.8904e-02],\n",
       "           [1.1679e-02, 1.0215e-01, 2.0298e-01,  ..., 1.4725e-02,\n",
       "            5.6741e-03, 2.7353e-02],\n",
       "           ...,\n",
       "           [4.7641e-01, 7.1359e-02, 2.4565e-03,  ..., 1.2791e-02,\n",
       "            8.4812e-03, 1.8400e-01],\n",
       "           [6.7097e-01, 2.1192e-02, 8.0953e-04,  ..., 8.6137e-03,\n",
       "            9.6266e-03, 1.0975e-01],\n",
       "           [1.2757e-04, 1.1846e-03, 1.0248e-03,  ..., 4.4744e-04,\n",
       "            5.9644e-04, 9.8244e-01]],\n",
       " \n",
       "          [[9.1943e-04, 1.3673e-04, 2.2098e-04,  ..., 1.5468e-01,\n",
       "            3.4344e-02, 6.1900e-01],\n",
       "           [8.5702e-05, 5.5880e-03, 6.9661e-03,  ..., 3.0021e-03,\n",
       "            1.3074e-03, 8.7409e-01],\n",
       "           [7.4562e-05, 2.3288e-02, 2.6855e-02,  ..., 3.1146e-03,\n",
       "            1.2789e-03, 8.6253e-01],\n",
       "           ...,\n",
       "           [3.8691e-04, 6.4724e-04, 4.3855e-04,  ..., 1.1868e-02,\n",
       "            5.8928e-03, 9.0923e-01],\n",
       "           [8.2214e-04, 6.5190e-04, 4.1999e-04,  ..., 1.7565e-02,\n",
       "            7.2768e-03, 8.5490e-01],\n",
       "           [1.6430e-04, 5.5587e-04, 4.9243e-04,  ..., 3.5455e-04,\n",
       "            3.2866e-04, 9.9023e-01]],\n",
       " \n",
       "          [[7.0930e-04, 1.3877e-04, 5.7204e-06,  ..., 1.3282e-02,\n",
       "            6.1808e-03, 9.5246e-01],\n",
       "           [3.1349e-04, 1.8925e-02, 1.5926e-02,  ..., 1.4938e-02,\n",
       "            4.0036e-03, 7.0617e-01],\n",
       "           [3.3723e-04, 4.1354e-02, 2.0984e-02,  ..., 3.2445e-02,\n",
       "            6.9295e-03, 4.3465e-01],\n",
       "           ...,\n",
       "           [5.2240e-04, 1.1139e-04, 1.7122e-05,  ..., 1.3649e-02,\n",
       "            5.5139e-03, 9.3912e-01],\n",
       "           [6.2922e-04, 7.8938e-05, 1.8507e-05,  ..., 2.0378e-02,\n",
       "            6.9760e-03, 9.3637e-01],\n",
       "           [3.9887e-04, 1.2047e-03, 3.0152e-04,  ..., 1.2082e-03,\n",
       "            9.8455e-04, 9.4684e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.3118e-01, 2.4829e-03, 8.8136e-04,  ..., 1.1105e-01,\n",
       "            2.7331e-02, 3.1907e-01],\n",
       "           [7.1885e-04, 3.5719e-02, 2.2329e-02,  ..., 9.1930e-05,\n",
       "            3.9671e-05, 2.2790e-01],\n",
       "           [7.3910e-04, 4.7256e-02, 8.9732e-02,  ..., 8.8592e-05,\n",
       "            6.8402e-05, 4.2055e-01],\n",
       "           ...,\n",
       "           [1.3601e-01, 4.1418e-04, 6.7040e-04,  ..., 1.9350e-01,\n",
       "            5.0055e-02, 4.3337e-01],\n",
       "           [6.8691e-02, 2.2223e-04, 5.5976e-04,  ..., 1.4538e-01,\n",
       "            7.1128e-02, 5.7112e-01],\n",
       "           [2.5884e-03, 4.0990e-03, 3.1471e-03,  ..., 3.5756e-03,\n",
       "            2.6580e-03, 9.4034e-01]],\n",
       " \n",
       "          [[7.3693e-03, 4.2523e-04, 1.0730e-04,  ..., 1.9901e-02,\n",
       "            5.1910e-02, 8.7053e-01],\n",
       "           [3.9105e-05, 2.0133e-03, 9.1391e-04,  ..., 1.9731e-04,\n",
       "            7.8562e-04, 9.7700e-01],\n",
       "           [8.4445e-06, 4.0176e-03, 2.9804e-03,  ..., 3.0038e-05,\n",
       "            1.1590e-04, 9.7631e-01],\n",
       "           ...,\n",
       "           [6.8703e-04, 1.3206e-04, 3.1374e-05,  ..., 4.7849e-04,\n",
       "            4.9364e-04, 9.9423e-01],\n",
       "           [2.0611e-03, 2.7375e-04, 4.7751e-05,  ..., 8.4580e-04,\n",
       "            5.4521e-04, 9.8754e-01],\n",
       "           [6.9512e-05, 3.5341e-04, 3.5123e-04,  ..., 3.9185e-04,\n",
       "            6.6232e-04, 9.9602e-01]],\n",
       " \n",
       "          [[7.3325e-02, 1.0067e-03, 1.3361e-03,  ..., 2.8933e-01,\n",
       "            8.0085e-02, 1.8875e-01],\n",
       "           [5.7953e-03, 1.1192e-01, 3.5810e-02,  ..., 2.8407e-03,\n",
       "            7.8484e-04, 7.4296e-01],\n",
       "           [2.2809e-03, 2.7295e-01, 3.0093e-02,  ..., 9.7054e-04,\n",
       "            1.0138e-04, 5.9830e-01],\n",
       "           ...,\n",
       "           [1.5507e-02, 1.0799e-03, 1.1384e-03,  ..., 8.9505e-02,\n",
       "            1.4397e-02, 5.5089e-01],\n",
       "           [7.3104e-03, 3.6226e-04, 4.1624e-04,  ..., 1.2055e-01,\n",
       "            1.2150e-02, 6.0147e-01],\n",
       "           [9.3573e-03, 3.3619e-03, 1.9629e-03,  ..., 1.3144e-02,\n",
       "            3.6934e-03, 9.0171e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[7.1177e-03, 6.9498e-04, 1.2308e-03,  ..., 2.0551e-02,\n",
       "            2.6189e-02, 6.5004e-01],\n",
       "           [1.6045e-03, 8.2400e-03, 5.5988e-03,  ..., 1.5270e-02,\n",
       "            9.8007e-03, 7.2114e-01],\n",
       "           [1.1322e-03, 4.8473e-04, 2.5434e-02,  ..., 9.1547e-03,\n",
       "            1.0991e-02, 4.5305e-01],\n",
       "           ...,\n",
       "           [2.2526e-03, 2.4173e-04, 4.0878e-04,  ..., 6.4190e-02,\n",
       "            5.6469e-02, 5.6156e-01],\n",
       "           [2.0591e-03, 5.9423e-05, 3.9663e-04,  ..., 2.2229e-02,\n",
       "            9.6967e-02, 6.0171e-01],\n",
       "           [4.3050e-04, 1.6932e-04, 3.6669e-04,  ..., 1.7616e-03,\n",
       "            1.1266e-03, 9.6015e-01]],\n",
       " \n",
       "          [[8.7028e-04, 5.1363e-04, 2.4156e-03,  ..., 6.2815e-03,\n",
       "            4.1553e-03, 9.0809e-01],\n",
       "           [1.4552e-04, 5.7179e-04, 3.6415e-04,  ..., 2.7590e-04,\n",
       "            2.9384e-04, 9.4058e-01],\n",
       "           [7.8161e-04, 3.0227e-03, 7.1874e-04,  ..., 1.2350e-03,\n",
       "            5.2216e-04, 9.5545e-01],\n",
       "           ...,\n",
       "           [6.2073e-04, 9.7618e-04, 1.0281e-03,  ..., 7.1582e-04,\n",
       "            5.2971e-04, 9.6043e-01],\n",
       "           [7.2036e-04, 1.0938e-03, 8.0021e-04,  ..., 1.6068e-03,\n",
       "            6.9197e-04, 9.6003e-01],\n",
       "           [7.4738e-04, 1.5493e-03, 9.5348e-04,  ..., 5.6177e-03,\n",
       "            4.1976e-03, 9.3635e-01]],\n",
       " \n",
       "          [[4.2019e-02, 1.4567e-03, 8.5989e-04,  ..., 6.7629e-02,\n",
       "            2.2176e-02, 6.8848e-01],\n",
       "           [1.3314e-02, 3.3064e-02, 1.0211e-03,  ..., 4.5995e-03,\n",
       "            2.9694e-03, 9.0650e-01],\n",
       "           [1.7185e-02, 9.9502e-03, 9.0098e-03,  ..., 7.8967e-03,\n",
       "            2.8634e-03, 8.0210e-01],\n",
       "           ...,\n",
       "           [8.3290e-02, 1.0973e-03, 4.8665e-04,  ..., 1.0165e-01,\n",
       "            8.0597e-03, 6.3725e-01],\n",
       "           [2.7722e-02, 5.3115e-04, 2.4002e-04,  ..., 1.6388e-02,\n",
       "            9.0255e-02, 8.1388e-01],\n",
       "           [1.1871e-03, 6.7415e-04, 1.3477e-03,  ..., 1.9582e-03,\n",
       "            3.3327e-03, 9.4543e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.0893e-06, 1.2796e-04, 1.4659e-05,  ..., 2.7015e-04,\n",
       "            2.6645e-04, 9.9703e-01],\n",
       "           [1.4695e-05, 1.1343e-03, 8.7380e-04,  ..., 5.3658e-04,\n",
       "            4.6744e-04, 9.7314e-01],\n",
       "           [5.4711e-06, 1.1964e-03, 3.2994e-04,  ..., 1.8035e-04,\n",
       "            1.2422e-04, 9.8849e-01],\n",
       "           ...,\n",
       "           [2.0007e-05, 2.9903e-04, 9.2928e-05,  ..., 1.3644e-03,\n",
       "            1.1804e-03, 9.8009e-01],\n",
       "           [4.2624e-06, 4.7832e-05, 1.4986e-05,  ..., 6.6239e-04,\n",
       "            4.7397e-04, 9.9528e-01],\n",
       "           [3.3116e-05, 9.5201e-04, 7.7402e-04,  ..., 7.4159e-04,\n",
       "            4.3737e-04, 9.7534e-01]],\n",
       " \n",
       "          [[1.5106e-03, 2.4434e-04, 7.7491e-05,  ..., 2.6439e-02,\n",
       "            4.3384e-03, 8.9264e-01],\n",
       "           [3.3825e-04, 7.4515e-03, 7.4078e-03,  ..., 4.2667e-03,\n",
       "            6.6814e-04, 7.7914e-01],\n",
       "           [2.5465e-04, 1.5082e-02, 2.7172e-02,  ..., 5.5686e-03,\n",
       "            5.8868e-04, 6.8735e-01],\n",
       "           ...,\n",
       "           [5.2686e-03, 5.3369e-04, 1.1266e-04,  ..., 9.4511e-03,\n",
       "            2.5754e-03, 9.2855e-01],\n",
       "           [9.8375e-03, 3.6915e-04, 6.0164e-05,  ..., 2.5502e-02,\n",
       "            5.4945e-03, 8.9259e-01],\n",
       "           [1.0947e-03, 3.4445e-04, 5.1649e-04,  ..., 2.2293e-03,\n",
       "            1.2193e-03, 9.3666e-01]],\n",
       " \n",
       "          [[1.1652e-01, 5.9362e-03, 1.6778e-03,  ..., 4.6654e-02,\n",
       "            3.2965e-02, 1.5880e-01],\n",
       "           [2.4116e-03, 6.1936e-02, 8.7273e-02,  ..., 6.7832e-03,\n",
       "            2.8150e-03, 6.3969e-01],\n",
       "           [5.6649e-03, 1.1698e-01, 8.9833e-03,  ..., 7.7169e-03,\n",
       "            9.2859e-04, 6.4685e-01],\n",
       "           ...,\n",
       "           [2.0374e-01, 1.2493e-03, 9.1323e-04,  ..., 4.0905e-02,\n",
       "            5.3077e-02, 3.9813e-01],\n",
       "           [1.7970e-01, 1.4009e-03, 5.3417e-04,  ..., 3.7965e-02,\n",
       "            2.9685e-02, 5.3650e-01],\n",
       "           [2.9513e-03, 1.3162e-03, 1.7028e-03,  ..., 4.4137e-03,\n",
       "            5.4438e-03, 9.1593e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[7.5613e-03, 1.2148e-02, 1.8289e-02,  ..., 1.6546e-02,\n",
       "            1.6524e-02, 1.3907e-01],\n",
       "           [9.1280e-03, 1.6515e-02, 6.7816e-03,  ..., 6.0123e-03,\n",
       "            6.2577e-03, 7.6006e-01],\n",
       "           [7.8132e-03, 1.9009e-02, 1.1693e-02,  ..., 4.3159e-03,\n",
       "            3.4882e-03, 7.8297e-01],\n",
       "           ...,\n",
       "           [9.1896e-03, 5.7568e-03, 5.6531e-03,  ..., 2.9418e-02,\n",
       "            2.0829e-02, 5.0704e-01],\n",
       "           [5.1966e-03, 2.1444e-03, 3.9100e-03,  ..., 2.1076e-02,\n",
       "            1.0903e-02, 6.2047e-01],\n",
       "           [2.3174e-02, 2.6234e-02, 3.7814e-02,  ..., 2.6832e-02,\n",
       "            2.4906e-02, 2.7988e-02]],\n",
       " \n",
       "          [[7.2264e-02, 9.9608e-03, 4.1805e-03,  ..., 2.0115e-01,\n",
       "            8.9798e-02, 1.6901e-01],\n",
       "           [2.2840e-02, 3.8580e-01, 8.4280e-03,  ..., 4.0192e-02,\n",
       "            1.6072e-02, 6.6876e-02],\n",
       "           [6.9847e-03, 1.6760e-02, 1.5253e-01,  ..., 1.9738e-02,\n",
       "            2.2018e-02, 2.5285e-01],\n",
       "           ...,\n",
       "           [1.8230e-02, 1.5081e-03, 7.9936e-04,  ..., 6.7320e-01,\n",
       "            3.4334e-02, 1.0371e-02],\n",
       "           [1.4819e-02, 1.2745e-03, 4.3403e-04,  ..., 6.3614e-02,\n",
       "            7.3710e-01, 1.4723e-02],\n",
       "           [5.3078e-03, 2.2454e-02, 3.2844e-02,  ..., 3.2294e-02,\n",
       "            4.8220e-02, 4.5191e-01]],\n",
       " \n",
       "          [[1.4861e-02, 8.7510e-03, 1.8315e-03,  ..., 2.7068e-01,\n",
       "            1.5550e-01, 9.0918e-02],\n",
       "           [7.7898e-03, 3.0960e-02, 1.2236e-02,  ..., 7.9628e-02,\n",
       "            2.2809e-02, 5.4231e-01],\n",
       "           [8.9238e-03, 4.3418e-02, 2.4901e-02,  ..., 8.0654e-02,\n",
       "            2.4321e-02, 4.5321e-01],\n",
       "           ...,\n",
       "           [1.3392e-02, 4.5059e-03, 1.1205e-03,  ..., 2.0763e-01,\n",
       "            9.2129e-02, 1.8310e-01],\n",
       "           [1.8457e-02, 3.3405e-03, 1.0794e-03,  ..., 2.0629e-01,\n",
       "            8.7656e-02, 2.2936e-01],\n",
       "           [3.9142e-02, 2.9859e-02, 3.1492e-02,  ..., 7.2060e-02,\n",
       "            7.7694e-02, 4.4753e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.5640e-02, 1.6659e-02, 6.1076e-03,  ..., 1.3273e-01,\n",
       "            3.0631e-01, 4.9941e-02],\n",
       "           [2.1272e-02, 8.2529e-02, 8.0055e-03,  ..., 5.9505e-03,\n",
       "            1.2311e-02, 3.9185e-01],\n",
       "           [1.1455e-02, 5.4845e-02, 2.7313e-02,  ..., 6.0459e-03,\n",
       "            1.5306e-02, 4.0426e-01],\n",
       "           ...,\n",
       "           [3.3441e-02, 1.1487e-02, 5.6630e-03,  ..., 2.1888e-02,\n",
       "            3.4120e-02, 3.1681e-01],\n",
       "           [2.4447e-02, 1.3840e-02, 9.4665e-03,  ..., 1.0246e-02,\n",
       "            5.5490e-02, 4.9437e-01],\n",
       "           [4.3946e-02, 4.1850e-02, 3.7541e-02,  ..., 5.3720e-02,\n",
       "            4.3072e-02, 2.9933e-02]],\n",
       " \n",
       "          [[4.5916e-02, 1.4906e-02, 1.0092e-02,  ..., 1.4512e-01,\n",
       "            1.3585e-01, 3.7061e-02],\n",
       "           [1.2624e-02, 2.0359e-01, 5.9511e-03,  ..., 6.5025e-02,\n",
       "            3.8593e-02, 2.2846e-01],\n",
       "           [1.1050e-02, 7.1364e-03, 3.4446e-02,  ..., 2.4081e-02,\n",
       "            3.9998e-02, 3.5843e-01],\n",
       "           ...,\n",
       "           [2.6727e-02, 4.1124e-03, 3.4614e-03,  ..., 1.4374e-01,\n",
       "            1.4565e-01, 1.2963e-01],\n",
       "           [2.2455e-02, 3.6480e-03, 5.6226e-03,  ..., 6.7598e-02,\n",
       "            2.6542e-01, 1.9349e-01],\n",
       "           [1.9515e-02, 3.9274e-02, 5.2085e-02,  ..., 4.2725e-02,\n",
       "            4.0485e-02, 2.4171e-02]],\n",
       " \n",
       "          [[1.3568e-01, 1.9462e-02, 3.3931e-03,  ..., 2.0420e-01,\n",
       "            1.3709e-01, 6.2394e-02],\n",
       "           [1.5655e-02, 4.3804e-01, 1.1652e-02,  ..., 1.9579e-02,\n",
       "            1.8665e-02, 3.0023e-01],\n",
       "           [7.2301e-03, 7.6680e-03, 2.0077e-01,  ..., 1.3564e-02,\n",
       "            1.7031e-02, 5.1944e-01],\n",
       "           ...,\n",
       "           [3.9210e-02, 3.2023e-02, 2.5634e-03,  ..., 3.8380e-01,\n",
       "            5.3400e-02, 1.4656e-01],\n",
       "           [3.0948e-02, 1.3918e-02, 4.3983e-03,  ..., 5.4189e-02,\n",
       "            4.0839e-01, 2.2616e-01],\n",
       "           [1.3019e-02, 3.9770e-02, 1.1244e-02,  ..., 4.3042e-02,\n",
       "            5.9861e-02, 2.5202e-01]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = ds['train']\n",
    "model.to('cpu')\n",
    "inputs = tokenizer(ds['train'][0]['Title'],return_tensors='pt').to('cpu')\n",
    "model(**inputs)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE=calculate_AE_matrix(torch.cat([(layer) for layer in model(**inputs)[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日元上周净空头头寸创四个月新低 美元/日元低位盘整'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds['train'][0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1800/2329 [01:12<00:21, 24.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.5923, 1.5561, 2.0099, 2.6973, 0.8541, 2.6222, 2.8923, 2.3042, 2.6127,\n",
       "         2.6137, 2.2012, 2.0697],\n",
       "        [0.5199, 2.5442, 1.5475, 1.8903, 1.9164, 2.1648, 0.4883, 1.0825, 1.6545,\n",
       "         1.6226, 1.7700, 2.0162],\n",
       "        [1.2914, 0.6142, 1.3902, 0.7059, 1.2123, 2.0710, 1.2120, 0.9345, 0.5865,\n",
       "         1.1221, 1.0552, 0.7694],\n",
       "        [1.6220, 1.9222, 1.3270, 2.0777, 1.6365, 2.3013, 0.8948, 0.9061, 1.6672,\n",
       "         1.6411, 0.8219, 1.6394],\n",
       "        [1.8834, 1.4369, 0.4182, 0.9051, 1.4056, 0.5375, 1.2151, 1.5664, 0.9732,\n",
       "         1.9921, 1.2135, 1.6971],\n",
       "        [0.7581, 1.7851, 1.2582, 1.3668, 1.0130, 1.2287, 1.0818, 0.8757, 1.5096,\n",
       "         1.2374, 0.7788, 0.7061],\n",
       "        [1.5522, 1.0557, 0.4227,    nan, 1.5409, 1.0541, 0.6438, 0.7255, 1.0248,\n",
       "         0.9740, 0.7192, 0.4754],\n",
       "        [0.4826, 0.8135, 1.5645, 1.7033, 0.5167, 0.3270, 0.8177, 1.2983, 1.2855,\n",
       "         1.2419, 0.4703, 1.4300],\n",
       "        [1.3956, 1.0947, 0.8130, 0.5790, 0.7177, 0.9751, 1.4121, 1.1594, 0.7287,\n",
       "         1.5168, 0.8644, 0.4035],\n",
       "        [1.6857, 1.1609, 1.1728, 1.0181, 1.8569, 0.8209, 1.1848, 0.7725, 1.0614,\n",
       "         1.4906, 0.5182, 1.0082],\n",
       "        [0.4822, 0.6464, 0.9270, 1.0524, 0.5067, 0.3615, 0.4464, 0.6645, 1.0730,\n",
       "         0.3517, 0.7671, 1.1852],\n",
       "        [0.8727, 1.0812, 1.5439, 1.0700, 0.4010, 1.3691, 1.0584, 1.2262, 1.6072,\n",
       "         1.7332, 1.0779, 1.1225]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "AE_BERT=get_AE_matrix(model,ds['train'],heads_per_layer=12,layers=12)\n",
    "AE_BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分批计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['labels'])\n",
    "    def __getitem__(self,idx):\n",
    "        keys = list(self.dataset.keys())\n",
    "        values = list(self.dataset.values())\n",
    "        return {keys[0]:values[0][idx],keys[1]:values[1][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_=ds['train'][:1300]\n",
    "# dataset_=ds['train'][1300:]\n",
    "data_val = MyDataset(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(data_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300/1300 [00:53<00:00, 24.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.5733, 1.5489, 1.9908, 2.6849, 0.8528, 2.6035, 2.8764, 2.2905, 2.5992,\n",
       "         2.5950, 2.1883, 2.0580],\n",
       "        [0.5200, 2.5300, 1.5345, 1.8719, 1.8991, 2.1449, 0.4847, 1.0791, 1.6455,\n",
       "         1.6176, 1.7588, 2.0017],\n",
       "        [1.2789, 0.6137, 1.3807, 0.7062, 1.2101, 2.0585, 1.2067, 0.9304, 0.5866,\n",
       "         1.1217, 1.0524, 0.7685],\n",
       "        [1.6134, 1.9124, 1.3239, 2.0659, 1.6261, 2.2873, 0.8946, 0.9030, 1.6609,\n",
       "         1.6354, 0.8169, 1.6369],\n",
       "        [1.8620, 1.4244, 0.4146, 0.8953, 1.3967, 0.5329, 1.2030, 1.5551, 0.9651,\n",
       "         1.9833, 1.2056, 1.6829],\n",
       "        [0.7505, 1.7763, 1.2397, 1.3589, 1.0138, 1.2194, 1.0769, 0.8675, 1.4972,\n",
       "         1.2380, 0.7760, 0.7037],\n",
       "        [1.5486, 1.0500, 0.4174,    nan, 1.5292, 1.0432, 0.6414, 0.7189, 1.0224,\n",
       "         0.9723, 0.7162, 0.4707],\n",
       "        [0.4806, 0.8112, 1.5596, 1.6928, 0.5155, 0.3226, 0.8207, 1.2946, 1.2774,\n",
       "         1.2389, 0.4657, 1.4253],\n",
       "        [1.3947, 1.0846, 0.8034, 0.5748, 0.7155, 0.9705, 1.4104, 1.1547, 0.7272,\n",
       "         1.5060, 0.8572, 0.4022],\n",
       "        [1.6797, 1.1649, 1.1778, 1.0245, 1.8419, 0.8179, 1.1842, 0.7665, 1.0677,\n",
       "         1.4850, 0.5195, 0.9971],\n",
       "        [0.4820, 0.6380, 0.9202, 1.0320, 0.4952, 0.3556, 0.4483, 0.6570, 1.0899,\n",
       "         0.3401, 0.7524, 1.1753],\n",
       "        [0.8663, 1.0695, 1.5165, 1.0610, 0.4004, 1.3842, 1.0461, 1.2044, 1.6096,\n",
       "         1.7505, 1.0691, 1.1054]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_BERT_1300=get_AE_matrix(model,data_val,heads_per_layer=12,layers=12)\n",
    "AE_BERT_1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(AE_BERT_1300,\"./BERT_1300AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_=ds['train'][1300:]\n",
    "# dataset_=ds['train'][1300:]\n",
    "data_val = MyDataset(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1029/1029 [00:42<00:00, 24.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.6053, 1.5640, 2.0297, 2.7009, 0.8552, 2.6395, 2.9051, 2.3188, 2.6247,\n",
       "         2.6279, 2.2114, 2.0729],\n",
       "        [0.5219, 2.5554, 1.5603, 1.9269, 1.9452, 2.1911, 0.4886, 1.0849, 1.6617,\n",
       "         1.6322, 1.7862, 2.0481],\n",
       "        [1.3105, 0.6066, 1.4112, 0.7041, 1.2184, 2.0746, 1.2191, 0.9379, 0.5893,\n",
       "         1.1219, 1.0580, 0.7709],\n",
       "        [1.6385, 1.9381, 1.3334, 2.0964, 1.6491, 2.3310, 0.8967, 0.9106, 1.6754,\n",
       "         1.6492, 0.8303, 1.6455],\n",
       "        [1.9122, 1.4634, 0.4257, 0.9326, 1.4200, 0.5505, 1.2423, 1.5851, 1.0017,\n",
       "         2.0043, 1.2405, 1.7199],\n",
       "        [0.7802, 1.8214, 1.3231, 1.3877, 1.0071, 1.2804, 1.0980, 0.9036, 1.5637,\n",
       "         1.2551, 0.7820, 0.7039],\n",
       "        [1.5597, 1.0728, 0.4300,    nan, 1.5933, 1.0620, 0.6550, 0.7434, 1.0363,\n",
       "         0.9786, 0.7136, 0.4856],\n",
       "        [0.4849, 0.8250, 1.5636, 1.7377, 0.5206, 0.3269, 0.8168, 1.3085, 1.3191,\n",
       "         1.2624, 0.4630, 1.4556],\n",
       "        [1.4129, 1.1226, 0.8353, 0.5916, 0.7379, 0.9988, 1.4209, 1.1902, 0.7390,\n",
       "         1.5410, 0.8852, 0.4106],\n",
       "        [1.7129, 1.1379, 1.1663, 1.0025, 1.8923, 0.8178, 1.1496, 0.7823, 1.0258,\n",
       "         1.4744, 0.4987, 1.0439],\n",
       "        [0.5419, 0.6765, 0.9674, 1.1112, 0.5252, 0.4147, 0.4184, 0.7008, 1.0161,\n",
       "         0.3690, 0.8123, 1.2377],\n",
       "        [0.9533, 1.1556, 1.6800, 1.1567, 0.4350, 1.3571, 1.1404, 1.3410, 1.6520,\n",
       "         1.7278, 1.1888, 1.2289]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_BERT_1029=get_AE_matrix(model,data_val,heads_per_layer=12,layers=12)\n",
    "AE_BERT_1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(AE_BERT_1029,\"./BERT_1029AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_4396\\1536591026.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE1_1300=torch.load('BERT_1300AE.pt')\n",
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_4396\\1536591026.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE1301_2329=torch.load('BERT_1029AE.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "AE1_1300=torch.load('BERT_1300AE.pt')\n",
    "AE1301_2329=torch.load('BERT_1029AE.pt')\n",
    "AE_matrix=(AE1_1300*1300+AE1301_2329*(2329-1300))/2329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(AE_matrix,\"./BERT_2329AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc(train_dataset,model,tokenizer,device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)\n",
    "\n",
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.25:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            input_text_premise = data['premise']\n",
    "            input_text_hypothesis = data['hypothesis']\n",
    "            inputs = tokenizer(input_text_premise,input_text_hypothesis,return_tensors='pt').to('cuda')\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            input_text_premise = data['premise']\n",
    "            input_text_hypothesis = data['hypothesis']\n",
    "            inputs = tokenizer(input_text_premise,input_text_hypothesis,return_tensors='pt').to('cpu')\n",
    "            output = model(**inputs)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alireza1044/albert-base-v2-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Alireza1044/albert-base-v2-mnli\",output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds# 数据量太大，这里选择使用验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "inputs=tokenizer(data_val[0]['premise'],data_val[0]['hypothesis'],return_tensors='pt')\n",
    "attentions = model(**inputs)[1]\n",
    "attentions = torch.cat([(layer) for layer in attentions])\n",
    "attentions.to('cuda')\n",
    "AE = calculate_AE_matrix(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpruner import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER NAME       \t        #PARAMS\t     RATIO\t MEM(MB)\n",
      "--model:         \t     11,685,891\t   100.00%\t   44.58\n",
      "  --albert:      \t     11,683,584\t    99.98%\t   44.57\n",
      "    --embeddings:\t      3,906,048\t    33.43%\t   14.90\n",
      "    --encoder:   \t      7,186,944\t    61.50%\t   27.42\n",
      "    --pooler:    \t        590,592\t     5.05%\t    2.25\n",
      "  --classifier:  \t          2,307\t     0.02%\t    0.01\n",
      "    --weight:    \t          2,304\t     0.02%\t    0.01\n",
      "    --bias:      \t              3\t     0.00%\t    0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pruner\u001b[38;5;241m=\u001b[39mTransformerPruner(model)\n\u001b[0;32m      2\u001b[0m head_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m12\u001b[39m\u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m12\u001b[39m\u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpruner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\textpruner\\pruners\\transformer_pruner.py:81\u001b[0m, in \u001b[0;36mTransformerPruner.prune\u001b[1;34m(self, dataloader, adaptor, batch_postprocessor, head_mask, ffn_mask, keep_shape, save_model, rewrite_cache)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pruning_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ffn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_with_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPruning method is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but no masks are given.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\textpruner\\pruners\\transformer_pruner.py:121\u001b[0m, in \u001b[0;36mTransformerPruner.prune_with_masks\u001b[1;34m(self, head_mask, ffn_mask, keep_shape, set_masks, save_model)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    120\u001b[0m                     heads_to_prune_dict[layer_num]\u001b[38;5;241m.\u001b[39mappend(head_idx)\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheads_to_prune_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_shape \u001b[38;5;241m=\u001b[39m keep_shape\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:1621\u001b[0m, in \u001b[0;36mPreTrainedModel.prune_heads\u001b[1;34m(self, heads_to_prune)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     union_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mget(layer, [])) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(heads)\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads[layer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(union_heads)  \u001b[38;5;66;03m# Unfortunately we have to store it as list for JSON\u001b[39;00m\n\u001b[1;32m-> 1621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prune_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheads_to_prune\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:675\u001b[0m, in \u001b[0;36mAlbertModel._prune_heads\u001b[1;34m(self, heads_to_prune)\u001b[0m\n\u001b[0;32m    673\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(layer \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minner_group_num)\n\u001b[0;32m    674\u001b[0m inner_group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(layer \u001b[38;5;241m-\u001b[39m group_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minner_group_num)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39malbert_layers[inner_group_idx]\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mprune_heads(heads)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\container.py:297\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\container.py:287\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    285\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    289\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of range"
     ]
    }
   ],
   "source": [
    "pruner=TransformerPruner(model)\n",
    "head_mask=torch.tensor(12*[12*[0]])\n",
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "pruner=TransformerPruner(model)\n",
    "head_mask=torch.tensor(1*[12*[0]])\n",
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER NAME       \t        #PARAMS\t     RATIO\t MEM(MB)\n",
      "--model:         \t      9,324,291\t   100.00%\t   35.57\n",
      "  --albert:      \t      9,321,984\t    99.98%\t   35.56\n",
      "    --embeddings:\t      3,906,048\t    41.89%\t   14.90\n",
      "    --encoder:   \t      4,825,344\t    51.75%\t   18.41\n",
      "    --pooler:    \t        590,592\t     6.33%\t    2.25\n",
      "  --classifier:  \t          2,307\t     0.02%\t    0.01\n",
      "    --weight:    \t          2,304\t     0.02%\t    0.01\n",
      "    --bias:      \t              3\t     0.00%\t    0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner=TransformerPruner(model)\n",
    "head_mask=torch.tensor(1*[12*[12*[0]]])\n",
    "pruner.prune(save_model=False,head_mask=head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER NAME       \t        #PARAMS\t     RATIO\t MEM(MB)\n",
      "--model:         \t     11,685,891\t   100.00%\t   44.58\n",
      "  --albert:      \t     11,683,584\t    99.98%\t   44.57\n",
      "    --embeddings:\t      3,906,048\t    33.43%\t   14.90\n",
      "    --encoder:   \t      7,186,944\t    61.50%\t   27.42\n",
      "    --pooler:    \t        590,592\t     5.05%\t    2.25\n",
      "  --classifier:  \t          2,307\t     0.02%\t    0.01\n",
      "    --weight:    \t          2,304\t     0.02%\t    0.01\n",
      "    --bias:      \t              3\t     0.00%\t    0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary(model)) # albert的pruner很奇怪，不能只剪一个头，不然怎么可能12个掩码就够了……但它还是144个头的,猜测是跨层共享参数导致的，可能一个头12层共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model(**inputs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[3.9471e-02, 1.4022e-02, 2.5336e-02,  ..., 9.5757e-02,\n",
       "            4.2413e-02, 2.9814e-01],\n",
       "           [4.2149e-02, 1.7616e-02, 9.4237e-02,  ..., 1.1961e-01,\n",
       "            4.0377e-02, 6.3734e-03],\n",
       "           [1.5737e-01, 3.9418e-02, 1.1913e-01,  ..., 1.1628e-01,\n",
       "            2.8315e-02, 1.1212e-02],\n",
       "           ...,\n",
       "           [1.0692e-01, 5.1957e-03, 2.3589e-02,  ..., 3.8372e-01,\n",
       "            7.1231e-02, 5.4784e-02],\n",
       "           [7.4828e-02, 5.1109e-03, 1.5864e-02,  ..., 1.7942e-01,\n",
       "            2.4952e-01, 1.2513e-01],\n",
       "           [2.0523e-01, 5.8701e-02, 1.6576e-02,  ..., 5.7049e-02,\n",
       "            7.7924e-02, 6.7908e-02]],\n",
       " \n",
       "          [[9.0055e-01, 1.8377e-02, 6.0108e-03,  ..., 6.5669e-04,\n",
       "            5.8390e-04, 1.1724e-02],\n",
       "           [7.6343e-01, 7.3960e-02, 1.1563e-02,  ..., 1.5341e-04,\n",
       "            1.1493e-04, 1.0035e-03],\n",
       "           [5.7970e-01, 2.7745e-01, 3.6470e-02,  ..., 8.4274e-05,\n",
       "            2.8290e-05, 2.0670e-04],\n",
       "           ...,\n",
       "           [9.9369e-02, 1.6989e-02, 8.9583e-03,  ..., 6.7472e-02,\n",
       "            1.2286e-02, 8.7880e-02],\n",
       "           [5.3463e-02, 6.2187e-03, 3.4839e-03,  ..., 1.1798e-01,\n",
       "            4.4636e-02, 1.2956e-01],\n",
       "           [1.3991e-01, 1.0409e-03, 2.3158e-03,  ..., 9.2658e-02,\n",
       "            4.4130e-02, 5.2330e-01]],\n",
       " \n",
       "          [[9.5469e-01, 1.6109e-02, 2.3577e-03,  ..., 5.4776e-04,\n",
       "            2.4351e-04, 1.5997e-03],\n",
       "           [8.6937e-02, 3.6474e-03, 1.6213e-01,  ..., 2.0784e-02,\n",
       "            2.8731e-03, 5.3573e-04],\n",
       "           [4.6334e-01, 2.1080e-02, 5.7527e-02,  ..., 2.6688e-03,\n",
       "            3.3952e-04, 1.7637e-03],\n",
       "           ...,\n",
       "           [4.4438e-01, 4.9912e-04, 2.7072e-04,  ..., 6.3384e-02,\n",
       "            5.4277e-02, 3.2490e-01],\n",
       "           [3.1776e-01, 2.9835e-04, 3.6117e-05,  ..., 1.4969e-02,\n",
       "            2.0643e-01, 4.0397e-01],\n",
       "           [8.4290e-01, 2.2702e-04, 1.1921e-05,  ..., 7.4358e-03,\n",
       "            2.4173e-02, 1.0500e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.4781e-01, 6.9479e-06, 6.4383e-06,  ..., 2.0854e-03,\n",
       "            1.2787e-04, 1.2990e-01],\n",
       "           [4.6180e-01, 3.4560e-02, 1.3685e-02,  ..., 6.7969e-02,\n",
       "            3.1424e-02, 3.3353e-02],\n",
       "           [3.8970e-01, 5.5615e-02, 7.6855e-02,  ..., 1.3483e-01,\n",
       "            3.5270e-02, 1.5336e-02],\n",
       "           ...,\n",
       "           [4.8726e-01, 2.5435e-02, 5.9869e-02,  ..., 1.8613e-01,\n",
       "            1.9090e-02, 2.7677e-02],\n",
       "           [2.2831e-01, 1.0823e-01, 3.0068e-02,  ..., 3.6429e-02,\n",
       "            2.2501e-01, 1.5972e-02],\n",
       "           [7.1700e-03, 9.0402e-06, 1.4472e-06,  ..., 1.0294e-04,\n",
       "            3.2535e-05, 7.5771e-01]],\n",
       " \n",
       "          [[9.8289e-01, 4.0639e-03, 1.2384e-04,  ..., 2.8084e-04,\n",
       "            3.6924e-04, 8.6506e-04],\n",
       "           [7.3715e-01, 5.9112e-02, 1.5751e-03,  ..., 2.2427e-03,\n",
       "            2.0060e-03, 2.6366e-02],\n",
       "           [1.6768e-01, 3.6420e-03, 6.7975e-01,  ..., 6.6869e-02,\n",
       "            2.7433e-04, 3.0102e-02],\n",
       "           ...,\n",
       "           [5.7804e-01, 7.7456e-04, 1.8254e-02,  ..., 3.6573e-01,\n",
       "            1.0411e-03, 1.4961e-02],\n",
       "           [5.3049e-01, 3.3982e-03, 7.1054e-04,  ..., 9.5497e-03,\n",
       "            2.7413e-01, 9.8755e-02],\n",
       "           [2.3314e-01, 5.2395e-04, 4.1949e-05,  ..., 2.9860e-04,\n",
       "            1.2229e-03, 4.3497e-01]],\n",
       " \n",
       "          [[9.1224e-01, 1.1286e-02, 9.6821e-03,  ..., 3.9421e-03,\n",
       "            2.0473e-03, 7.9062e-03],\n",
       "           [1.0601e-01, 2.9129e-02, 3.8792e-02,  ..., 3.5651e-03,\n",
       "            2.2774e-03, 2.4348e-03],\n",
       "           [1.1912e-01, 3.2962e-02, 5.0154e-02,  ..., 7.9250e-03,\n",
       "            3.1874e-03, 5.6259e-03],\n",
       "           ...,\n",
       "           [2.6450e-01, 8.5087e-04, 3.2706e-04,  ..., 7.6337e-02,\n",
       "            5.5340e-02, 4.8852e-01],\n",
       "           [2.5150e-01, 8.7510e-04, 2.1467e-04,  ..., 2.7638e-02,\n",
       "            7.4373e-02, 5.5975e-01],\n",
       "           [7.6653e-01, 1.6749e-04, 6.3950e-05,  ..., 4.3807e-03,\n",
       "            6.8044e-03, 1.9954e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.6060e-02, 1.7246e-02, 2.7758e-02,  ..., 1.0879e-01,\n",
       "            5.6866e-02, 2.0752e-01],\n",
       "           [8.5420e-03, 2.5138e-02, 8.8143e-02,  ..., 1.5419e-01,\n",
       "            2.6892e-02, 1.8307e-01],\n",
       "           [1.6802e-02, 1.5333e-02, 4.6912e-02,  ..., 5.6240e-02,\n",
       "            2.5077e-02, 2.8084e-01],\n",
       "           ...,\n",
       "           [9.4499e-03, 5.2404e-03, 7.7327e-03,  ..., 6.8229e-02,\n",
       "            3.2411e-02, 3.6409e-01],\n",
       "           [1.8474e-02, 3.0677e-03, 8.3103e-03,  ..., 9.2511e-02,\n",
       "            4.7547e-02, 3.3515e-01],\n",
       "           [2.1881e-02, 1.2092e-03, 1.3384e-03,  ..., 2.5200e-03,\n",
       "            1.8104e-03, 4.2693e-01]],\n",
       " \n",
       "          [[1.1331e-02, 5.1456e-04, 6.7756e-04,  ..., 4.7976e-04,\n",
       "            5.2153e-04, 4.1249e-01],\n",
       "           [1.0978e-01, 3.0235e-02, 2.4767e-02,  ..., 2.2246e-03,\n",
       "            2.2673e-03, 3.1205e-01],\n",
       "           [1.0297e-02, 6.0620e-02, 2.8490e-02,  ..., 1.1644e-03,\n",
       "            4.2432e-04, 3.4284e-01],\n",
       "           ...,\n",
       "           [2.7337e-03, 1.0250e-03, 7.8258e-04,  ..., 4.2952e-02,\n",
       "            1.1838e-02, 3.9032e-01],\n",
       "           [6.3454e-03, 6.8566e-04, 5.2832e-04,  ..., 7.9778e-02,\n",
       "            1.9064e-02, 3.4310e-01],\n",
       "           [1.3236e-02, 2.1147e-03, 1.8812e-03,  ..., 1.7077e-03,\n",
       "            1.4750e-03, 4.3797e-01]],\n",
       " \n",
       "          [[1.0337e-02, 1.5659e-03, 7.8650e-04,  ..., 4.0847e-04,\n",
       "            1.1892e-03, 4.3653e-01],\n",
       "           [3.6470e-04, 3.2924e-03, 2.0188e-01,  ..., 1.4905e-02,\n",
       "            1.5692e-03, 6.7560e-02],\n",
       "           [7.4397e-04, 3.4626e-03, 1.9755e-02,  ..., 1.3781e-04,\n",
       "            5.2622e-04, 1.8924e-01],\n",
       "           ...,\n",
       "           [3.8807e-03, 7.7807e-04, 1.1944e-03,  ..., 1.0002e-02,\n",
       "            9.8703e-02, 3.9341e-01],\n",
       "           [2.4733e-03, 1.8894e-04, 3.9129e-04,  ..., 3.8037e-03,\n",
       "            1.1654e-02, 4.4154e-01],\n",
       "           [4.7873e-02, 1.6849e-03, 1.1120e-03,  ..., 1.6029e-03,\n",
       "            2.3662e-03, 4.2921e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2198e-01, 8.5263e-03, 3.7610e-03,  ..., 1.5884e-02,\n",
       "            6.6771e-03, 2.9075e-01],\n",
       "           [6.6840e-02, 5.1010e-02, 2.3550e-02,  ..., 6.4822e-02,\n",
       "            3.2126e-02, 2.4667e-01],\n",
       "           [1.4718e-02, 6.3493e-03, 4.7542e-02,  ..., 2.0707e-01,\n",
       "            1.6720e-02, 3.0073e-01],\n",
       "           ...,\n",
       "           [1.1850e-02, 2.1941e-02, 5.3756e-02,  ..., 1.0333e-01,\n",
       "            2.8330e-03, 3.4832e-01],\n",
       "           [3.4215e-02, 8.4720e-03, 1.3821e-02,  ..., 1.0132e-02,\n",
       "            8.8649e-02, 3.3389e-01],\n",
       "           [5.3659e-02, 7.1216e-03, 3.2895e-03,  ..., 6.7451e-03,\n",
       "            3.1977e-03, 4.0477e-01]],\n",
       " \n",
       "          [[3.4038e-02, 5.1258e-04, 2.2424e-03,  ..., 4.7987e-02,\n",
       "            1.5350e-02, 4.0345e-01],\n",
       "           [8.5390e-04, 6.3184e-03, 3.5418e-04,  ..., 6.5006e-03,\n",
       "            2.1739e-03, 4.4601e-01],\n",
       "           [8.6828e-04, 4.2869e-05, 5.1728e-03,  ..., 3.5033e-02,\n",
       "            2.7805e-04, 4.2444e-01],\n",
       "           ...,\n",
       "           [2.9431e-04, 9.4899e-06, 6.7279e-04,  ..., 7.9622e-02,\n",
       "            1.3809e-03, 4.0948e-01],\n",
       "           [4.0298e-04, 4.5609e-06, 4.3301e-05,  ..., 1.2642e-03,\n",
       "            1.5812e-03, 4.2373e-01],\n",
       "           [5.4254e-04, 2.0712e-05, 4.0091e-05,  ..., 5.2379e-04,\n",
       "            9.6637e-05, 4.4542e-01]],\n",
       " \n",
       "          [[2.5422e-03, 1.3276e-03, 2.1779e-03,  ..., 7.5765e-03,\n",
       "            1.2566e-02, 4.1625e-01],\n",
       "           [3.1874e-03, 4.1989e-02, 1.1383e-01,  ..., 4.9339e-03,\n",
       "            2.9139e-03, 1.1129e-01],\n",
       "           [4.9273e-04, 7.5196e-03, 3.7487e-02,  ..., 4.2317e-03,\n",
       "            1.7373e-03, 2.8417e-01],\n",
       "           ...,\n",
       "           [8.8848e-04, 1.2208e-03, 2.0932e-03,  ..., 5.9644e-02,\n",
       "            1.9715e-02, 4.0642e-01],\n",
       "           [2.7662e-03, 8.1489e-04, 8.8837e-04,  ..., 1.7377e-02,\n",
       "            1.6506e-02, 4.2645e-01],\n",
       "           [3.9166e-03, 1.2987e-03, 1.3617e-03,  ..., 3.9179e-03,\n",
       "            3.8459e-03, 4.4537e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.4985e-03, 3.9900e-02, 7.4187e-03,  ..., 1.2216e-01,\n",
       "            8.6276e-02, 1.6131e-01],\n",
       "           [5.9937e-03, 1.1157e-01, 6.7856e-02,  ..., 8.1717e-02,\n",
       "            1.2897e-01, 8.5543e-02],\n",
       "           [7.8393e-03, 4.8007e-02, 3.7467e-02,  ..., 5.3298e-02,\n",
       "            5.9860e-02, 2.4858e-01],\n",
       "           ...,\n",
       "           [1.8413e-02, 1.3697e-02, 1.6329e-02,  ..., 5.4636e-02,\n",
       "            5.1227e-02, 3.2512e-01],\n",
       "           [1.4884e-02, 5.3171e-03, 6.8022e-03,  ..., 1.0120e-01,\n",
       "            1.9693e-01, 1.8979e-01],\n",
       "           [9.2699e-03, 4.1438e-03, 6.9431e-04,  ..., 2.8191e-03,\n",
       "            2.6051e-03, 4.5591e-01]],\n",
       " \n",
       "          [[9.4206e-03, 6.6089e-03, 1.4912e-02,  ..., 1.9327e-03,\n",
       "            1.5602e-03, 4.4438e-01],\n",
       "           [9.2819e-02, 8.6512e-02, 4.0528e-02,  ..., 3.6852e-03,\n",
       "            7.2866e-03, 2.9539e-01],\n",
       "           [1.3052e-02, 9.1923e-02, 4.2277e-02,  ..., 4.6709e-03,\n",
       "            3.7920e-03, 3.4500e-01],\n",
       "           ...,\n",
       "           [4.9386e-03, 1.8747e-03, 2.4489e-03,  ..., 1.0797e-01,\n",
       "            4.3259e-02, 2.1198e-01],\n",
       "           [9.7824e-03, 7.2456e-04, 5.3412e-04,  ..., 1.3422e-01,\n",
       "            3.3145e-02, 2.9571e-01],\n",
       "           [5.4408e-03, 3.4265e-03, 2.4168e-03,  ..., 3.5092e-03,\n",
       "            1.7185e-03, 4.6718e-01]],\n",
       " \n",
       "          [[7.6224e-03, 4.0953e-03, 3.5995e-04,  ..., 1.0343e-03,\n",
       "            2.5158e-03, 4.6974e-01],\n",
       "           [1.5854e-03, 2.7457e-02, 3.1560e-01,  ..., 2.2726e-02,\n",
       "            3.0154e-03, 1.6263e-01],\n",
       "           [2.4481e-03, 3.3666e-03, 1.8148e-02,  ..., 1.2263e-03,\n",
       "            1.7068e-03, 3.5646e-01],\n",
       "           ...,\n",
       "           [1.0970e-02, 1.2406e-03, 1.5587e-03,  ..., 1.8940e-02,\n",
       "            8.2458e-02, 4.1198e-01],\n",
       "           [2.2478e-03, 2.1709e-03, 2.1945e-03,  ..., 4.9145e-03,\n",
       "            1.7886e-02, 4.5974e-01],\n",
       "           [1.9936e-02, 1.4605e-03, 1.1095e-03,  ..., 4.3340e-03,\n",
       "            6.4778e-03, 4.6161e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[7.6642e-02, 2.3757e-02, 2.2302e-03,  ..., 1.2499e-02,\n",
       "            9.9597e-03, 3.5913e-01],\n",
       "           [1.0748e-01, 1.3883e-01, 2.7073e-02,  ..., 4.2267e-02,\n",
       "            7.5574e-02, 1.2807e-01],\n",
       "           [6.3206e-03, 1.8606e-02, 4.3709e-02,  ..., 2.4151e-01,\n",
       "            2.0072e-02, 2.6464e-01],\n",
       "           ...,\n",
       "           [1.3334e-02, 2.9252e-02, 9.6734e-02,  ..., 1.1050e-01,\n",
       "            3.3153e-03, 3.0464e-01],\n",
       "           [1.2338e-02, 2.6472e-02, 1.2388e-02,  ..., 3.7309e-03,\n",
       "            1.7612e-01, 2.1541e-01],\n",
       "           [1.1893e-02, 3.9299e-03, 2.8583e-03,  ..., 5.7746e-03,\n",
       "            5.8570e-03, 4.4397e-01]],\n",
       " \n",
       "          [[2.4250e-02, 3.3579e-04, 1.2486e-03,  ..., 7.4335e-03,\n",
       "            9.7860e-03, 4.3295e-01],\n",
       "           [2.1217e-03, 2.7866e-03, 6.0092e-04,  ..., 6.4981e-03,\n",
       "            3.5613e-02, 4.4879e-01],\n",
       "           [2.6744e-04, 1.8521e-05, 2.2788e-03,  ..., 1.8231e-03,\n",
       "            3.7061e-04, 4.7337e-01],\n",
       "           ...,\n",
       "           [5.4736e-05, 9.5111e-06, 8.8957e-05,  ..., 3.7070e-03,\n",
       "            1.6547e-03, 4.7165e-01],\n",
       "           [2.1852e-04, 7.0157e-05, 1.7703e-05,  ..., 1.0484e-03,\n",
       "            1.4700e-02, 4.6645e-01],\n",
       "           [4.5976e-04, 1.1976e-05, 1.1072e-05,  ..., 1.5924e-04,\n",
       "            1.7614e-04, 4.7878e-01]],\n",
       " \n",
       "          [[2.6222e-03, 2.2981e-03, 2.5324e-03,  ..., 6.0278e-02,\n",
       "            3.0349e-02, 3.6153e-01],\n",
       "           [7.8281e-03, 5.6919e-02, 1.0169e-01,  ..., 1.5235e-02,\n",
       "            6.5209e-03, 1.5938e-01],\n",
       "           [1.8638e-03, 1.0617e-02, 4.3195e-02,  ..., 1.3047e-02,\n",
       "            3.7574e-03, 3.3083e-01],\n",
       "           ...,\n",
       "           [1.7487e-03, 1.9838e-03, 4.3751e-03,  ..., 8.3271e-02,\n",
       "            4.0278e-02, 3.8241e-01],\n",
       "           [4.7571e-03, 4.2249e-03, 3.6253e-03,  ..., 3.0524e-02,\n",
       "            3.7078e-02, 4.1885e-01],\n",
       "           [2.1667e-03, 1.0713e-03, 1.1184e-03,  ..., 5.5128e-03,\n",
       "            4.4094e-03, 4.6866e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.5634e-03, 2.7737e-02, 1.0649e-02,  ..., 7.8808e-02,\n",
       "            9.2688e-02, 1.2426e-01],\n",
       "           [6.1160e-03, 5.4380e-02, 5.1175e-02,  ..., 1.0389e-01,\n",
       "            1.4408e-01, 8.5970e-02],\n",
       "           [8.6867e-03, 4.7396e-02, 2.6355e-02,  ..., 3.9803e-02,\n",
       "            5.9795e-02, 2.3553e-01],\n",
       "           ...,\n",
       "           [3.8186e-02, 2.0249e-02, 1.3376e-02,  ..., 5.0007e-02,\n",
       "            5.7531e-02, 3.2257e-01],\n",
       "           [1.7311e-02, 6.1923e-03, 7.1125e-03,  ..., 1.0385e-01,\n",
       "            1.8941e-01, 1.9413e-01],\n",
       "           [1.5276e-02, 5.9067e-03, 1.4647e-03,  ..., 3.4816e-03,\n",
       "            5.1466e-03, 4.5256e-01]],\n",
       " \n",
       "          [[1.7274e-02, 6.2398e-03, 2.3290e-02,  ..., 2.5592e-03,\n",
       "            1.4396e-03, 4.4214e-01],\n",
       "           [7.0247e-02, 1.0308e-01, 3.5569e-02,  ..., 5.9384e-03,\n",
       "            2.0125e-02, 2.8768e-01],\n",
       "           [1.0324e-02, 1.4141e-01, 4.6149e-02,  ..., 3.6027e-03,\n",
       "            1.2112e-02, 3.0793e-01],\n",
       "           ...,\n",
       "           [7.6064e-03, 3.3898e-03, 2.5256e-03,  ..., 9.7608e-02,\n",
       "            4.3827e-02, 2.5230e-01],\n",
       "           [1.2938e-02, 5.1953e-03, 3.4460e-03,  ..., 1.3498e-01,\n",
       "            4.6694e-02, 2.4898e-01],\n",
       "           [4.6220e-03, 3.0305e-03, 2.5468e-03,  ..., 2.6205e-03,\n",
       "            2.7222e-03, 4.7423e-01]],\n",
       " \n",
       "          [[1.1671e-02, 4.6771e-03, 6.8398e-04,  ..., 2.5155e-03,\n",
       "            5.2605e-03, 4.7039e-01],\n",
       "           [3.6449e-03, 2.1147e-02, 1.1872e-01,  ..., 1.3840e-02,\n",
       "            1.1893e-02, 1.0057e-01],\n",
       "           [4.2272e-03, 6.4937e-03, 1.4946e-02,  ..., 1.0719e-03,\n",
       "            8.2040e-03, 2.4326e-01],\n",
       "           ...,\n",
       "           [9.6155e-03, 2.2455e-03, 9.4436e-04,  ..., 1.5503e-02,\n",
       "            1.3385e-01, 3.9171e-01],\n",
       "           [5.5745e-03, 8.9363e-03, 4.4838e-03,  ..., 7.2249e-03,\n",
       "            2.8679e-02, 4.3068e-01],\n",
       "           [2.1229e-02, 1.8510e-03, 1.0863e-03,  ..., 3.6890e-03,\n",
       "            9.5829e-03, 4.6729e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.3044e-02, 4.5860e-02, 3.4339e-03,  ..., 3.0460e-02,\n",
       "            1.4169e-02, 3.2464e-01],\n",
       "           [4.8758e-02, 1.0333e-01, 3.7225e-02,  ..., 8.1740e-02,\n",
       "            8.1985e-02, 1.1203e-01],\n",
       "           [8.0148e-03, 3.9634e-02, 4.3826e-02,  ..., 3.4243e-01,\n",
       "            2.0334e-02, 1.9247e-01],\n",
       "           ...,\n",
       "           [1.3627e-02, 3.6836e-02, 8.9632e-02,  ..., 2.5540e-01,\n",
       "            4.0163e-03, 2.2186e-01],\n",
       "           [1.4496e-02, 4.2385e-02, 1.9382e-02,  ..., 8.5626e-03,\n",
       "            1.4590e-01, 1.9443e-01],\n",
       "           [1.3298e-02, 7.5087e-03, 3.4492e-03,  ..., 8.9353e-03,\n",
       "            1.0124e-02, 4.4343e-01]],\n",
       " \n",
       "          [[6.0155e-02, 1.0477e-03, 9.2835e-04,  ..., 5.7917e-03,\n",
       "            4.0435e-02, 3.5343e-01],\n",
       "           [3.4181e-03, 1.2860e-02, 3.6594e-04,  ..., 5.3609e-03,\n",
       "            6.1742e-02, 4.4388e-01],\n",
       "           [5.4250e-04, 1.3848e-04, 6.5430e-03,  ..., 3.5502e-03,\n",
       "            1.9489e-03, 4.8328e-01],\n",
       "           ...,\n",
       "           [2.1516e-04, 3.1131e-05, 2.3358e-04,  ..., 6.2858e-03,\n",
       "            4.2643e-03, 4.8464e-01],\n",
       "           [1.5912e-03, 3.2465e-04, 1.8517e-04,  ..., 3.3652e-03,\n",
       "            4.7434e-02, 4.6291e-01],\n",
       "           [5.5006e-04, 1.7943e-05, 2.0050e-05,  ..., 7.2125e-05,\n",
       "            2.7440e-04, 4.9179e-01]],\n",
       " \n",
       "          [[4.7010e-03, 2.2012e-03, 3.2562e-03,  ..., 5.0533e-02,\n",
       "            1.5021e-02, 3.9442e-01],\n",
       "           [7.6110e-03, 5.6572e-02, 1.4132e-01,  ..., 1.3145e-02,\n",
       "            1.3548e-02, 1.0162e-01],\n",
       "           [2.8398e-03, 1.4044e-02, 8.2432e-02,  ..., 9.8580e-03,\n",
       "            5.8499e-03, 2.5429e-01],\n",
       "           ...,\n",
       "           [4.1817e-03, 4.1543e-03, 4.2873e-03,  ..., 8.0249e-02,\n",
       "            4.0424e-02, 3.8070e-01],\n",
       "           [1.4564e-02, 1.4015e-02, 1.6615e-02,  ..., 2.5068e-02,\n",
       "            3.9077e-02, 3.6203e-01],\n",
       "           [1.9240e-03, 1.6757e-03, 1.6619e-03,  ..., 4.6995e-03,\n",
       "            5.2450e-03, 4.7806e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[5.7109e-03, 2.3465e-02, 1.0779e-02,  ..., 4.8240e-02,\n",
       "            7.8263e-02, 1.0603e-01],\n",
       "           [5.7305e-03, 4.1653e-02, 6.7595e-02,  ..., 5.5031e-02,\n",
       "            6.1968e-02, 1.0235e-01],\n",
       "           [7.1957e-03, 3.3078e-02, 3.1922e-02,  ..., 3.7981e-02,\n",
       "            4.0415e-02, 2.3684e-01],\n",
       "           ...,\n",
       "           [4.6715e-02, 1.6341e-02, 1.7639e-02,  ..., 6.0816e-02,\n",
       "            5.9774e-02, 3.0910e-01],\n",
       "           [2.1771e-02, 6.4124e-03, 1.2311e-02,  ..., 1.1330e-01,\n",
       "            1.6163e-01, 2.1789e-01],\n",
       "           [2.1368e-02, 7.1160e-03, 1.8958e-03,  ..., 2.9091e-03,\n",
       "            5.5300e-03, 4.5006e-01]],\n",
       " \n",
       "          [[1.2934e-02, 1.1782e-02, 9.2626e-02,  ..., 2.7452e-03,\n",
       "            1.6103e-03, 4.0100e-01],\n",
       "           [5.8319e-02, 1.0759e-01, 4.5708e-02,  ..., 3.7815e-03,\n",
       "            9.7633e-03, 3.0926e-01],\n",
       "           [1.6351e-02, 1.7665e-01, 5.5563e-02,  ..., 4.1300e-03,\n",
       "            1.0269e-02, 2.8366e-01],\n",
       "           ...,\n",
       "           [1.0631e-02, 3.5847e-03, 2.6890e-03,  ..., 8.6434e-02,\n",
       "            5.0123e-02, 2.3089e-01],\n",
       "           [1.0769e-02, 4.0066e-03, 2.3407e-03,  ..., 1.1136e-01,\n",
       "            4.5863e-02, 2.0576e-01],\n",
       "           [3.9399e-03, 3.7602e-03, 4.9358e-03,  ..., 2.6226e-03,\n",
       "            2.5951e-03, 4.7791e-01]],\n",
       " \n",
       "          [[9.2988e-03, 8.2146e-03, 1.4567e-03,  ..., 1.8362e-03,\n",
       "            5.3769e-03, 4.7174e-01],\n",
       "           [3.1499e-03, 1.6417e-02, 1.3192e-01,  ..., 1.2086e-02,\n",
       "            6.3187e-03, 7.0269e-02],\n",
       "           [2.9594e-03, 6.9373e-03, 3.6505e-02,  ..., 1.1021e-03,\n",
       "            3.9402e-03, 2.2246e-01],\n",
       "           ...,\n",
       "           [7.5762e-03, 3.1099e-03, 3.7973e-03,  ..., 2.6807e-02,\n",
       "            1.4304e-01, 3.6289e-01],\n",
       "           [4.9662e-03, 7.1216e-03, 6.2032e-03,  ..., 1.0217e-02,\n",
       "            3.9710e-02, 4.2937e-01],\n",
       "           [2.1404e-02, 2.6227e-03, 2.0434e-03,  ..., 3.3527e-03,\n",
       "            8.4767e-03, 4.6899e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1200e-01, 5.2830e-02, 2.1803e-03,  ..., 1.2907e-02,\n",
       "            2.7174e-02, 3.0039e-01],\n",
       "           [4.8458e-02, 6.9187e-02, 4.3263e-02,  ..., 4.8829e-02,\n",
       "            1.1612e-01, 1.1173e-01],\n",
       "           [1.0736e-02, 2.5350e-02, 4.2456e-02,  ..., 2.0095e-01,\n",
       "            4.5289e-02, 2.7953e-01],\n",
       "           ...,\n",
       "           [2.2909e-02, 3.9262e-02, 1.3237e-01,  ..., 1.3158e-01,\n",
       "            5.9656e-03, 2.7124e-01],\n",
       "           [2.1341e-02, 2.9023e-02, 1.5392e-02,  ..., 5.3868e-03,\n",
       "            3.0649e-01, 1.8308e-01],\n",
       "           [1.6533e-02, 1.1662e-02, 4.0844e-03,  ..., 6.3110e-03,\n",
       "            1.1873e-02, 4.4626e-01]],\n",
       " \n",
       "          [[5.0185e-02, 8.6269e-04, 5.7293e-04,  ..., 3.5196e-03,\n",
       "            4.9574e-02, 4.0103e-01],\n",
       "           [2.6463e-03, 1.2971e-02, 7.8224e-04,  ..., 3.0690e-03,\n",
       "            4.4111e-02, 4.6015e-01],\n",
       "           [1.3543e-03, 8.5130e-04, 4.0455e-02,  ..., 7.4392e-03,\n",
       "            5.6434e-03, 4.6350e-01],\n",
       "           ...,\n",
       "           [3.4711e-04, 1.8472e-05, 1.3912e-04,  ..., 3.5724e-03,\n",
       "            5.0481e-03, 4.9056e-01],\n",
       "           [3.2750e-03, 6.4562e-04, 4.3618e-04,  ..., 3.3484e-03,\n",
       "            5.6762e-02, 4.5984e-01],\n",
       "           [6.2296e-04, 1.8585e-05, 1.7380e-05,  ..., 5.0757e-05,\n",
       "            4.0192e-04, 4.9602e-01]],\n",
       " \n",
       "          [[7.4675e-03, 1.4503e-03, 5.6362e-03,  ..., 3.1370e-02,\n",
       "            1.1661e-02, 4.1005e-01],\n",
       "           [7.8622e-03, 5.7885e-02, 1.2826e-01,  ..., 6.4801e-03,\n",
       "            6.8539e-03, 8.1539e-02],\n",
       "           [3.1432e-03, 1.8775e-02, 6.9673e-02,  ..., 5.9161e-03,\n",
       "            4.1092e-03, 2.4697e-01],\n",
       "           ...,\n",
       "           [8.6499e-03, 7.0168e-03, 1.1440e-02,  ..., 5.2862e-02,\n",
       "            3.6263e-02, 3.6749e-01],\n",
       "           [2.7227e-02, 1.3011e-02, 2.3874e-02,  ..., 2.3686e-02,\n",
       "            5.0105e-02, 3.3881e-01],\n",
       "           [1.8602e-03, 2.3896e-03, 1.9421e-03,  ..., 3.1956e-03,\n",
       "            4.7744e-03, 4.8260e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[9.4489e-03, 1.4853e-02, 6.6662e-03,  ..., 4.5509e-02,\n",
       "            1.4153e-01, 1.0962e-01],\n",
       "           [4.2569e-03, 7.9241e-02, 7.7653e-02,  ..., 5.5885e-02,\n",
       "            8.5571e-02, 8.6405e-02],\n",
       "           [6.5513e-03, 8.0770e-02, 5.0034e-02,  ..., 3.0982e-02,\n",
       "            5.8236e-02, 1.6902e-01],\n",
       "           ...,\n",
       "           [6.0121e-02, 2.5043e-02, 1.0560e-02,  ..., 8.8138e-02,\n",
       "            1.0718e-01, 2.5079e-01],\n",
       "           [1.9210e-02, 8.1654e-03, 7.8067e-03,  ..., 1.2786e-01,\n",
       "            2.8385e-01, 1.5631e-01],\n",
       "           [3.5193e-02, 6.4758e-03, 2.3188e-03,  ..., 3.4881e-03,\n",
       "            6.8434e-03, 4.4195e-01]],\n",
       " \n",
       "          [[1.0130e-02, 6.9382e-03, 1.4751e-02,  ..., 2.2126e-03,\n",
       "            9.1657e-04, 4.3929e-01],\n",
       "           [5.5302e-02, 1.3095e-01, 3.1433e-02,  ..., 2.5705e-03,\n",
       "            1.4607e-02, 2.5849e-01],\n",
       "           [2.3149e-02, 2.4022e-01, 3.8650e-02,  ..., 1.9210e-03,\n",
       "            6.4693e-03, 2.6173e-01],\n",
       "           ...,\n",
       "           [1.4818e-02, 2.2384e-03, 9.8166e-04,  ..., 1.1641e-01,\n",
       "            5.9634e-02, 1.5748e-01],\n",
       "           [1.0240e-02, 4.1565e-03, 1.2904e-03,  ..., 1.6340e-01,\n",
       "            7.8204e-02, 1.9481e-01],\n",
       "           [3.6684e-03, 3.5671e-03, 2.8592e-03,  ..., 3.0164e-03,\n",
       "            2.2476e-03, 4.8017e-01]],\n",
       " \n",
       "          [[6.0075e-03, 1.2389e-02, 2.3965e-03,  ..., 2.2777e-03,\n",
       "            7.0824e-03, 4.7067e-01],\n",
       "           [6.1113e-03, 4.1378e-02, 2.4731e-01,  ..., 1.3138e-02,\n",
       "            1.0614e-02, 8.7138e-02],\n",
       "           [3.1923e-03, 1.4311e-02, 3.4654e-02,  ..., 9.4525e-04,\n",
       "            4.7559e-03, 2.3811e-01],\n",
       "           ...,\n",
       "           [9.6078e-03, 2.8020e-03, 1.9068e-03,  ..., 3.0616e-02,\n",
       "            2.0149e-01, 3.4699e-01],\n",
       "           [7.4674e-03, 7.5915e-03, 3.9257e-03,  ..., 1.1967e-02,\n",
       "            7.3169e-02, 4.1657e-01],\n",
       "           [2.1217e-02, 2.7047e-03, 1.2162e-03,  ..., 2.6581e-03,\n",
       "            8.3732e-03, 4.7058e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.6505e-01, 4.1507e-02, 1.8913e-03,  ..., 1.4100e-02,\n",
       "            2.7517e-02, 2.7709e-01],\n",
       "           [2.5405e-02, 6.8167e-02, 4.2082e-02,  ..., 1.2209e-01,\n",
       "            1.2892e-01, 8.4195e-02],\n",
       "           [9.9197e-03, 2.6454e-02, 6.9148e-02,  ..., 5.2683e-01,\n",
       "            6.1942e-02, 1.1090e-01],\n",
       "           ...,\n",
       "           [2.0866e-02, 4.8516e-02, 1.1504e-01,  ..., 2.8224e-01,\n",
       "            1.2281e-02, 1.8619e-01],\n",
       "           [2.9877e-02, 7.9634e-02, 1.3699e-02,  ..., 5.7425e-03,\n",
       "            2.2527e-01, 1.6023e-01],\n",
       "           [2.1164e-02, 1.3310e-02, 3.8012e-03,  ..., 8.0669e-03,\n",
       "            1.6846e-02, 4.3508e-01]],\n",
       " \n",
       "          [[3.0289e-02, 3.9093e-04, 4.5444e-04,  ..., 2.3817e-03,\n",
       "            3.3429e-02, 4.4037e-01],\n",
       "           [2.0171e-03, 2.2066e-02, 9.1374e-04,  ..., 2.7164e-03,\n",
       "            2.7930e-02, 4.6720e-01],\n",
       "           [2.0853e-03, 9.8486e-04, 8.4806e-02,  ..., 1.3483e-02,\n",
       "            1.0262e-02, 4.3168e-01],\n",
       "           ...,\n",
       "           [1.2205e-03, 9.7684e-05, 2.1288e-04,  ..., 1.7777e-02,\n",
       "            1.5137e-02, 4.7701e-01],\n",
       "           [3.1845e-03, 1.0426e-03, 4.7816e-04,  ..., 5.0318e-03,\n",
       "            7.9936e-02, 4.4352e-01],\n",
       "           [8.4843e-04, 1.8223e-05, 1.3018e-05,  ..., 4.1308e-05,\n",
       "            6.1805e-04, 4.9713e-01]],\n",
       " \n",
       "          [[6.3768e-03, 1.5997e-03, 3.1916e-03,  ..., 6.7227e-02,\n",
       "            1.4996e-02, 4.1715e-01],\n",
       "           [5.9402e-03, 1.0045e-01, 8.5491e-02,  ..., 1.1294e-02,\n",
       "            1.2124e-02, 9.5629e-02],\n",
       "           [2.6112e-03, 3.3026e-02, 4.9237e-02,  ..., 6.3184e-03,\n",
       "            4.8328e-03, 1.9869e-01],\n",
       "           ...,\n",
       "           [8.5373e-03, 8.9218e-03, 5.8258e-03,  ..., 8.1918e-02,\n",
       "            8.0452e-02, 3.3803e-01],\n",
       "           [2.9767e-02, 2.1304e-02, 8.5111e-03,  ..., 3.8024e-02,\n",
       "            8.6690e-02, 3.3784e-01],\n",
       "           [1.6708e-03, 2.2306e-03, 1.3242e-03,  ..., 2.7079e-03,\n",
       "            5.6102e-03, 4.8328e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.2110e-02, 7.7183e-03, 7.1370e-03,  ..., 5.0357e-02,\n",
       "            4.9849e-02, 1.5893e-01],\n",
       "           [4.3712e-03, 6.9316e-02, 7.8080e-02,  ..., 4.7944e-02,\n",
       "            2.5199e-02, 1.0057e-01],\n",
       "           [4.1523e-03, 4.1642e-02, 6.2271e-02,  ..., 3.0036e-02,\n",
       "            2.2270e-02, 2.5830e-01],\n",
       "           ...,\n",
       "           [2.4705e-02, 1.6891e-02, 2.2032e-02,  ..., 9.0058e-02,\n",
       "            5.9970e-02, 2.8631e-01],\n",
       "           [1.6820e-02, 6.2942e-03, 1.3045e-02,  ..., 1.0141e-01,\n",
       "            9.2281e-02, 2.8405e-01],\n",
       "           [5.5026e-02, 4.4423e-03, 2.1636e-03,  ..., 3.4818e-03,\n",
       "            4.6389e-03, 4.3901e-01]],\n",
       " \n",
       "          [[1.2467e-02, 1.5914e-02, 7.8297e-02,  ..., 2.4483e-03,\n",
       "            9.6138e-04, 3.9128e-01],\n",
       "           [4.1427e-02, 9.9466e-02, 6.0753e-02,  ..., 2.6504e-03,\n",
       "            8.1237e-03, 2.2763e-01],\n",
       "           [1.3508e-02, 2.1500e-01, 6.0068e-02,  ..., 1.4979e-03,\n",
       "            5.8612e-03, 2.1950e-01],\n",
       "           ...,\n",
       "           [2.0975e-02, 2.8572e-03, 1.1883e-03,  ..., 8.6245e-02,\n",
       "            7.1370e-02, 1.8343e-01],\n",
       "           [1.6479e-02, 2.6307e-03, 1.0061e-03,  ..., 1.6118e-01,\n",
       "            8.2283e-02, 1.7831e-01],\n",
       "           [3.7531e-03, 3.4358e-03, 4.4273e-03,  ..., 2.1193e-03,\n",
       "            1.9373e-03, 4.8159e-01]],\n",
       " \n",
       "          [[6.8721e-03, 1.3288e-02, 2.8450e-03,  ..., 2.3944e-03,\n",
       "            5.1905e-03, 4.7132e-01],\n",
       "           [9.3355e-03, 3.2118e-02, 3.2548e-01,  ..., 8.5395e-03,\n",
       "            4.6704e-03, 8.4717e-02],\n",
       "           [3.2735e-03, 1.0891e-02, 4.8609e-02,  ..., 1.0035e-03,\n",
       "            3.2059e-03, 2.4298e-01],\n",
       "           ...,\n",
       "           [1.1075e-02, 2.0236e-03, 2.6584e-03,  ..., 2.5444e-02,\n",
       "            1.8341e-01, 3.6517e-01],\n",
       "           [6.5597e-03, 5.5029e-03, 5.8296e-03,  ..., 1.5030e-02,\n",
       "            6.3880e-02, 4.0708e-01],\n",
       "           [2.3648e-02, 1.9557e-03, 1.5945e-03,  ..., 1.9659e-03,\n",
       "            7.1014e-03, 4.7262e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.6157e-01, 2.6465e-02, 2.0449e-03,  ..., 1.6244e-02,\n",
       "            2.4515e-02, 2.7539e-01],\n",
       "           [1.7461e-02, 1.2609e-01, 3.2760e-02,  ..., 1.0489e-01,\n",
       "            5.8362e-02, 1.1175e-01],\n",
       "           [1.0589e-02, 5.4988e-02, 1.9802e-02,  ..., 3.5003e-01,\n",
       "            4.5546e-02, 2.1388e-01],\n",
       "           ...,\n",
       "           [2.3634e-02, 1.0746e-01, 1.7624e-01,  ..., 1.9346e-01,\n",
       "            7.4868e-03, 1.8000e-01],\n",
       "           [2.1678e-02, 5.7778e-02, 1.5711e-02,  ..., 1.1993e-02,\n",
       "            2.1646e-01, 1.8209e-01],\n",
       "           [2.1436e-02, 1.0316e-02, 3.2791e-03,  ..., 8.9465e-03,\n",
       "            1.2646e-02, 4.4507e-01]],\n",
       " \n",
       "          [[4.3814e-02, 3.2915e-04, 7.7291e-04,  ..., 2.6955e-03,\n",
       "            1.7144e-02, 4.5574e-01],\n",
       "           [9.5700e-04, 3.8980e-02, 2.3080e-03,  ..., 2.5577e-03,\n",
       "            2.8372e-02, 4.5911e-01],\n",
       "           [1.5002e-03, 2.2063e-03, 1.7931e-01,  ..., 3.7506e-02,\n",
       "            1.6037e-02, 3.7082e-01],\n",
       "           ...,\n",
       "           [2.1687e-03, 1.1322e-04, 1.0453e-03,  ..., 4.9063e-02,\n",
       "            3.1810e-02, 4.4836e-01],\n",
       "           [2.2047e-03, 2.5139e-03, 2.1998e-03,  ..., 1.0236e-02,\n",
       "            1.0504e-01, 4.2570e-01],\n",
       "           [9.6155e-04, 1.3327e-05, 1.1383e-05,  ..., 5.3582e-05,\n",
       "            4.3697e-04, 4.9809e-01]],\n",
       " \n",
       "          [[4.0728e-03, 1.2925e-03, 4.5175e-03,  ..., 4.5336e-02,\n",
       "            2.4675e-02, 4.0969e-01],\n",
       "           [6.1965e-03, 6.8804e-02, 1.2428e-01,  ..., 1.2236e-02,\n",
       "            3.9055e-03, 8.0229e-02],\n",
       "           [4.3325e-03, 3.3925e-02, 5.0450e-02,  ..., 5.3283e-03,\n",
       "            1.7555e-03, 1.4568e-01],\n",
       "           ...,\n",
       "           [1.1244e-02, 1.2930e-02, 7.7068e-03,  ..., 7.0470e-02,\n",
       "            5.3813e-02, 3.5532e-01],\n",
       "           [3.8113e-02, 2.7834e-02, 1.8474e-02,  ..., 5.0364e-02,\n",
       "            6.7921e-02, 3.0089e-01],\n",
       "           [1.7458e-03, 1.8839e-03, 1.5621e-03,  ..., 2.7108e-03,\n",
       "            5.2322e-03, 4.8521e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[7.7047e-03, 5.6956e-03, 6.1712e-03,  ..., 3.0949e-02,\n",
       "            1.3825e-01, 1.8539e-01],\n",
       "           [3.6951e-03, 7.2688e-02, 7.6688e-02,  ..., 2.1900e-02,\n",
       "            2.3886e-02, 1.9042e-01],\n",
       "           [2.4251e-03, 2.2664e-02, 8.7976e-02,  ..., 1.1341e-02,\n",
       "            2.7893e-02, 2.7112e-01],\n",
       "           ...,\n",
       "           [1.6138e-02, 8.0639e-03, 1.6756e-02,  ..., 4.8775e-02,\n",
       "            6.9136e-02, 3.5766e-01],\n",
       "           [8.7565e-03, 4.0809e-03, 1.8575e-02,  ..., 5.9913e-02,\n",
       "            2.4920e-01, 2.2192e-01],\n",
       "           [4.5011e-02, 3.2272e-03, 2.6256e-03,  ..., 2.7807e-03,\n",
       "            5.3600e-03, 4.4341e-01]],\n",
       " \n",
       "          [[7.6195e-03, 1.8477e-02, 1.0480e-01,  ..., 1.1157e-03,\n",
       "            3.9070e-04, 3.7327e-01],\n",
       "           [5.5754e-02, 5.9946e-02, 2.2662e-02,  ..., 1.1199e-03,\n",
       "            4.2764e-03, 2.6097e-01],\n",
       "           [2.6533e-02, 1.4704e-01, 3.5025e-02,  ..., 6.9438e-04,\n",
       "            2.4873e-03, 2.6147e-01],\n",
       "           ...,\n",
       "           [1.7141e-02, 8.5308e-04, 4.3169e-04,  ..., 6.5304e-02,\n",
       "            7.8877e-02, 1.7105e-01],\n",
       "           [1.3425e-02, 1.7900e-03, 7.5925e-04,  ..., 1.1140e-01,\n",
       "            9.1778e-02, 2.2987e-01],\n",
       "           [3.2285e-03, 2.5048e-03, 3.7031e-03,  ..., 1.8423e-03,\n",
       "            1.2092e-03, 4.8362e-01]],\n",
       " \n",
       "          [[1.1179e-02, 4.1815e-03, 2.4798e-03,  ..., 9.1149e-04,\n",
       "            5.2811e-03, 4.7603e-01],\n",
       "           [7.8062e-03, 3.7108e-02, 3.4757e-01,  ..., 4.3197e-03,\n",
       "            2.4923e-03, 1.0628e-01],\n",
       "           [2.7017e-03, 6.5535e-03, 7.0438e-02,  ..., 4.5225e-04,\n",
       "            1.8733e-03, 2.2903e-01],\n",
       "           ...,\n",
       "           [9.6696e-03, 8.4335e-04, 2.2521e-03,  ..., 5.6751e-02,\n",
       "            1.5560e-01, 3.7187e-01],\n",
       "           [9.5793e-03, 2.8764e-03, 3.9329e-03,  ..., 2.0314e-02,\n",
       "            8.5902e-02, 3.8623e-01],\n",
       "           [2.6129e-02, 1.7009e-03, 1.7009e-03,  ..., 1.4451e-03,\n",
       "            6.9499e-03, 4.7244e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.0080e-01, 9.6515e-03, 2.4808e-03,  ..., 9.6022e-03,\n",
       "            5.5139e-02, 3.1446e-01],\n",
       "           [8.3639e-03, 9.6145e-02, 2.3446e-02,  ..., 5.7659e-02,\n",
       "            4.9004e-02, 1.0361e-01],\n",
       "           [1.1949e-02, 3.5700e-02, 4.2144e-02,  ..., 3.0215e-01,\n",
       "            8.0359e-02, 1.8975e-01],\n",
       "           ...,\n",
       "           [1.0333e-02, 5.2020e-02, 3.1986e-01,  ..., 2.6977e-01,\n",
       "            5.4893e-03, 1.3420e-01],\n",
       "           [2.7325e-02, 2.8812e-02, 4.5777e-02,  ..., 4.3446e-03,\n",
       "            1.4480e-01, 1.7184e-01],\n",
       "           [1.5615e-02, 5.4579e-03, 5.2813e-03,  ..., 6.5783e-03,\n",
       "            1.8981e-02, 4.4985e-01]],\n",
       " \n",
       "          [[3.6928e-02, 8.7968e-05, 7.2910e-04,  ..., 2.1585e-03,\n",
       "            1.2971e-02, 4.6650e-01],\n",
       "           [1.6827e-04, 3.2878e-02, 2.2403e-03,  ..., 1.7899e-03,\n",
       "            7.6220e-03, 4.7364e-01],\n",
       "           [5.2651e-04, 2.1367e-03, 2.3248e-01,  ..., 1.6864e-02,\n",
       "            1.3580e-02, 3.5587e-01],\n",
       "           ...,\n",
       "           [6.0048e-04, 3.0920e-05, 8.3458e-04,  ..., 2.6762e-02,\n",
       "            8.7940e-03, 4.7778e-01],\n",
       "           [4.8319e-04, 8.3942e-04, 1.2501e-03,  ..., 1.9655e-03,\n",
       "            2.5187e-01, 3.5609e-01],\n",
       "           [9.7507e-04, 8.0513e-06, 7.8952e-06,  ..., 1.8250e-05,\n",
       "            6.5921e-04, 4.9853e-01]],\n",
       " \n",
       "          [[1.5559e-03, 1.7804e-03, 5.2774e-03,  ..., 7.2217e-02,\n",
       "            2.3684e-02, 4.1528e-01],\n",
       "           [6.1741e-03, 8.2873e-02, 8.7403e-02,  ..., 5.5105e-03,\n",
       "            3.8709e-03, 1.2910e-01],\n",
       "           [5.2442e-03, 3.0867e-02, 3.9871e-02,  ..., 4.0978e-03,\n",
       "            1.5089e-03, 1.8885e-01],\n",
       "           ...,\n",
       "           [6.7792e-03, 6.3736e-03, 3.7463e-03,  ..., 5.8989e-02,\n",
       "            4.3506e-02, 3.7138e-01],\n",
       "           [2.0458e-02, 1.4228e-02, 7.5359e-03,  ..., 3.8871e-02,\n",
       "            5.0631e-02, 3.4467e-01],\n",
       "           [1.7350e-03, 1.3828e-03, 1.1885e-03,  ..., 2.7558e-03,\n",
       "            5.2273e-03, 4.8536e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.2339e-03, 6.2592e-03, 4.3664e-03,  ..., 3.7498e-02,\n",
       "            6.8523e-02, 2.5491e-01],\n",
       "           [4.2067e-03, 5.4419e-02, 1.0027e-01,  ..., 2.5480e-02,\n",
       "            2.0333e-02, 1.9592e-01],\n",
       "           [2.3620e-03, 5.1327e-02, 1.1605e-01,  ..., 4.6484e-03,\n",
       "            1.7878e-02, 2.2484e-01],\n",
       "           ...,\n",
       "           [8.5938e-03, 1.2245e-02, 9.2084e-03,  ..., 2.4359e-02,\n",
       "            4.0031e-02, 3.9200e-01],\n",
       "           [8.6168e-03, 7.1626e-03, 1.0005e-02,  ..., 3.5199e-02,\n",
       "            1.3157e-01, 3.0178e-01],\n",
       "           [2.9458e-02, 4.6759e-03, 2.3481e-03,  ..., 2.0603e-03,\n",
       "            3.9758e-03, 4.5106e-01]],\n",
       " \n",
       "          [[3.0087e-03, 9.1250e-02, 9.9608e-02,  ..., 8.8874e-04,\n",
       "            3.3818e-04, 3.0409e-01],\n",
       "           [1.0987e-01, 8.6158e-02, 2.1406e-02,  ..., 1.7864e-03,\n",
       "            1.7919e-02, 2.6943e-01],\n",
       "           [4.8963e-02, 2.5879e-01, 5.1505e-02,  ..., 4.6826e-04,\n",
       "            6.6428e-03, 2.0186e-01],\n",
       "           ...,\n",
       "           [4.1052e-02, 2.4087e-03, 3.4359e-04,  ..., 4.0003e-02,\n",
       "            9.6783e-02, 1.9404e-01],\n",
       "           [1.4846e-02, 4.8823e-03, 6.6250e-04,  ..., 7.1677e-02,\n",
       "            1.7316e-01, 1.6470e-01],\n",
       "           [2.7653e-03, 4.1681e-03, 3.1042e-03,  ..., 1.0578e-03,\n",
       "            1.1359e-03, 4.8364e-01]],\n",
       " \n",
       "          [[1.1630e-02, 6.2582e-03, 2.3973e-03,  ..., 5.1931e-04,\n",
       "            4.2137e-03, 4.7023e-01],\n",
       "           [4.9709e-03, 3.2304e-02, 3.2323e-01,  ..., 6.0948e-03,\n",
       "            4.1904e-03, 1.1031e-01],\n",
       "           [3.0892e-03, 7.2389e-03, 7.7275e-02,  ..., 2.7079e-04,\n",
       "            1.5268e-03, 1.6162e-01],\n",
       "           ...,\n",
       "           [1.2304e-02, 2.2425e-03, 3.3355e-03,  ..., 3.7519e-02,\n",
       "            1.3522e-01, 3.7877e-01],\n",
       "           [5.6194e-03, 2.0019e-02, 1.1405e-02,  ..., 1.2902e-02,\n",
       "            5.9702e-02, 3.7043e-01],\n",
       "           [1.5927e-02, 2.9686e-03, 1.4497e-03,  ..., 7.0982e-04,\n",
       "            7.6374e-03, 4.7527e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1697e-01, 1.0290e-02, 2.8041e-03,  ..., 6.0171e-03,\n",
       "            6.2719e-02, 3.4808e-01],\n",
       "           [1.1898e-02, 1.2057e-01, 4.3141e-02,  ..., 6.4405e-02,\n",
       "            5.4081e-02, 1.3986e-01],\n",
       "           [2.6386e-02, 2.2462e-02, 9.5267e-02,  ..., 2.2184e-01,\n",
       "            9.2582e-02, 2.1206e-01],\n",
       "           ...,\n",
       "           [2.1208e-02, 4.9580e-02, 2.3084e-01,  ..., 1.0775e-01,\n",
       "            8.9072e-03, 2.3844e-01],\n",
       "           [2.5285e-02, 2.3296e-02, 4.0731e-02,  ..., 2.8875e-03,\n",
       "            1.5478e-01, 1.2927e-01],\n",
       "           [1.3330e-02, 6.1537e-03, 4.9018e-03,  ..., 3.2564e-03,\n",
       "            1.1887e-02, 4.4915e-01]],\n",
       " \n",
       "          [[1.8059e-02, 4.9699e-05, 8.8902e-05,  ..., 3.5625e-04,\n",
       "            4.3645e-03, 4.8355e-01],\n",
       "           [3.8919e-04, 8.1379e-02, 2.2667e-03,  ..., 1.8722e-03,\n",
       "            5.9064e-03, 4.4712e-01],\n",
       "           [9.6908e-04, 9.4167e-04, 1.6838e-01,  ..., 8.3864e-03,\n",
       "            3.6988e-03, 4.0323e-01],\n",
       "           ...,\n",
       "           [7.2539e-04, 4.4700e-05, 5.3341e-04,  ..., 6.2270e-03,\n",
       "            4.5833e-03, 4.9022e-01],\n",
       "           [4.3829e-04, 5.3989e-04, 1.5006e-04,  ..., 4.0423e-04,\n",
       "            1.1915e-01, 4.2791e-01],\n",
       "           [1.7834e-03, 1.0450e-05, 2.2894e-06,  ..., 5.4104e-06,\n",
       "            3.5727e-04, 4.9828e-01]],\n",
       " \n",
       "          [[2.2729e-03, 2.8311e-03, 1.7514e-03,  ..., 1.2497e-01,\n",
       "            8.1910e-02, 3.5951e-01],\n",
       "           [5.6283e-03, 1.0299e-01, 9.0906e-02,  ..., 9.1983e-03,\n",
       "            7.9253e-03, 1.1220e-01],\n",
       "           [3.5042e-03, 2.9969e-02, 4.1506e-02,  ..., 1.7278e-03,\n",
       "            1.2929e-03, 1.7251e-01],\n",
       "           ...,\n",
       "           [6.7967e-03, 8.1707e-03, 2.8831e-03,  ..., 4.1437e-02,\n",
       "            4.7526e-02, 3.8809e-01],\n",
       "           [1.4027e-02, 5.5188e-02, 1.4863e-02,  ..., 8.0453e-02,\n",
       "            9.5464e-02, 2.6057e-01],\n",
       "           [1.5958e-03, 2.5142e-03, 1.0336e-03,  ..., 1.3551e-03,\n",
       "            4.5256e-03, 4.8467e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.3409e-04, 9.1549e-03, 3.9685e-03,  ..., 9.5297e-02,\n",
       "            4.6675e-02, 2.3902e-01],\n",
       "           [6.1486e-03, 7.3454e-02, 1.4586e-01,  ..., 2.9802e-02,\n",
       "            1.3982e-02, 1.9966e-01],\n",
       "           [6.5461e-03, 4.5253e-02, 5.4892e-02,  ..., 1.0143e-02,\n",
       "            1.3808e-02, 2.7111e-01],\n",
       "           ...,\n",
       "           [9.7346e-03, 1.4250e-02, 1.8549e-02,  ..., 2.5488e-02,\n",
       "            1.8602e-02, 3.8776e-01],\n",
       "           [9.4047e-03, 6.0255e-03, 1.5931e-02,  ..., 8.0734e-02,\n",
       "            7.9266e-02, 2.8397e-01],\n",
       "           [1.7534e-02, 4.5496e-03, 3.4096e-03,  ..., 4.8786e-03,\n",
       "            2.6086e-03, 4.5992e-01]],\n",
       " \n",
       "          [[2.1518e-03, 3.4618e-02, 2.3291e-02,  ..., 1.0046e-03,\n",
       "            1.1939e-04, 3.8309e-01],\n",
       "           [1.3720e-01, 9.7194e-02, 4.2257e-02,  ..., 2.0468e-03,\n",
       "            9.5850e-03, 2.2245e-01],\n",
       "           [3.6272e-02, 2.0353e-01, 4.1070e-02,  ..., 1.1170e-03,\n",
       "            5.2167e-03, 2.5717e-01],\n",
       "           ...,\n",
       "           [2.1047e-02, 2.2974e-03, 1.1399e-03,  ..., 9.8751e-02,\n",
       "            8.8767e-02, 9.0700e-02],\n",
       "           [3.7506e-03, 1.9871e-03, 6.2487e-04,  ..., 1.6239e-01,\n",
       "            1.2543e-01, 1.1163e-01],\n",
       "           [2.9505e-03, 2.1842e-03, 1.5614e-03,  ..., 1.6859e-03,\n",
       "            9.2631e-04, 4.8712e-01]],\n",
       " \n",
       "          [[1.3796e-02, 8.4379e-03, 3.1913e-03,  ..., 6.1867e-04,\n",
       "            1.6754e-03, 4.5804e-01],\n",
       "           [5.3921e-03, 5.6068e-02, 4.2723e-01,  ..., 6.5937e-03,\n",
       "            2.6714e-03, 6.7354e-02],\n",
       "           [6.9408e-03, 1.2008e-02, 9.4562e-02,  ..., 1.0246e-03,\n",
       "            1.4441e-03, 1.6585e-01],\n",
       "           ...,\n",
       "           [1.5716e-02, 6.3863e-03, 8.0408e-03,  ..., 4.9281e-02,\n",
       "            1.0318e-01, 3.6701e-01],\n",
       "           [5.7667e-03, 4.7939e-02, 3.5126e-02,  ..., 2.1010e-02,\n",
       "            4.3809e-02, 2.4822e-01],\n",
       "           [1.2691e-02, 2.5170e-03, 1.3674e-03,  ..., 1.2800e-03,\n",
       "            6.3408e-03, 4.7765e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[8.3579e-02, 8.1160e-03, 3.4591e-03,  ..., 1.5814e-02,\n",
       "            3.1386e-02, 3.6372e-01],\n",
       "           [2.1501e-02, 1.4661e-01, 1.1032e-01,  ..., 1.2687e-01,\n",
       "            4.9705e-02, 1.2680e-01],\n",
       "           [1.8905e-02, 1.8717e-02, 4.1561e-02,  ..., 5.1679e-01,\n",
       "            9.2085e-02, 1.1726e-01],\n",
       "           ...,\n",
       "           [3.6115e-02, 5.9924e-02, 3.1720e-01,  ..., 7.7913e-02,\n",
       "            1.6721e-03, 2.1374e-01],\n",
       "           [2.1891e-02, 3.2034e-02, 7.9076e-02,  ..., 1.6302e-03,\n",
       "            2.1632e-02, 1.3880e-01],\n",
       "           [1.2557e-02, 4.3680e-03, 5.7511e-03,  ..., 6.5330e-03,\n",
       "            7.0851e-03, 4.5643e-01]],\n",
       " \n",
       "          [[9.6369e-03, 4.3816e-05, 3.7502e-05,  ..., 6.5216e-04,\n",
       "            5.7871e-04, 4.9136e-01],\n",
       "           [1.8550e-03, 2.5039e-02, 9.6879e-03,  ..., 9.3026e-03,\n",
       "            2.8588e-03, 4.6750e-01],\n",
       "           [2.4809e-03, 6.9077e-04, 2.0722e-01,  ..., 1.3110e-02,\n",
       "            1.8132e-03, 3.7008e-01],\n",
       "           ...,\n",
       "           [1.1418e-03, 4.9615e-05, 9.3774e-04,  ..., 2.4900e-02,\n",
       "            2.3918e-03, 4.8189e-01],\n",
       "           [8.7259e-04, 3.0214e-04, 1.6491e-04,  ..., 2.2635e-03,\n",
       "            6.5225e-02, 4.5609e-01],\n",
       "           [3.3897e-03, 5.7503e-06, 1.0298e-06,  ..., 1.7109e-05,\n",
       "            1.0967e-04, 4.9774e-01]],\n",
       " \n",
       "          [[1.0064e-02, 9.9201e-04, 2.3373e-04,  ..., 7.8612e-02,\n",
       "            4.6132e-02, 3.5349e-01],\n",
       "           [2.1797e-03, 7.1712e-02, 1.1970e-01,  ..., 1.4909e-02,\n",
       "            4.0234e-03, 8.0178e-02],\n",
       "           [2.3225e-03, 2.2581e-02, 2.8509e-02,  ..., 6.1269e-03,\n",
       "            1.3048e-03, 1.9547e-01],\n",
       "           ...,\n",
       "           [7.1595e-03, 8.5346e-03, 5.3016e-03,  ..., 7.5795e-02,\n",
       "            3.8181e-02, 3.1428e-01],\n",
       "           [1.4917e-02, 2.9901e-02, 1.0767e-02,  ..., 7.6622e-02,\n",
       "            3.0770e-02, 2.8088e-01],\n",
       "           [1.5936e-03, 1.6431e-03, 7.0196e-04,  ..., 1.7445e-03,\n",
       "            3.9756e-03, 4.8641e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.3637e-04, 5.8265e-03, 2.2395e-03,  ..., 9.9328e-02,\n",
       "            3.8379e-02, 3.3262e-01],\n",
       "           [2.2716e-02, 4.7600e-02, 7.4583e-02,  ..., 1.6571e-02,\n",
       "            2.7151e-03, 3.2907e-01],\n",
       "           [5.1723e-03, 8.2202e-03, 3.4896e-02,  ..., 8.2232e-03,\n",
       "            3.5392e-03, 3.9891e-01],\n",
       "           ...,\n",
       "           [1.3916e-02, 6.4359e-03, 2.1838e-02,  ..., 2.1207e-02,\n",
       "            3.0544e-02, 3.4721e-01],\n",
       "           [2.8626e-02, 1.7736e-03, 6.4610e-03,  ..., 4.3883e-02,\n",
       "            4.0705e-02, 3.6043e-01],\n",
       "           [1.7788e-02, 1.9910e-03, 2.6175e-03,  ..., 3.6959e-03,\n",
       "            2.1201e-03, 4.6450e-01]],\n",
       " \n",
       "          [[3.4631e-03, 4.4164e-02, 1.2801e-01,  ..., 1.5904e-03,\n",
       "            7.0386e-04, 2.9843e-01],\n",
       "           [4.2246e-01, 3.0480e-02, 2.3672e-02,  ..., 4.0601e-03,\n",
       "            3.9697e-03, 1.6539e-01],\n",
       "           [7.9428e-02, 6.0885e-02, 2.7533e-02,  ..., 4.2200e-03,\n",
       "            8.3578e-03, 2.6622e-01],\n",
       "           ...,\n",
       "           [3.5881e-02, 2.2621e-03, 3.0354e-03,  ..., 9.2784e-02,\n",
       "            7.0015e-02, 7.0585e-02],\n",
       "           [8.6640e-03, 1.3190e-03, 1.1916e-03,  ..., 1.3951e-01,\n",
       "            1.1003e-01, 1.3993e-01],\n",
       "           [3.8469e-03, 8.6594e-04, 1.7626e-03,  ..., 7.0356e-04,\n",
       "            5.1175e-04, 4.9049e-01]],\n",
       " \n",
       "          [[2.5178e-02, 2.3235e-02, 6.1607e-03,  ..., 1.5615e-03,\n",
       "            1.1350e-03, 3.7936e-01],\n",
       "           [4.0901e-02, 7.8115e-02, 2.7810e-01,  ..., 1.9223e-02,\n",
       "            4.7500e-03, 1.8052e-01],\n",
       "           [2.1846e-02, 5.0435e-03, 2.7127e-02,  ..., 2.0491e-03,\n",
       "            1.7415e-03, 4.2504e-01],\n",
       "           ...,\n",
       "           [2.9367e-02, 2.3387e-02, 2.2781e-02,  ..., 1.3939e-02,\n",
       "            4.6162e-02, 3.7785e-01],\n",
       "           [2.4983e-02, 2.3598e-02, 1.8558e-02,  ..., 2.0717e-02,\n",
       "            1.2417e-02, 3.2285e-01],\n",
       "           [1.5358e-02, 1.4902e-03, 1.5775e-03,  ..., 9.8454e-04,\n",
       "            2.7115e-03, 4.8005e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[7.3236e-02, 1.9417e-02, 8.5738e-03,  ..., 7.9414e-03,\n",
       "            9.9972e-03, 3.6866e-01],\n",
       "           [3.6282e-02, 2.4793e-01, 7.8857e-02,  ..., 2.8188e-01,\n",
       "            3.8885e-02, 5.8135e-02],\n",
       "           [2.7828e-02, 9.2882e-02, 5.0443e-02,  ..., 3.1426e-01,\n",
       "            3.3319e-02, 1.9615e-01],\n",
       "           ...,\n",
       "           [5.0698e-02, 1.5456e-01, 1.8998e-01,  ..., 2.3177e-02,\n",
       "            3.1659e-03, 2.1262e-01],\n",
       "           [4.8520e-02, 6.6446e-02, 2.8350e-02,  ..., 5.4262e-03,\n",
       "            1.5499e-02, 2.5847e-01],\n",
       "           [1.7068e-02, 2.2009e-03, 4.8989e-03,  ..., 4.3050e-03,\n",
       "            4.7520e-03, 4.6694e-01]],\n",
       " \n",
       "          [[1.3525e-02, 5.3356e-05, 3.6560e-05,  ..., 2.5773e-03,\n",
       "            1.3505e-03, 4.8597e-01],\n",
       "           [9.5756e-03, 6.8816e-02, 3.2784e-02,  ..., 3.1654e-02,\n",
       "            1.5272e-02, 3.9975e-01],\n",
       "           [7.6036e-03, 5.0769e-04, 2.2277e-01,  ..., 1.2892e-02,\n",
       "            5.2010e-03, 3.3952e-01],\n",
       "           ...,\n",
       "           [1.7357e-02, 2.8178e-04, 5.0138e-03,  ..., 1.4539e-01,\n",
       "            1.7807e-02, 3.8743e-01],\n",
       "           [7.9064e-03, 6.0494e-04, 4.8489e-04,  ..., 9.6441e-03,\n",
       "            5.2848e-01, 1.9671e-01],\n",
       "           [1.8138e-02, 2.7892e-06, 3.4388e-06,  ..., 4.8613e-05,\n",
       "            3.2389e-04, 4.8919e-01]],\n",
       " \n",
       "          [[1.6187e-02, 3.0934e-04, 3.0377e-04,  ..., 1.1572e-01,\n",
       "            2.4955e-01, 1.6075e-01],\n",
       "           [4.1659e-03, 6.3485e-02, 4.2894e-02,  ..., 3.0351e-02,\n",
       "            3.0170e-03, 1.5388e-01],\n",
       "           [8.6811e-03, 1.2197e-02, 1.8691e-02,  ..., 1.9062e-02,\n",
       "            3.1366e-03, 2.1398e-01],\n",
       "           ...,\n",
       "           [2.0276e-02, 6.5195e-03, 8.0587e-03,  ..., 6.5571e-02,\n",
       "            3.5337e-02, 3.2955e-01],\n",
       "           [3.4751e-02, 1.7606e-02, 2.2845e-02,  ..., 1.0227e-01,\n",
       "            3.5529e-02, 2.8172e-01],\n",
       "           [2.3120e-03, 4.7470e-04, 5.7436e-04,  ..., 9.5825e-04,\n",
       "            2.0854e-03, 4.8996e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.8259e-03, 9.1049e-03, 1.9037e-03,  ..., 1.5479e-02,\n",
       "            1.4757e-02, 4.5704e-01],\n",
       "           [5.9288e-02, 2.9884e-02, 5.1609e-02,  ..., 7.2493e-03,\n",
       "            5.4992e-03, 3.7272e-01],\n",
       "           [3.3045e-02, 1.1413e-02, 4.5343e-02,  ..., 5.3855e-03,\n",
       "            1.2927e-02, 2.4296e-01],\n",
       "           ...,\n",
       "           [1.3468e-02, 4.2567e-03, 8.3414e-03,  ..., 6.2916e-02,\n",
       "            8.4110e-02, 1.6459e-01],\n",
       "           [2.0807e-02, 5.1614e-03, 7.0431e-03,  ..., 6.9196e-02,\n",
       "            1.2972e-01, 1.5970e-01],\n",
       "           [2.1679e-02, 1.5019e-03, 1.5476e-03,  ..., 4.1997e-03,\n",
       "            5.8745e-03, 4.5227e-01]],\n",
       " \n",
       "          [[8.1898e-03, 1.4911e-01, 1.8672e-01,  ..., 1.1018e-02,\n",
       "            7.1147e-03, 1.2740e-01],\n",
       "           [2.1854e-01, 2.0092e-02, 6.1988e-03,  ..., 1.1880e-02,\n",
       "            1.5398e-02, 2.6442e-01],\n",
       "           [1.0390e-01, 2.0240e-02, 9.7662e-03,  ..., 7.8093e-03,\n",
       "            2.7573e-02, 1.6388e-01],\n",
       "           ...,\n",
       "           [6.1992e-02, 3.5208e-03, 3.5677e-03,  ..., 9.2729e-02,\n",
       "            7.1865e-02, 3.6973e-02],\n",
       "           [3.5804e-02, 1.4413e-03, 6.9978e-04,  ..., 1.2918e-01,\n",
       "            1.5633e-01, 4.1430e-02],\n",
       "           [5.0904e-03, 1.8433e-03, 1.8277e-03,  ..., 1.9078e-03,\n",
       "            1.9462e-03, 4.8126e-01]],\n",
       " \n",
       "          [[2.5526e-02, 3.2208e-02, 2.7560e-02,  ..., 1.0804e-02,\n",
       "            8.9170e-03, 1.4692e-01],\n",
       "           [4.9600e-02, 2.9557e-02, 2.2631e-02,  ..., 8.5903e-03,\n",
       "            9.3367e-03, 3.9174e-01],\n",
       "           [5.8957e-02, 1.1369e-02, 2.9278e-02,  ..., 4.6751e-03,\n",
       "            2.2074e-02, 3.6899e-01],\n",
       "           ...,\n",
       "           [3.7921e-02, 1.1641e-02, 8.1019e-03,  ..., 4.0451e-02,\n",
       "            3.0151e-01, 9.9333e-02],\n",
       "           [1.7206e-02, 1.4417e-02, 4.9545e-03,  ..., 5.8257e-02,\n",
       "            8.2502e-02, 2.0173e-01],\n",
       "           [1.8161e-02, 3.1537e-03, 1.8845e-03,  ..., 1.4592e-03,\n",
       "            6.5417e-03, 4.7081e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.5903e-01, 1.9955e-02, 2.2010e-02,  ..., 1.4879e-02,\n",
       "            2.7592e-02, 2.4051e-01],\n",
       "           [9.3312e-02, 1.2284e-01, 2.4609e-02,  ..., 2.9094e-01,\n",
       "            2.4304e-02, 1.5782e-01],\n",
       "           [1.8070e-02, 5.0459e-02, 3.2357e-02,  ..., 1.1976e-01,\n",
       "            1.7630e-02, 3.2501e-01],\n",
       "           ...,\n",
       "           [8.9390e-02, 3.8015e-02, 8.2919e-03,  ..., 2.8643e-02,\n",
       "            6.8216e-03, 2.6851e-01],\n",
       "           [7.4652e-02, 1.0490e-02, 3.0732e-03,  ..., 1.1904e-02,\n",
       "            2.8179e-02, 3.4464e-01],\n",
       "           [3.8617e-02, 1.3106e-03, 1.2782e-03,  ..., 1.9086e-03,\n",
       "            7.5600e-03, 4.5508e-01]],\n",
       " \n",
       "          [[6.9626e-03, 3.7204e-04, 8.1170e-05,  ..., 4.4113e-03,\n",
       "            7.0942e-04, 4.8203e-01],\n",
       "           [3.7398e-02, 1.1961e-01, 1.6997e-03,  ..., 1.5419e-03,\n",
       "            1.7758e-03, 2.8526e-01],\n",
       "           [1.6099e-01, 1.1964e-03, 5.0976e-03,  ..., 4.7710e-03,\n",
       "            1.6596e-03, 3.0484e-01],\n",
       "           ...,\n",
       "           [6.9429e-02, 1.1975e-03, 5.5588e-04,  ..., 2.9309e-01,\n",
       "            1.6893e-02, 2.5129e-01],\n",
       "           [6.9578e-02, 6.2123e-03, 8.4370e-05,  ..., 2.9961e-03,\n",
       "            2.0678e-01, 2.5455e-01],\n",
       "           [1.1041e-02, 4.7622e-05, 3.3654e-06,  ..., 1.0150e-04,\n",
       "            1.7588e-04, 4.8812e-01]],\n",
       " \n",
       "          [[1.8840e-03, 2.5409e-04, 2.0443e-04,  ..., 2.0141e-02,\n",
       "            2.1468e-01, 8.2385e-03],\n",
       "           [1.1022e-02, 1.4180e-02, 2.5870e-02,  ..., 2.0426e-02,\n",
       "            1.7536e-02, 2.1491e-01],\n",
       "           [3.4949e-02, 9.3587e-03, 2.1815e-02,  ..., 1.5943e-02,\n",
       "            9.5381e-03, 2.3040e-01],\n",
       "           ...,\n",
       "           [6.8136e-02, 4.8922e-03, 5.5715e-03,  ..., 6.2829e-02,\n",
       "            1.7603e-01, 8.6218e-02],\n",
       "           [4.7760e-02, 3.5456e-02, 2.2309e-02,  ..., 9.4101e-02,\n",
       "            1.0231e-01, 1.0367e-01],\n",
       "           [3.2393e-03, 8.6073e-04, 1.1106e-03,  ..., 2.0651e-03,\n",
       "            5.7211e-03, 4.7363e-01]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)\n",
    "\n",
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.5:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            input_text_premise = data['premise']\n",
    "            input_text_hypothesis = data['hypothesis']\n",
    "            inputs = tokenizer(input_text_premise,input_text_hypothesis,return_tensors='pt').to('cuda')\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            input_text_premise = data['premise']\n",
    "            input_text_hypothesis = data['hypothesis']\n",
    "            inputs = tokenizer(input_text_premise,input_text_hypothesis,return_tensors='pt').to('cpu')\n",
    "            output = model(**inputs)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alireza1044/albert-base-v2-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Alireza1044/albert-base-v2-mnli\",output_attentions=True)\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"mnli\")\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['label'])\n",
    "    def __getitem__(self,idx):\n",
    "        keys = list(self.dataset.keys())\n",
    "        values = list(self.dataset.values())\n",
    "        return {keys[0]:values[0][idx],keys[1]:values[1][idx],keys[2]:values[2][idx]}\n",
    "\n",
    "slices = [500*i for i in range(1,21)]\n",
    "# dataset_=ds['validation_matched'][:slices[0]] #500\n",
    "# dataset_=ds['validation_matched'][slices[0]:slices[1]] #500-1000\n",
    "# dataset_=ds['validation_matched'][slices[1]:slices[2]] #1000-1500\n",
    "# dataset_=ds['validation_matched'][slices[2]:slices[3]] #1500-2000\n",
    "# dataset_=ds['validation_matched'][slices[3]:slices[4]] #2000-2500\n",
    "# dataset_=ds['validation_matched'][slices[4]:slices[5]] #2500-3000\n",
    "# dataset_=ds['validation_matched'][slices[5]:slices[6]] #3000-3500\n",
    "# dataset_=ds['validation_matched'][slices[6]:slices[7]] #3500-4000\n",
    "# dataset_=ds['validation_matched'][slices[7]:slices[8]] #4000-4500\n",
    "# dataset_=ds['validation_matched'][slices[8]:slices[9]] #4500-5000\n",
    "# dataset_=ds['validation_matched'][slices[9]:slices[10]] #5000-5500\n",
    "# dataset_=ds['validation_matched'][slices[10]:slices[11]] #5500-6000\n",
    "# dataset_=ds['validation_matched'][slices[11]:slices[12]] #6000-6500\n",
    "# dataset_=ds['validation_matched'][slices[12]:slices[13]] #6500-7000\n",
    "# dataset_=ds['validation_matched'][slices[13]:slices[14]] #7000-7500\n",
    "# dataset_=ds['validation_matched'][slices[14]:slices[15]] #7500-8000\n",
    "# dataset_=ds['validation_matched'][slices[15]:slices[16]] #8000-8500\n",
    "# dataset_=ds['validation_matched'][slices[16]:slices[17]] #8500-9000\n",
    "# dataset_=ds['validation_matched'][slices[17]:slices[18]] #9000-9500\n",
    "dataset_=ds['validation_matched'][slices[18]:slices[19]] #9500-10000\n",
    "data_val = MyDataset(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:17<00:00, 18.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.0224, 2.1710, 1.8527, 0.6527, 0.7127, 3.1449, 2.0625, 2.1028, 2.4925,\n",
       "         2.7966, 1.4605, 2.3825],\n",
       "        [2.3129, 1.9487, 1.5824, 1.1471, 1.1490, 2.1990, 1.6244, 1.7697, 2.3100,\n",
       "         2.3633, 0.8459, 1.8759],\n",
       "        [2.3871, 2.0191, 1.6216, 1.2562, 1.2409, 2.2774, 1.6369, 1.7791, 2.2870,\n",
       "         2.3291, 0.8378, 1.9776],\n",
       "        [2.3471, 1.9955, 1.6162, 1.2800, 1.2775, 2.2823, 1.6111, 1.8245, 2.2594,\n",
       "         2.3187, 0.8968, 1.9425],\n",
       "        [2.3449, 2.0319, 1.6648, 1.3451, 1.3447, 2.3173, 1.6479, 1.8410, 2.2736,\n",
       "         2.2930, 0.9327, 1.9993],\n",
       "        [2.3186, 2.0453, 1.6857, 1.3974, 1.3911, 2.3278, 1.6563, 1.8490, 2.2493,\n",
       "         2.2702, 0.9529, 1.9987],\n",
       "        [2.2633, 2.0516, 1.6938, 1.4395, 1.4327, 2.3182, 1.6596, 1.8533, 2.2233,\n",
       "         2.2241, 0.9701, 2.0102],\n",
       "        [2.2203, 2.0680, 1.7373, 1.4951, 1.4823, 2.3323, 1.6900, 1.8319, 2.2289,\n",
       "         2.2167, 0.9957, 2.0360],\n",
       "        [2.1536, 2.0869, 1.7666, 1.5636, 1.5328, 2.2807, 1.6984, 1.8203, 2.2397,\n",
       "         2.1574, 0.9574, 2.0318],\n",
       "        [2.2007, 2.1155, 1.8200, 1.6140, 1.5813, 2.2985, 1.7275, 1.9024, 2.2778,\n",
       "         2.1638, 0.9890, 2.0850],\n",
       "        [2.0893, 2.0747, 1.7904, 1.6213, 1.6156, 2.2301, 1.6730, 1.9808, 2.3179,\n",
       "         2.1701, 1.1661, 2.0540],\n",
       "        [2.0322, 2.0611, 1.9148, 1.8279, 1.8872, 2.1020, 1.7646, 1.7661, 2.2987,\n",
       "         2.0308, 1.3583, 2.0843]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_ALBERT=get_AE_matrix(model,data_val,heads_per_layer=12,layers=12)\n",
    "AE_ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(AE_ALBERT,\"./ALBERT_AE/ALBERT9500_9814AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_24712\\3021857992.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE_ALBERT_LIST.append(torch.load(f'./ALBERT_AE/ALBERT{(i-1)*500}_{i*500-1}AE.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "AE_ALBERT_LIST=[]\n",
    "for i in range(1,20):\n",
    "    AE_ALBERT_LIST.append(torch.load(f'./ALBERT_AE/ALBERT{(i-1)*500}_{i*500-1}AE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_24712\\4058187998.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ALBERT9500_9814AE=torch.load('ALBERT_AE/ALBERT9500_9814AE.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.0224, 2.1710, 1.8527, 0.6527, 0.7127, 3.1449, 2.0625, 2.1028, 2.4925,\n",
       "         2.7966, 1.4605, 2.3825],\n",
       "        [2.3129, 1.9487, 1.5824, 1.1471, 1.1490, 2.1990, 1.6244, 1.7697, 2.3100,\n",
       "         2.3633, 0.8459, 1.8759],\n",
       "        [2.3871, 2.0191, 1.6216, 1.2562, 1.2409, 2.2774, 1.6369, 1.7791, 2.2870,\n",
       "         2.3291, 0.8378, 1.9776],\n",
       "        [2.3471, 1.9955, 1.6162, 1.2800, 1.2775, 2.2823, 1.6111, 1.8245, 2.2594,\n",
       "         2.3187, 0.8968, 1.9425],\n",
       "        [2.3449, 2.0319, 1.6648, 1.3451, 1.3447, 2.3173, 1.6479, 1.8410, 2.2736,\n",
       "         2.2930, 0.9327, 1.9993],\n",
       "        [2.3186, 2.0453, 1.6857, 1.3974, 1.3911, 2.3278, 1.6563, 1.8490, 2.2493,\n",
       "         2.2702, 0.9529, 1.9987],\n",
       "        [2.2633, 2.0516, 1.6938, 1.4395, 1.4327, 2.3182, 1.6596, 1.8533, 2.2233,\n",
       "         2.2241, 0.9701, 2.0102],\n",
       "        [2.2203, 2.0680, 1.7373, 1.4951, 1.4823, 2.3323, 1.6900, 1.8319, 2.2289,\n",
       "         2.2167, 0.9957, 2.0360],\n",
       "        [2.1536, 2.0869, 1.7666, 1.5636, 1.5328, 2.2807, 1.6984, 1.8203, 2.2397,\n",
       "         2.1574, 0.9574, 2.0318],\n",
       "        [2.2007, 2.1155, 1.8200, 1.6140, 1.5813, 2.2985, 1.7275, 1.9024, 2.2778,\n",
       "         2.1638, 0.9890, 2.0850],\n",
       "        [2.0893, 2.0747, 1.7904, 1.6213, 1.6156, 2.2301, 1.6730, 1.9808, 2.3179,\n",
       "         2.1701, 1.1661, 2.0540],\n",
       "        [2.0322, 2.0611, 1.9148, 1.8279, 1.8872, 2.1020, 1.7646, 1.7661, 2.2987,\n",
       "         2.0308, 1.3583, 2.0843]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "ALBERT9500_9814AE=torch.load('ALBERT_AE/ALBERT9500_9814AE.pt')\n",
    "ALBERT9500_9814AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_matrix=(sum([ae*500 for ae in AE_ALBERT_LIST])+ALBERT9500_9814AE*315)/9815\n",
    "# torch.save(AE_matrix,\"./ALBERT_AE/ALBERT_9815AE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\",output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"rahmaabusalma/tweets_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label', 'sentiment'],\n",
       "        num_rows: 31232\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'label', 'sentiment'],\n",
       "        num_rows: 5205\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label', 'sentiment'],\n",
       "        num_rows: 5206\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 317,\n",
       " 'text': 'Laying in bed til workkk... Oh the life. Definitely pinched a nerve.',\n",
       " 'label': 0,\n",
       " 'sentiment': 'negative'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(ds['validation'][0]['text'],return_tensors='pt')\n",
    "len(model(**inputs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\",output_attentions=True)\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"rahmaabusalma/tweets_sentiment_analysis\")\n",
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)\n",
    "\n",
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.4:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            input_text = data['text']\n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cuda')\n",
    "            output = model(**inputs)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            input_text = data['text']\n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cpu')\n",
    "            output = model(**inputs)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['label'])\n",
    "    def __getitem__(self,idx):\n",
    "        keys = list(self.dataset.keys())\n",
    "        values = list(self.dataset.values())\n",
    "        return {keys[0]:values[0][idx],keys[1]:values[1][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:28<00:00,  7.08it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_ = ds['validation'][5000:5500]\n",
    "data_val = MyDataset(dataset_)\n",
    "AE_ROBERTA=get_AE_matrix(model,data_val,16,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(AE_ROBERTA,\"./ROBERTA_AE/ROBERTA5000_5204AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_18568\\1678175550.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE_ROBERTA_LIST.append(torch.load(f'./ROBERTA_AE/ROBERTA{(i-1)*500}_{i*500-1}AE.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "AE_ROBERTA_LIST=[]\n",
    "for i in range(1,11):\n",
    "    AE_ROBERTA_LIST.append(torch.load(f'./ROBERTA_AE/ROBERTA{(i-1)*500}_{i*500-1}AE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_18568\\3983594318.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE_ROBERTA_205 = torch.load('./ROBERTA_AE/ROBERTA5000_5204AE.pt')\n"
     ]
    }
   ],
   "source": [
    "AE_ROBERTA_205 = torch.load('./ROBERTA_AE/ROBERTA5000_5204AE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_matrix=(sum([ae*500 for ae in AE_ROBERTA_LIST])+205*AE_ROBERTA_205)/5205\n",
    "torch.save(AE_matrix,\"./ROBERTA_AE/ROBERTA_5205AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6263e+00, 2.5077e+00, 2.5571e+00, 2.5777e+00, 2.5178e+00, 2.6367e+00,\n",
       "         2.5353e+00, 2.4944e+00, 2.6276e+00, 2.1178e+00, 2.5601e+00, 1.6013e+00,\n",
       "         2.5379e+00, 2.6502e+00, 2.5943e+00, 2.3914e+00],\n",
       "        [1.0713e+00, 1.0350e+00, 1.5223e+00, 1.5613e+00, 7.4442e-01, 1.7089e+00,\n",
       "         1.9589e+00, 1.7404e+00, 1.6072e+00,        nan, 1.1792e+00, 8.7922e-01,\n",
       "         1.9684e+00, 1.3926e+00,        nan, 1.0701e+00],\n",
       "        [1.6133e+00, 8.2368e-01, 3.8522e-03, 3.8029e-01, 1.2726e+00,        nan,\n",
       "         4.2718e-01, 4.6054e-01, 1.0652e+00,        nan,        nan, 7.4415e-01,\n",
       "                nan, 3.9545e-07, 1.2667e+00, 9.9539e-01],\n",
       "        [5.7705e-01, 9.4560e-01, 1.1531e+00, 8.6386e-05, 8.0989e-01, 8.7567e-01,\n",
       "         8.3881e-01, 1.4427e+00, 1.2176e+00, 8.1250e-01, 5.6353e-01, 8.0658e-01,\n",
       "                nan, 6.5196e-01, 5.1093e-01, 1.1017e+00],\n",
       "        [9.0117e-01, 1.8855e+00, 1.2955e+00, 6.7242e-01, 1.0615e+00, 1.1279e+00,\n",
       "         7.7869e-01, 1.3825e+00, 2.0021e+00, 1.0977e+00, 1.3424e+00, 1.4140e+00,\n",
       "         5.5616e-01, 1.2161e+00, 1.2799e+00, 1.1825e+00],\n",
       "        [1.3268e+00, 1.1734e+00, 1.1163e+00, 1.6849e+00, 1.5307e+00, 1.3083e+00,\n",
       "         1.4640e+00, 1.5817e+00, 1.8983e+00, 1.5080e+00, 1.3130e+00, 1.7523e+00,\n",
       "         1.6158e+00, 1.7067e+00, 1.5966e+00, 7.8218e-01],\n",
       "        [1.3957e+00, 1.4990e+00, 1.3893e+00, 1.1127e+00, 6.1138e-01, 1.3839e+00,\n",
       "         1.8340e+00, 2.6961e-01, 1.2982e+00, 1.3410e+00, 1.2983e+00, 1.1075e+00,\n",
       "         1.3148e+00, 1.1508e+00, 9.5742e-01, 1.4574e+00],\n",
       "        [1.0867e+00, 1.1220e+00, 1.6471e+00, 1.4897e+00, 1.1574e+00, 1.3830e+00,\n",
       "         5.8711e-01, 1.4408e+00, 1.2676e+00, 1.3939e+00, 1.7420e+00, 1.4253e+00,\n",
       "         1.1237e+00, 1.4053e+00, 1.0057e+00, 1.4891e+00],\n",
       "        [1.1693e+00, 1.7985e+00, 6.8138e-01, 1.4781e+00, 1.9377e+00, 1.1238e+00,\n",
       "         1.3206e+00, 1.0967e+00, 1.0135e+00, 1.1826e+00, 1.0804e+00, 1.2931e+00,\n",
       "         1.8915e+00, 9.6637e-01, 1.5690e+00, 1.1431e+00],\n",
       "        [1.9343e+00, 1.2314e+00, 1.3847e+00, 1.1462e+00, 1.7443e+00, 1.8787e+00,\n",
       "         1.7028e+00, 1.6122e+00, 9.2279e-01, 8.0093e-01, 2.0264e+00, 1.3714e+00,\n",
       "         1.0780e+00, 1.2426e+00, 1.4234e+00, 1.6081e+00],\n",
       "        [1.1728e+00, 9.2306e-01, 9.2450e-01, 4.2828e-01, 8.4145e-01, 8.4013e-01,\n",
       "         1.1261e+00, 1.1657e+00,        nan, 1.8856e+00, 1.2514e+00, 1.2180e+00,\n",
       "         5.5893e-01, 8.7666e-01, 1.1889e+00, 1.3336e+00],\n",
       "        [1.3044e+00, 1.4936e+00, 1.2679e+00, 1.1963e+00, 1.1640e+00, 9.4332e-01,\n",
       "         8.2166e-01, 1.5893e+00, 1.2626e+00, 1.0048e+00, 1.1998e+00, 1.1388e+00,\n",
       "         4.8258e-01, 1.1686e+00, 1.3141e+00, 1.2038e+00],\n",
       "        [1.1263e+00, 7.6244e-01, 9.3770e-01, 1.2489e+00, 1.4855e+00, 1.3257e+00,\n",
       "         1.2317e+00, 1.1011e+00, 1.1806e+00, 1.4573e+00, 1.4868e+00, 1.5662e+00,\n",
       "         1.5411e+00, 1.4948e+00, 1.3638e+00, 1.2424e+00],\n",
       "        [1.3065e+00, 1.3557e+00, 1.1473e+00, 1.2213e+00, 1.4077e+00, 1.6043e+00,\n",
       "         5.8029e-01, 1.1867e+00, 9.5388e-01, 1.2926e+00, 1.0057e+00, 1.3230e+00,\n",
       "         1.0356e+00, 1.0961e+00, 9.7668e-01, 1.3035e+00],\n",
       "        [1.0044e+00, 1.3549e+00, 1.4947e+00, 1.5549e+00, 1.4003e+00, 1.5876e+00,\n",
       "         1.4732e+00, 1.0692e+00, 1.3962e+00, 1.3588e+00, 1.3254e+00, 1.6922e+00,\n",
       "         1.4791e+00, 1.1463e+00, 1.2848e+00, 1.4327e+00],\n",
       "        [1.5284e+00, 9.0087e-01, 1.5205e+00, 1.2344e+00, 9.2044e-01, 1.7466e+00,\n",
       "         8.7228e-01, 2.0604e+00, 1.3522e+00, 1.3701e+00, 1.7417e+00, 1.8109e+00,\n",
       "         1.3214e+00, 1.0180e+00, 1.2162e+00, 1.1126e+00],\n",
       "        [1.6982e+00, 2.2755e+00, 1.9508e+00, 1.0546e+00, 2.0881e+00, 1.3706e+00,\n",
       "         1.8883e+00, 1.2056e+00, 2.0515e+00, 1.7340e+00, 1.3729e+00, 1.3628e+00,\n",
       "         1.4900e+00, 1.5394e+00, 1.4869e+00, 1.7173e+00],\n",
       "        [1.1962e+00, 1.5316e+00, 1.4744e+00, 1.5589e+00, 1.9205e+00, 1.1961e+00,\n",
       "         1.6693e+00, 1.4303e+00, 1.3850e+00, 1.0897e+00, 1.5168e+00, 1.3709e+00,\n",
       "         2.1145e+00, 1.4148e+00, 1.5852e+00, 1.9869e+00],\n",
       "        [1.3092e+00, 1.3473e+00, 2.2138e+00, 9.1218e-01, 2.1226e+00, 1.8850e+00,\n",
       "         1.0755e+00, 6.8789e-01, 1.9636e+00, 2.0695e+00, 1.1394e+00, 1.7763e+00,\n",
       "         1.5330e+00, 1.8237e+00, 1.7183e+00, 2.0607e+00],\n",
       "        [2.3000e+00, 1.9898e+00, 2.4436e+00, 2.4043e+00, 2.5676e+00, 2.4414e+00,\n",
       "         2.4372e+00, 2.4127e+00, 2.2203e+00, 1.9514e+00, 2.2386e+00, 2.1265e+00,\n",
       "         2.4363e+00, 8.0421e-01, 2.1666e+00, 1.4922e+00],\n",
       "        [2.6084e+00, 2.5150e+00, 2.1992e+00, 2.2685e+00, 1.8009e+00, 1.7788e+00,\n",
       "         2.5267e+00, 2.4123e+00, 2.1295e+00, 2.1564e+00, 2.3142e+00, 2.6479e+00,\n",
       "         2.5275e+00, 2.3162e+00, 2.7238e+00, 1.2966e+00],\n",
       "        [2.8716e+00, 2.9283e+00, 2.8632e+00, 2.9039e+00, 2.8103e+00, 2.9451e+00,\n",
       "         2.9268e+00, 2.8698e+00, 2.7581e+00, 2.9046e+00, 2.8850e+00, 2.8839e+00,\n",
       "         2.6984e+00, 2.8488e+00, 2.9383e+00, 2.6306e+00],\n",
       "        [2.9666e+00, 2.9248e+00, 2.9839e+00, 2.9954e+00, 2.9403e+00, 2.9366e+00,\n",
       "         2.9382e+00, 2.9660e+00, 2.9239e+00, 2.8501e+00, 2.9705e+00, 2.9935e+00,\n",
       "         2.8973e+00, 2.9978e+00, 2.9438e+00, 2.9578e+00],\n",
       "        [2.9898e+00, 3.0156e+00, 3.0124e+00, 3.0001e+00, 3.0158e+00, 3.0007e+00,\n",
       "         3.0181e+00, 3.0111e+00, 3.0139e+00, 3.0150e+00, 3.0166e+00, 3.0151e+00,\n",
       "         3.0155e+00, 3.0140e+00, 2.9710e+00, 2.9989e+00]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的问题是nan，个人猜测是下溢出，因为log(0)的存在导致下溢出问题进而出现nan，应该是计算注意力熵的时候出现的问题。后续可以对计算方式进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(ds['train'][0]['text'],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 32963,   154, 28562, 26432,   281,     6,  1423, 22383,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model(**inputs)[1][0][0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"papluca/language-identification\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\",output_attentions=True)\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"papluca/language-identification\")\n",
    "\n",
    "def calculate_AE_matrix(attention_matrix):\n",
    "    return (torch.log(attention_matrix) * attention_matrix * (-1)).sum(dim=3).mean(dim=2)\n",
    "\n",
    "# 函数原型如下，这里的if条件判断则是为了防止gpu上放置数据过多，导致核崩溃\n",
    "# 这里需要基于这个函数来微微调整得到各个模型的AE矩阵计算的代码\n",
    "\n",
    "def get_AE_matrix(model,dataset,heads_per_layer,layers):\n",
    "    model.to('cpu')\n",
    "    attention_entropy=torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    AE_matrix = torch.tensor([[0.]*heads_per_layer]*layers)\n",
    "    data_amount = 0\n",
    "    for data in tqdm(dataset):\n",
    "        if data_amount/len(dataset)<=0.4:\n",
    "            if model.device!=torch.device(type='cuda',index=0):\n",
    "                model.to('cuda')\n",
    "            attention_entropy=attention_entropy.to('cuda')\n",
    "            input_text = data['text']\n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cuda')\n",
    "            output = model(**inputs)\n",
    "            \n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attentions.to('cuda')\n",
    "            AE = calculate_AE_matrix(attentions)\n",
    "            AE.to('cuda')\n",
    "            attention_entropy += AE\n",
    "            data_amount+=1\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "            attention_entropy=attention_entropy.to('cpu')\n",
    "            input_text = data['text']\n",
    "            inputs = tokenizer(input_text,return_tensors='pt').to('cpu')\n",
    "            output = model(**inputs)\n",
    "            attentions = output[1]\n",
    "            attentions = torch.cat([(layer) for layer in attentions])\n",
    "            attention_entropy += calculate_AE_matrix(attentions.to('cpu'))\n",
    "            data_amount+=1\n",
    "    AE_matrix = attention_entropy / data_amount    \n",
    "    return AE_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['labels'])\n",
    "    def __getitem__(self,idx):\n",
    "        keys = list(self.dataset.keys())\n",
    "        values = list(self.dataset.values())\n",
    "        return {keys[0]:values[0][idx],keys[1]:values[1][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:15<00:00, 17.48it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_ = ds['validation'][7724:8000]\n",
    "data_val = MyDataset(dataset_)\n",
    "AE_XLM_ROBERTA=get_AE_matrix(model,data_val,12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(AE_XLM_ROBERTA,\"./XLM_ROBERTA_AE/XLM_ROBERTA7724_7999AE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_15768\\1308520168.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  AE_XLM_ROBERTA_LIST.append(torch.load(f'./XLM_ROBERTA_AE/XLM_ROBERTA{(i-1)*1000}_{i*1000-1}AE.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "AE_XLM_ROBERTA_LIST=[]\n",
    "for i in range(1,11):\n",
    "    if i==8:\n",
    "        continue\n",
    "    AE_XLM_ROBERTA_LIST.append(torch.load(f'./XLM_ROBERTA_AE/XLM_ROBERTA{(i-1)*1000}_{i*1000-1}AE.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2.7208, 2.9040, 2.5561, 2.7971, 3.0390, 2.6836, 2.5233, 2.7726, 1.5327,\n",
       "          2.7072, 2.9090, 2.9809],\n",
       "         [0.7757, 1.3725, 2.3615, 1.5987, 0.6239, 1.5879, 2.1649, 0.8296,    nan,\n",
       "          1.9505, 1.9480, 1.4741],\n",
       "         [0.7764, 0.9156, 1.0146, 1.8412, 0.6824, 0.1279, 0.7044, 1.8126, 1.3499,\n",
       "          1.9542, 1.9365, 1.3603],\n",
       "         [0.4372, 2.2146, 1.4126, 1.3566, 1.1266, 0.2592, 1.6192, 2.2266, 0.6341,\n",
       "          1.2258, 1.7459, 1.0886],\n",
       "         [1.7033, 1.7393, 1.0916, 0.5789, 1.0968, 0.8719, 2.3253, 0.0756, 2.0827,\n",
       "          1.2578, 2.4729, 1.1472],\n",
       "         [1.4798, 1.8523, 1.8628, 2.5126, 1.2633, 1.4314, 1.1985, 2.2842, 1.1645,\n",
       "          1.6478, 2.1991, 1.7491],\n",
       "         [1.8639, 1.2761, 0.2099, 1.9127, 1.3415, 1.7540, 2.0983, 1.9856, 1.8368,\n",
       "          1.7333, 1.5643, 0.2295],\n",
       "         [1.8684, 0.7614, 1.5729, 0.5282, 1.9519, 2.3432, 2.2022, 1.8511, 2.4962,\n",
       "          2.5041, 1.7224, 0.9746],\n",
       "         [0.7545, 1.7521, 1.4338, 2.6584, 1.7590, 0.3367, 1.6397, 1.0442, 0.8488,\n",
       "          1.8385, 1.9334, 1.4995],\n",
       "         [2.5305, 2.8543, 2.6058, 2.1893, 2.8075, 2.5620, 2.1597, 2.8164, 1.7198,\n",
       "          2.6252, 2.9196, 1.5390],\n",
       "         [2.7854, 3.1750, 3.1009, 3.1120, 3.1441, 3.0482, 3.0531, 2.5814, 3.0660,\n",
       "          3.1692, 3.1114, 2.8384],\n",
       "         [2.6429, 3.0746, 3.0452, 2.4147, 3.0653, 3.1156, 0.7690, 3.0405, 3.0580,\n",
       "          3.0101, 3.0759, 3.1073]], requires_grad=True),\n",
       " tensor([[2.7489, 2.9236, 2.5840, 2.8177, 3.0596, 2.7016, 2.5381, 2.8030, 1.5491,\n",
       "          2.7283, 2.9310, 3.0008],\n",
       "         [0.7819, 1.3982, 2.3809, 1.6072, 0.6282, 1.5994, 2.1702, 0.8320,    nan,\n",
       "          1.9523, 1.9746, 1.4780],\n",
       "         [0.7843, 0.9201, 1.0218, 1.8577, 0.6837, 0.1253, 0.7037, 1.8345, 1.3630,\n",
       "          1.9699, 1.9610, 1.3579],\n",
       "         [0.4373, 2.2290, 1.4300, 1.3668, 1.1426, 0.2661, 1.6488, 2.2436, 0.6354,\n",
       "          1.2356, 1.7608, 1.0980],\n",
       "         [1.7125, 1.7530, 1.0932, 0.5938, 1.0973, 0.8854, 2.3710, 0.0816, 2.1195,\n",
       "          1.2615, 2.4938, 1.1656],\n",
       "         [1.4927, 1.8548, 1.8746, 2.5217, 1.2940, 1.4314, 1.2095, 2.2911, 1.1660,\n",
       "          1.6395, 2.2032, 1.7528],\n",
       "         [1.8659, 1.2873, 0.2190, 1.9265, 1.3497, 1.7757, 2.1114, 2.0014, 1.8552,\n",
       "          1.7515, 1.5957, 0.2394],\n",
       "         [1.9088, 0.7780, 1.5743, 0.5319, 1.9609, 2.3583, 2.2188, 1.8504, 2.5093,\n",
       "          2.5202, 1.7188, 0.9955],\n",
       "         [0.7912, 1.7513, 1.4096, 2.6850, 1.7527, 0.3282, 1.6079, 1.0202, 0.8889,\n",
       "          1.8101, 1.9169, 1.4779],\n",
       "         [2.5733, 2.8682, 2.6115, 2.1759, 2.8207, 2.5766, 2.1702, 2.8268, 1.7109,\n",
       "          2.6293, 2.9402, 1.5484],\n",
       "         [2.8001, 3.1954, 3.1207, 3.1366, 3.1630, 3.0686, 3.0657, 2.5739, 3.0901,\n",
       "          3.1879, 3.1303, 2.8517],\n",
       "         [2.6477, 3.0933, 3.0580, 2.4884, 3.0781, 3.1344, 0.7786, 3.0534, 3.0731,\n",
       "          3.0230, 3.0882, 3.1243]], requires_grad=True),\n",
       " tensor([[2.7357, 2.9134, 2.5703, 2.8015, 3.0438, 2.6857, 2.5226, 2.7786, 1.5340,\n",
       "          2.7112, 2.9154, 2.9865],\n",
       "         [0.7779, 1.3767, 2.3567, 1.5965, 0.6243, 1.5679, 2.1501, 0.8320,    nan,\n",
       "          1.9337, 1.9443, 1.4734],\n",
       "         [0.7767, 0.9215, 1.0186, 1.8513, 0.6817,    nan, 0.7030, 1.8221, 1.3405,\n",
       "          1.9679, 1.9289, 1.3446],\n",
       "         [0.4374, 2.2150, 1.4227, 1.3535, 1.1340, 0.2642, 1.6204, 2.2315, 0.6317,\n",
       "          1.2211, 1.7506, 1.0981],\n",
       "         [1.6921, 1.7353, 1.0847, 0.5883, 1.1015, 0.8799, 2.3463, 0.0827, 2.0971,\n",
       "          1.2497, 2.4873, 1.1658],\n",
       "         [1.4800, 1.8414, 1.8551, 2.5090, 1.2846, 1.4261, 1.2185, 2.2829, 1.1637,\n",
       "          1.6274, 2.2019, 1.7432],\n",
       "         [1.8605, 1.2774, 0.2207, 1.9130, 1.3439, 1.7693, 2.0921, 1.9877, 1.8301,\n",
       "          1.7424, 1.5835, 0.2408],\n",
       "         [1.8925, 0.7691, 1.5639, 0.5309, 1.9461, 2.3471, 2.2006, 1.8399, 2.4953,\n",
       "          2.5089, 1.7020, 0.9804],\n",
       "         [0.7779, 1.7352, 1.4165, 2.6562, 1.7488, 0.3481, 1.6235, 1.0252, 0.8622,\n",
       "          1.8088, 1.9017, 1.4941],\n",
       "         [2.5128, 2.8469, 2.5926, 2.1729, 2.8064, 2.5614, 2.1617, 2.8013, 1.6801,\n",
       "          2.6273, 2.9120, 1.5261],\n",
       "         [2.7822, 3.1773, 3.0982, 3.1155, 3.1423, 3.0471, 3.0530, 2.5982, 3.0658,\n",
       "          3.1707, 3.1135, 2.8035],\n",
       "         [2.5888, 3.0786, 3.0393, 2.3843, 3.0623, 3.1198, 0.7900, 3.0424, 3.0305,\n",
       "          3.0173, 3.0707, 3.1115]], requires_grad=True),\n",
       " tensor([[2.7454, 2.9161, 2.5713, 2.8147, 3.0523, 2.6950, 2.5274, 2.7849, 1.5465,\n",
       "          2.7096, 2.9222, 2.9924],\n",
       "         [0.7836, 1.3770, 2.3687, 1.6092, 0.6272, 1.5909, 2.1514, 0.8351,    nan,\n",
       "          1.9465, 1.9634, 1.4947],\n",
       "         [0.7850, 0.9283, 1.0249, 1.8606, 0.6858,    nan, 0.7115, 1.8316, 1.3448,\n",
       "          1.9618, 1.9527, 1.3575],\n",
       "         [0.4435, 2.2282, 1.4244, 1.3601, 1.1332, 0.2586, 1.6262, 2.2445, 0.6394,\n",
       "          1.2340, 1.7554, 1.0900],\n",
       "         [1.7100, 1.7525, 1.1026, 0.5878, 1.1259, 0.8926, 2.3360, 0.0847, 2.1033,\n",
       "          1.2632, 2.4980, 1.1696],\n",
       "         [1.4849, 1.8669, 1.8721, 2.5150, 1.2875, 1.4402, 1.2259, 2.2950, 1.1824,\n",
       "          1.6558, 2.2045, 1.7603],\n",
       "         [1.8794, 1.2773, 0.2232, 1.9215, 1.3470, 1.7719, 2.1067, 2.0077, 1.8576,\n",
       "          1.7648, 1.5957, 0.2433],\n",
       "         [1.8797, 0.7771, 1.5716, 0.5363, 1.9720, 2.3574, 2.2093, 1.8614, 2.5098,\n",
       "          2.5149, 1.7411, 0.9904],\n",
       "         [0.7903, 1.7580, 1.4542, 2.6757, 1.7500, 0.3465, 1.6165, 1.0585, 0.8614,\n",
       "          1.8316, 1.9525, 1.5090],\n",
       "         [2.5619, 2.8703, 2.6176, 2.1791, 2.8092, 2.5742, 2.1703, 2.8239, 1.6997,\n",
       "          2.6203, 2.9418, 1.5219],\n",
       "         [2.8077, 3.1917, 3.1159, 3.1303, 3.1586, 3.0641, 3.0648, 2.5996, 3.0882,\n",
       "          3.1869, 3.1271, 2.8598],\n",
       "         [2.6365, 3.0908, 3.0606, 2.4789, 3.0777, 3.1329, 0.7824, 3.0491, 3.0714,\n",
       "          3.0292, 3.0877, 3.1246]], requires_grad=True),\n",
       " tensor([[2.7422, 2.9317, 2.5855, 2.8276, 3.0676, 2.7104, 2.5419, 2.7998, 1.5473,\n",
       "          2.7325, 2.9364, 3.0067],\n",
       "         [0.7816, 1.3867, 2.3847, 1.5963, 0.6385, 1.6021, 2.1640, 0.8385,    nan,\n",
       "          1.9734, 1.9694, 1.4987],\n",
       "         [0.7927, 0.9277, 1.0287, 1.8679, 0.6879,    nan, 0.7101, 1.8364, 1.3763,\n",
       "          1.9744, 1.9694, 1.3866],\n",
       "         [0.4426, 2.2434, 1.4521, 1.3585, 1.1654, 0.2629, 1.6437, 2.2551, 0.6409,\n",
       "          1.2325, 1.7739, 1.1030],\n",
       "         [1.7120, 1.7638, 1.0935, 0.5872, 1.1147, 0.8954, 2.3630, 0.0846, 2.1218,\n",
       "          1.2548, 2.5159, 1.1841],\n",
       "         [1.4864, 1.8615, 1.8753, 2.5491, 1.2950, 1.4308, 1.2182, 2.3078, 1.1746,\n",
       "          1.6466, 2.2227, 1.7620],\n",
       "         [1.8695, 1.2667, 0.2150, 1.9246, 1.3418, 1.7714, 2.1059, 1.9999, 1.8298,\n",
       "          1.7667, 1.6130, 0.2403],\n",
       "         [1.9114, 0.7562, 1.5687, 0.5264, 1.9671, 2.3577, 2.2139, 1.8469, 2.5105,\n",
       "          2.5424, 1.7171, 0.9488],\n",
       "         [0.7781, 1.7636, 1.4640, 2.6882, 1.7486, 0.3508, 1.6425, 1.0601, 0.8943,\n",
       "          1.8321, 1.9572, 1.5335],\n",
       "         [2.5642, 2.8780, 2.6221, 2.1936, 2.8309, 2.6015, 2.1682, 2.8313, 1.7439,\n",
       "          2.6480, 2.9478, 1.5349],\n",
       "         [2.8195, 3.2038, 3.1276, 3.1467, 3.1698, 3.0739, 3.0715, 2.6373, 3.1008,\n",
       "          3.1978, 3.1439, 2.8718],\n",
       "         [2.6108, 3.1074, 3.0769, 2.4464, 3.0962, 3.1487, 0.7768, 3.0611, 3.0856,\n",
       "          3.0430, 3.1062, 3.1389]], requires_grad=True),\n",
       " tensor([[2.7535, 2.9337, 2.5852, 2.8303, 3.0700, 2.7060, 2.5447, 2.8051, 1.5674,\n",
       "          2.7350, 2.9392, 3.0067],\n",
       "         [0.7854, 1.3988, 2.3825, 1.6071, 0.6350, 1.5974, 2.1574, 0.8435,    nan,\n",
       "          1.9516, 1.9431, 1.4769],\n",
       "         [0.7825, 0.9193, 1.0233, 1.8539, 0.6873,    nan, 0.7072, 1.8233, 1.3587,\n",
       "          1.9894, 1.9602, 1.3660],\n",
       "         [0.4404, 2.2349, 1.4300, 1.3606, 1.1408, 0.2633, 1.6317, 2.2435, 0.6384,\n",
       "          1.2345, 1.7645, 1.1039],\n",
       "         [1.7054, 1.7528, 1.0905, 0.5944, 1.1098, 0.8845, 2.3587, 0.0777, 2.1168,\n",
       "          1.2543, 2.5022, 1.1777],\n",
       "         [1.4876, 1.8519, 1.8707, 2.5425, 1.2836, 1.4318, 1.2385, 2.3026, 1.1665,\n",
       "          1.6415, 2.2163, 1.7565],\n",
       "         [1.8529, 1.2772, 0.2121, 1.9173, 1.3398, 1.7797, 2.0976, 1.9909, 1.8383,\n",
       "          1.7539, 1.5914, 0.2320],\n",
       "         [1.9017, 0.7664, 1.5745, 0.5290, 1.9624, 2.3616, 2.2070, 1.8466, 2.5016,\n",
       "          2.5383, 1.7193, 0.9713],\n",
       "         [0.7679, 1.7621, 1.4293, 2.6835, 1.7453, 0.3426, 1.6309, 1.0256, 0.8931,\n",
       "          1.8295, 1.9324, 1.4954],\n",
       "         [2.5771, 2.8806, 2.6220, 2.1750, 2.8329, 2.5991, 2.1857, 2.8295, 1.7591,\n",
       "          2.6532, 2.9510, 1.5341],\n",
       "         [2.8407, 3.2049, 3.1283, 3.1439, 3.1760, 3.0752, 3.0774, 2.6380, 3.0999,\n",
       "          3.2002, 3.1457, 2.8734],\n",
       "         [2.6490, 3.1062, 3.0752, 2.3961, 3.0931, 3.1449, 0.7696, 3.0689, 3.0909,\n",
       "          3.0453, 3.1036, 3.1378]], requires_grad=True),\n",
       " tensor([[2.7678, 2.9522, 2.5868, 2.8526, 3.0854, 2.7181, 2.5593, 2.8153, 1.5606,\n",
       "          2.7517, 2.9533, 3.0287],\n",
       "         [0.7813, 1.3880, 2.3974, 1.6154, 0.6305, 1.6070, 2.1794, 0.8344,    nan,\n",
       "          1.9708, 1.9766, 1.4968],\n",
       "         [0.7969, 0.9213, 1.0259, 1.8678, 0.6873,    nan, 0.7072, 1.8446, 1.3752,\n",
       "          1.9784, 1.9760, 1.3870],\n",
       "         [0.4416, 2.2553, 1.4503, 1.3723, 1.1720, 0.2675, 1.6595, 2.2620, 0.6413,\n",
       "          1.2449, 1.7806, 1.1103],\n",
       "         [1.7279, 1.7678, 1.1066, 0.6011, 1.1177, 0.9037, 2.3712, 0.0893, 2.1359,\n",
       "          1.2712, 2.5114, 1.1945],\n",
       "         [1.5019, 1.8596, 1.8817, 2.5381, 1.2996, 1.4397, 1.2416, 2.3132, 1.1784,\n",
       "          1.6617, 2.2284, 1.7666],\n",
       "         [1.8843, 1.2896, 0.2284, 1.9326, 1.3559, 1.8024, 2.1311, 2.0181, 1.8707,\n",
       "          1.7718, 1.6087, 0.2388],\n",
       "         [1.9035, 0.7729, 1.5867, 0.5347, 1.9782, 2.3653, 2.2269, 1.8703, 2.5194,\n",
       "          2.5562, 1.7453, 0.9849],\n",
       "         [0.7859, 1.7691, 1.4435, 2.6956, 1.7647, 0.3565, 1.6378, 1.0586, 0.9066,\n",
       "          1.8339, 1.9515, 1.5261],\n",
       "         [2.5905, 2.8832, 2.6272, 2.1901, 2.8394, 2.5989, 2.1675, 2.8398, 1.7444,\n",
       "          2.6465, 2.9695, 1.5496],\n",
       "         [2.8167, 3.2206, 3.1423, 3.1596, 3.1884, 3.0866, 3.0899, 2.6309, 3.1139,\n",
       "          3.2129, 3.1534, 2.8746],\n",
       "         [2.6342, 3.1212, 3.0905, 2.4497, 3.1084, 3.1576, 0.7917, 3.0767, 3.0975,\n",
       "          3.0569, 3.1190, 3.1528]], requires_grad=True),\n",
       " tensor([[2.7615, 2.9461, 2.6058, 2.8419, 3.0801, 2.7155, 2.5573, 2.8127, 1.5666,\n",
       "          2.7358, 2.9489, 3.0196],\n",
       "         [0.7824, 1.3982, 2.3988, 1.6168, 0.6374, 1.6148, 2.1659, 0.8465,    nan,\n",
       "          1.9680, 1.9786, 1.4958],\n",
       "         [0.7946, 0.9212, 1.0259, 1.8758, 0.6847, 0.1236, 0.7080, 1.8413, 1.3684,\n",
       "          1.9797, 1.9749, 1.3691],\n",
       "         [0.4387, 2.2567, 1.4452, 1.3718, 1.1552, 0.2644, 1.6474, 2.2695, 0.6416,\n",
       "          1.2431, 1.7747, 1.1062],\n",
       "         [1.7256, 1.7729, 1.0976, 0.5970, 1.1190, 0.8976, 2.3916, 0.0847, 2.1412,\n",
       "          1.2656, 2.5189, 1.1975],\n",
       "         [1.4941, 1.8579, 1.8838, 2.5560, 1.3030, 1.4356, 1.2352, 2.3038, 1.1737,\n",
       "          1.6451, 2.2193, 1.7600],\n",
       "         [1.8671, 1.2801, 0.2189, 1.9327, 1.3492, 1.7929, 2.1123, 2.0113, 1.8421,\n",
       "          1.7583, 1.6145, 0.2404],\n",
       "         [1.9213, 0.7740, 1.5772, 0.5386, 1.9685, 2.3672, 2.2167, 1.8561, 2.5156,\n",
       "          2.5335, 1.7201, 0.9769],\n",
       "         [0.7769, 1.7722, 1.4295, 2.6920, 1.7366, 0.3440, 1.6073, 1.0420, 0.9156,\n",
       "          1.8395, 1.9437, 1.4979],\n",
       "         [2.5855, 2.8868, 2.6219, 2.1869, 2.8404, 2.5953, 2.1743, 2.8465, 1.7180,\n",
       "          2.6434, 2.9624, 1.5476],\n",
       "         [2.8312, 3.2135, 3.1409, 3.1611, 3.1820, 3.0890, 3.0843, 2.5977, 3.1130,\n",
       "          3.2102, 3.1573, 2.8849],\n",
       "         [2.6248, 3.1168, 3.0848, 2.4409, 3.1028, 3.1601, 0.7847, 3.0713, 3.1005,\n",
       "          3.0515, 3.1134, 3.1501]], requires_grad=True),\n",
       " tensor([[2.7588, 2.9319, 2.5865, 2.8296, 3.0696, 2.7101, 2.5512, 2.8057, 1.5562,\n",
       "          2.7343, 2.9394, 3.0112],\n",
       "         [0.7831, 1.3699, 2.3690, 1.6043, 0.6283, 1.5888, 2.1557, 0.8349,    nan,\n",
       "          1.9582, 1.9675, 1.4781],\n",
       "         [0.7824, 0.9244, 1.0247, 1.8674, 0.6874, 0.1256, 0.7081, 1.8404, 1.3385,\n",
       "          1.9699, 1.9582, 1.3450],\n",
       "         [0.4418, 2.2400, 1.4238, 1.3741, 1.1349, 0.2614, 1.6276, 2.2647, 0.6410,\n",
       "          1.2396, 1.7522, 1.1029],\n",
       "         [1.7273, 1.7678, 1.1048, 0.5990, 1.1263, 0.9024, 2.3889, 0.0881, 2.1304,\n",
       "          1.2751, 2.4949, 1.1883],\n",
       "         [1.5023, 1.8533, 1.8861, 2.5205, 1.3072, 1.4396, 1.2273, 2.3057, 1.1790,\n",
       "          1.6472, 2.2261, 1.7597],\n",
       "         [1.8791, 1.2844, 0.2243, 1.9264, 1.3498, 1.7891, 2.1148, 2.0084, 1.8567,\n",
       "          1.7589, 1.6115, 0.2435],\n",
       "         [1.9062, 0.7792, 1.5658, 0.5327, 1.9600, 2.3683, 2.2142, 1.8530, 2.5169,\n",
       "          2.5355, 1.7203, 1.0051],\n",
       "         [0.7848, 1.7556, 1.4098, 2.6856, 1.7377, 0.3444, 1.6059, 1.0241, 0.8721,\n",
       "          1.8204, 1.9208, 1.4994],\n",
       "         [2.5639, 2.8778, 2.6202, 2.1841, 2.8291, 2.5870, 2.1614, 2.8366, 1.6785,\n",
       "          2.6386, 2.9409, 1.5401],\n",
       "         [2.8094, 3.1991, 3.1277, 3.1405, 3.1652, 3.0742, 3.0727, 2.6062, 3.0905,\n",
       "          3.1955, 3.1384, 2.8373],\n",
       "         [2.6312, 3.1032, 3.0696, 2.4447, 3.0914, 3.1463, 0.7767, 3.0554, 3.0585,\n",
       "          3.0373, 3.0988, 3.1352]], requires_grad=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_XLM_ROBERTA_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_15768\\3356834335.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = torch.load('./XLM_ROBERTA_AE/XLM_ROBERTA7000_7722AE.pt')\n",
      "C:\\Users\\m1830\\AppData\\Local\\Temp\\ipykernel_15768\\3356834335.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b = torch.load('./XLM_ROBERTA_AE/XLM_ROBERTA7724_7999AE.pt')\n"
     ]
    }
   ],
   "source": [
    "a = torch.load('./XLM_ROBERTA_AE/XLM_ROBERTA7000_7722AE.pt')\n",
    "b = torch.load('./XLM_ROBERTA_AE/XLM_ROBERTA7724_7999AE.pt')\n",
    "AE_matrix=(sum([ae*1000 for ae in AE_XLM_ROBERTA_LIST])+a*723+b*276)/9999\n",
    "torch.save(AE_matrix,\"./XLM_ROBERTA_AE/XLM_ROBERTA_9999AE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里选择跳过一个因为，第7724的位置那个数据长大于864，导致报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7472, 2.9270, 2.5793, 2.8226, 3.0622, 2.7021, 2.5405, 2.7961, 1.5514,\n",
       "         2.7265, 2.9318, 3.0028],\n",
       "        [0.7816, 1.3866, 2.3770, 1.6064, 0.6307, 1.5959, 2.1630, 0.8361,    nan,\n",
       "         1.9572, 1.9643, 1.4853],\n",
       "        [0.7862, 0.9225, 1.0233, 1.8597, 0.6854,    nan, 0.7071, 1.8317, 1.3581,\n",
       "         1.9713, 1.9579, 1.3653],\n",
       "        [0.4398, 2.2347, 1.4329, 1.3632, 1.1460, 0.2639, 1.6362, 2.2474, 0.6381,\n",
       "         1.2343, 1.7628, 1.1011],\n",
       "        [1.7116, 1.7552, 1.0954, 0.5926, 1.1112, 0.8905, 2.3594, 0.0836, 2.1158,\n",
       "         1.2604, 2.5000, 1.1761],\n",
       "        [1.4902, 1.8560, 1.8729, 2.5291, 1.2911, 1.4341, 1.2276, 2.2982, 1.1720,\n",
       "         1.6461, 2.2137, 1.7572],\n",
       "        [1.8695, 1.2799, 0.2200, 1.9231, 1.3465, 1.7786, 2.1079, 2.0013, 1.8468,\n",
       "         1.7569, 1.5979, 0.2396],\n",
       "        [1.8985, 0.7705, 1.5725, 0.5332, 1.9640, 2.3576, 2.2119, 1.8527, 2.5077,\n",
       "         2.5288, 1.7239, 0.9798],\n",
       "        [0.7803, 1.7603, 1.4389, 2.6791, 1.7531, 0.3474, 1.6300, 1.0447, 0.8844,\n",
       "         1.8299, 1.9384, 1.5079],\n",
       "        [2.5593, 2.8712, 2.6149, 2.1843, 2.8236, 2.5844, 2.1701, 2.8270, 1.7202,\n",
       "         2.6380, 2.9417, 1.5400],\n",
       "        [2.8091, 3.1974, 3.1214, 3.1377, 3.1648, 3.0683, 3.0704, 2.6086, 3.0912,\n",
       "         3.1915, 3.1343, 2.8540],\n",
       "        [2.6257, 3.0988, 3.0661, 2.4374, 3.0855, 3.1390, 0.7803, 3.0571, 3.0731,\n",
       "         3.0339, 3.0955, 3.1310]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的问题是nan，个人猜测是下溢出，因为log(0)的存在导致下溢出问题进而出现nan，应该是计算注意力熵的时候出现的问题。后续可以对计算方式进行微调。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
